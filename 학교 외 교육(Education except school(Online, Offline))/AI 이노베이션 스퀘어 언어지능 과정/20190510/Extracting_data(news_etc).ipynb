{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, params={}, retries=3):\n",
    "    resp = None\n",
    "    \n",
    "    header = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36\"}\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, params=params, headers = header)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500 <= e.response.status_code < 600 and retries > 0:\n",
    "            print(retries)\n",
    "            resp = download(url, params, retries - 1)\n",
    "        else:\n",
    "            print(e.response.status_code)\n",
    "            print(e.response.reason)\n",
    "            print(e.request.headers)\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html = download(\"https://media.daum.net/breakingnews/society\")\n",
    "daumnews = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link_kakao\" href=\"http://www.kakaocorp.com/\">Kakao Corp.</a>\n"
     ]
    }
   ],
   "source": [
    "daumnewstitellists = daumnews.select(\"strong > a\")\n",
    "\n",
    "output_file_name = \"DaumNews_Urls.txt\"\n",
    "output_file = open(output_file_name, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for links in daumnewstitellists:\n",
    "    #print(links.text)\n",
    "    links.get('href')\n",
    "    print()\n",
    "\n",
    "output_file_name = \"DaumNews_Urls.txt\"\n",
    "output_file = open(output_file_name, \"w\", encoding=\"utf-8\")\n",
    "page_num = 1\n",
    "max_page_num = 2\n",
    "\n",
    "user_agent = \"'Mozilla/5.0\"\n",
    "headers ={\"User-Agent\" : user_agent}\n",
    "\n",
    "while page_num<=max_page_num:\n",
    "\n",
    "    page_url = \"https://media.daum.net/breakingnews/society\"\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    html = response.text\n",
    "\n",
    "    \"\"\"\n",
    "    주어진 HTML에서 기사 URL을 추출한다.\n",
    "    \"\"\"\n",
    "    url_frags = re.findall('<a href=\"(.*?)\"',html)\n",
    "\n",
    "    urls = []\n",
    "    \n",
    "    for url_frag in url_frags:\n",
    "            urls.append(url_frag)\n",
    "\n",
    "    for url in urls:\n",
    "        print(url, file=output_file)\n",
    "    time.sleep(2)\n",
    "\n",
    "    page_num+=1\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = download('http://v.media.daum.net/v/20190512030900250')\n",
    "\n",
    "daumnews = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"desc_translate txt_newsview\">Translated by <a class=\"link_kakaoi #util #translate #kakaoi_link\" href=\"https://kakao.ai/\" target=\"_blank\">kakao i</a></p>, <p dmcf-pid=\"aoYV6Gu4M3\" dmcf-ptype=\"general\">어제저녁 8시 반쯤 울산 남구에 있는 2층짜리 건물의 2층 술집에서 불이 났습니다.</p>, <p dmcf-pid=\"aZU9zLBDDj\" dmcf-ptype=\"general\">불은 소방서 추산 470여만 원의 피해를 낸 뒤 30분 만에 꺼졌으며 종업원 2명이 불을 끄려다 연기를 마셔 병원에서 치료를 받았습니다.</p>, <p dmcf-pid=\"age5qpehgX\" dmcf-ptype=\"general\">소방당국은 주방에서 튀김 요리를 하다 불이 난 것으로 보고 정확한 원인을 조사하고 있습니다.</p>, <p dmcf-pid=\"aj3J8COfk5\" dmcf-ptype=\"general\">김대근 [kimdaegeun@ytn.co.kr]</p>]\n",
      "김대근 [kimdaegeun@ytn.co.kr]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "daumnewstitellists = daumnews.select(\"p\")\n",
    "\n",
    "print(daumnewstitellists)\n",
    "\n",
    "for links in daumnewstitellists:\n",
    "    a = links.text\n",
    "\n",
    "print(a)   \n",
    "with open('사회-2019051101.txt', 'w+', encoding='utf-8') as json_file:\n",
    "        json.dump(a, json_file, ensure_ascii=False, indent='\\n', sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://v.media.daum.net/v/20190510192814791', 'http://v.media.daum.net/v/20190510192814791', 'http://v.media.daum.net/v/20190510230851842', 'http://v.media.daum.net/v/20190510120023692', 'http://v.media.daum.net/v/20190510060104105', 'http://v.media.daum.net/v/20190510223306488', '/issue/1310235', 'http://v.media.daum.net/v/20190510223306488', 'http://v.media.daum.net/v/20190510204216049', 'http://v.media.daum.net/v/20190510203904991', 'http://v.media.daum.net/v/20190510221354336', 'http://v.media.daum.net/v/20190510221354336', 'http://v.media.daum.net/v/20190510221424338', 'http://v.media.daum.net/v/20190510214006005', 'http://v.media.daum.net/v/20190510203905992', 'http://v.media.daum.net/v/20190510203905992', 'http://v.media.daum.net/v/20190510214554075', 'http://v.media.daum.net/v/20190510134823729', 'http://v.media.daum.net/v/20190510214337048', 'http://v.media.daum.net/v/20190510214337048', 'http://v.media.daum.net/v/20190510213454914', 'http://v.media.daum.net/v/20190510203022796', 'http://v.media.daum.net/v/20190510221352333', 'http://v.media.daum.net/v/20190510220507245', 'http://v.media.daum.net/v/20190510214934117', 'http://v.media.daum.net/v/20190510212521741', 'http://v.media.daum.net/v/20190510211623617']\n",
      "['\\n', ' 바꿔치기 ', '\\n', ' 언론중재법 ', '\\n', ' 번역 안내 ', '\\n', '\\n', 'The copyright belongs to the original writer of the content, and there may be errors in machine translation results.', '\\n', '版权归内容原作者所有。机器翻译结果可能存在错误。', '\\n', '原文の著作権は原著著作者にあり、機械翻訳の結果にエラーが含まれることがあります。', '\\n', 'Hak cipta milik penulis asli dari konten, dan mungkin ditemukan kesalahan dalam hasil terjemahan mesin.', '\\n', 'Bản quyền thuộc về tác giả gốc của nội dung và có thể có lỗi trong kết quả dịch bằng máy.', '\\n', '\\n', '\\n', '\\n', '\\n', '(수원=연합뉴스) 강영훈 기자 = 자유한국당 경기도당 사무실에 침입해 당 해체 구호를 외치며 농성한 대학생 추정 남녀 5명이 경찰에 체포됐다.', '\\n', '경기 수원중부경찰서는 10일 건조물 침입 혐의로 A 씨 등 5명을 붙잡아 조사하고 있다고 밝혔다.', '\\n', '\\n', '\\n', '\\n           자유한국당 경기도당 로고 [자유한국당 경기도당 제공]\\n          ', '\\n', '\\n', 'A 씨 등은 이날 오후 4시 30분께 수원시 장안구 영화동 소재 자유한국당 경기도당 사무실에 들어가 \"자유한국당 해체하라\"는 등의 문구가 적힌 피켓을 들고 누워 구호를 외치는 등 농성한 혐의를 받고 있다.', '\\n', '경찰은 \"당 사무실에 대학생들이 들어와 소리 지르고 있다\"는 112 신고를 받고 출동해 A 씨 등을 현행범으로 체포했다.', '\\n', '이 과정에서 다친 사람이나 파손된 기물은 없었다.', '\\n', '붙잡힌 이들은 남성 4명과 여성 1명으로, 묵비권을 행사하며 이름과 나이 등 신원을 밝히지 않고 있다.', '\\n', '경찰은 이들을 상대로 자세한 사건 경위를 조사하고 있다.', '\\n', 'kyh@yna.co.kr', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '관련 태그', '\\n', '\\n', '\\n', '연재', '\\n', ' MCCP-856 연재 영역 수정 ', '\\n', '\\n', '\\n', ' //MCCP-856 연재 영역 수정 ', '\\n', '더보기', '\\n', '\\n', '\\n', '저작권자(c)연합뉴스. 무단전재-재배포금지', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', ' 언론사 관련기사(주요뉴스) ', '\\n', '\\n', '연합뉴스 주요 뉴스', '해당 언론사로 연결됩니다.', '\\n', '\\n', '고속道 여배우 사망…2차선에 차 세운 이유 남편 \"몰라\"', '\\n', '배우 조수현, 극단 선택 시도…병원 응급 이송', '\\n', '강다니엘 독자활동 길 열렸다…LM 전속계약 효력정지', '\\n', \"'닥꼬티 얼마예요? 기안84, 장애인 희화화 웹툰 사과\", '\\n', '연기자 보라·가수 필독, 공개 연애 2년만에 결별', '\\n', '길가던 여고생 차로 치고 납치·성폭행 남성, 징역10년', '\\n', '文대통령 \"대담서 더 공격적 공방 오갔어도 괜찮았겠다\"', '\\n', '정준영, 법정서 혐의 인정…\"합의 원해\" 전략 바꾼듯', '\\n', '日문화청장관 \"한국은 일본에 형 누나 같은 존재\"', '\\n', '기술 고도화하랬더니…中업체에 몽땅 빼돌린 中企직원', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '공지', '\\n', \"똑똑한 뉴스 챗봇 '뉴스봇'을 소개합니다.\", '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ast\n",
    "\n",
    "base_url = 'https://media.daum.net/society/'\n",
    "req = requests.get(base_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "newslist = soup.find(name=\"div\", attrs={\"class\":\"section_cate section_headline\"})\n",
    "newslist_atag = newslist.find_all('a')\n",
    "#print(newslist_atag)\n",
    "url_list = []\n",
    "for a in newslist_atag:\n",
    "    url_list.append(a.get('href'))\n",
    "print(url_list)\n",
    "#print(url_list)\n",
    "\n",
    "# 각 기사에서 텍스트만 정제하여 추출\n",
    "req = requests.get(url_list[0])\n",
    "#print(req)\n",
    "html = req.content\n",
    "#print(html)\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "text = ''\n",
    "doc = None\n",
    "for item in soup.find_all('div', id='mArticle'):\n",
    "    text = text + str(item.find_all(text=True))\n",
    "    text = ast.literal_eval(text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://v.media.daum.net/v/20190510120023692\n",
      "['\\n', ' 바꿔치기 ', '\\n', ' 언론중재법 ', '\\n', ' 번역 안내 ', '\\n', '\\n', 'The copyright belongs to the original writer of the content, and there may be errors in machine translation results.', '\\n', '版权归内容原作者所有。机器翻译结果可能存在错误。', '\\n', '原文の著作権は原著著作者にあり、機械翻訳の結果にエラーが含まれることがあります。', '\\n', 'Hak cipta milik penulis asli dari konten, dan mungkin ditemukan kesalahan dalam hasil terjemahan mesin.', '\\n', 'Bản quyền thuộc về tác giả gốc của nội dung và có thể có lỗi trong kết quả dịch bằng máy.', '\\n', '\\n', '\\n', '9세 어린이 치어 다리 골절상 입혀', '집에 데려다주고 아닌 척 가 버려', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '【서울=뉴시스】김온유 기자 = 전동휠을 타고 가다 어린이를 치고선 본인이 낸 사고가 아닌 것처럼 행세하며 집까지 데려다 준 20대가 검찰에 넘겨졌다.', '\\n', '서울 수서경찰서는 A씨(29)를 특정범죄가중처벌등에관한법률 위반 혐의로 검거해 지난 1일 검찰에 송치했다고 10일 밝혔다.', '\\n', '경찰에 따르면 지난 3월27일 오후 2시30분께 서울 강남구 대치동 인근에서 전동휠을 타고 가던 A씨는 한 아파트 후문에서 나오던 9세 어린이를 쳐 다리에 골절상 등을 입게 했다.', '\\n', 'A씨는 사고를 당한 어린이를 안아서 집에 데려다 준 뒤 본인이 낸 사고가 아닌 척 행동한 것으로 알려졌다.', '\\n', '사건은 해당 어린이가 보호자와 병원으로 가던 중 \"전동휠에 부딪혔다\"고 말하면서 경찰에 신고됐다. 경찰은 사고 현장 주변 폐쇄회로(CC)TV 60여대를 분석, A씨를 특정해 검거했다.', '\\n', '경찰은 \"최근 전동휠이나 퀵보드 관련 교통사고 발생 시 운전자로서 제대로 조치하지 않아 형사처벌 대상이 되거나 면허 취소가 되는 경우가 일어나고 있다\"면서 \"안전 속도와 안전 장구 등 도로교통법상 법규를 준수해야 한다\"고 당부했다.', '\\n', 'ohnew@newsis.com ', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '관련 태그', '\\n', '\\n', '\\n', '연재', '\\n', ' MCCP-856 연재 영역 수정 ', '\\n', '\\n', '\\n', ' //MCCP-856 연재 영역 수정 ', '\\n', '더보기', '\\n', '\\n', '\\n', '<저작권자ⓒ 공감언론 뉴시스통신사. 무단전재-재배포 금지.>', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', ' 언론사 관련기사(주요뉴스) ', '\\n', '\\n', '뉴시스 주요 뉴스', '해당 언론사로 연결됩니다.', '\\n', '\\n', \"'술 마시기 게임' 빙자해 집단성폭행 저지른 4명 실형\", '\\n', '기안84, 웹툰서 장애인 희화화…관련단체 \"공개 사과하라\"', '\\n', '여학생 일부러 차로 친 후 납치해 성폭행한 30대', '\\n', '한지성 음주여부 확인 2주 걸려… 술마신 남편 책임은', '\\n', '고등학교서 동급생 성추행 논란…수사의뢰', '\\n', '\"부적 주겠다\" 점 봤던 여성 성폭행한 무속인 징역 6년', '\\n', '강제추행 40대, 꽁초 DNA로 11년 전 성범죄도 들통', '\\n', '30대 여경 숨진채 발견…\"상관 때문에 힘들어 했다\"', '\\n', '환자 성폭행 혐의 정신과 의사 수사…\"그루밍 성폭력\" 주장', '\\n', \"'감히 이별을 통보해?'…전 여친 흉기로 마구 찌른 男 체포\", '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '공지', '\\n', \"똑똑한 뉴스 챗봇 '뉴스봇'을 소개합니다.\", '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(url_list[3])\n",
    "req = requests.get(url_list[3])\n",
    "#print(req)\n",
    "html = req.content\n",
    "#print(html)\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "text = ''\n",
    "doc = None\n",
    "for item in soup.find_all('div', id='mArticle'):\n",
    "    text = text + str(item.find_all(text=True))\n",
    "    text = ast.literal_eval(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그럼 국회에서 드러눕고 깽판친 한국당 시키들은 왜 구속안시키는건데\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import json\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://media.daum.net/society/')\n",
    "driver.find_element_by_xpath('//*[@id=\"cSub\"]/div/div[1]/div[1]/div/strong/a').click()\n",
    "driver.implicitly_wait(5)\n",
    "html = driver.page_source\n",
    "daumnews = BeautifulSoup(html, \"lxml\")\n",
    "lists = daumnews.select(\"p\")\n",
    "\n",
    "data  = {}\n",
    "\n",
    "for contents in lists:\n",
    "    a = contents.text\n",
    "print(a)\n",
    "    \n",
    "with open('daumnews-society.json', 'w+') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "#ensure_ascii=False, indent='\\t'\n",
    "# encoding='utf-8'\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html = download(\"https://media.daum.net/society/\")\n",
    "daumnews = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(daumnews)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#-*- coding: utf-8 -*\n",
    "\n",
    "\"\"\"\n",
    "네이버 경제 뉴스 중 증권관련 뉴스의 기사 URL을 수집합니다. 최근 10개의 페이지만 가져오겠습니다.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "\n",
    "eval_d = \"20190511\"\n",
    "output_file_name = \"DaumNews_Urls.txt\"\n",
    "output_file = open(output_file_name, \"w\", encoding=\"utf-8\")\n",
    "page_num = 1\n",
    "max_page_num = 2\n",
    "\n",
    "user_agent = \"'Mozilla/5.0\"\n",
    "headers ={\"User-Agent\" : user_agent}\n",
    "\n",
    "while page_num<=max_page_num:\n",
    "\n",
    "    page_url = \"https://media.daum.net/breakingnews/society\"\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    html = response.text\n",
    "\n",
    "    \"\"\"\n",
    "    주어진 HTML에서 기사 URL을 추출한다.\n",
    "    \"\"\"\n",
    "    url_frags = re.findall('<a href=\"(.*?)\"',html)\n",
    "\n",
    "    urls = []\n",
    "    \n",
    "    for url_frag in url_frags:\n",
    "            urls.append(url_frag)\n",
    "\n",
    "    for url in urls:\n",
    "        print(url, file=output_file)\n",
    "    time.sleep(2)\n",
    "\n",
    "    page_num+=1\n",
    "\n",
    "output_file.close()\n",
    "#[출처] 증권뉴스 데이터 수집(1/3)|작성자 엉드루"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter url file name : DaumNews_Urls.txt\n",
      "Enter output file name : society-20190511.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "네이버 뉴스 기사를 수집한다.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_url_file_name() :\n",
    "    \"\"\"\n",
    "    url 파일 이름을 받아 돌려준다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    url_file_name = input(\"Enter url file name : \")\n",
    "\n",
    "    return url_file_name\n",
    "\n",
    "def get_output_file_name():\n",
    "    \"\"\"\n",
    "    철력 파일의 이름을 입력받아 돌려준다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    output_file_name = input(\"Enter output file name : \")\n",
    "\n",
    "    return output_file_name\n",
    "\n",
    "def open_url_file(url_file_name):\n",
    "    \"\"\"\n",
    "    URL 파일을 연다.\n",
    "    :param url_file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    url_file = open(url_file_name, \"r\", encoding =\"utf-8\")\n",
    "\n",
    "    return url_file\n",
    "\n",
    "def create_output_file(output_file_name):\n",
    "    \"\"\"\n",
    "    출력 파일을 생성한다.\n",
    "    :param output_file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    output_file = open(output_file_name, \"w\", encoding='utf-8')\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def gen_print_url(url_line):\n",
    "    \"\"\"\n",
    "    주어진 기사 링크 URL로 부터 인쇄용 URL을 만들어 돌려준다.\n",
    "    :param url_line:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    article_id = url_line[(len(url_line)-24):len(url_line)]\n",
    "    print_url = \"https://media.daum.net/breakingnews/society\" + article_id\n",
    "\n",
    "    return print_url\n",
    "\n",
    "def get_html(print_url) :\n",
    "    \"\"\"\n",
    "    주어진 인쇄용 URL에 접근하여 HTML을 읽어서 돌려준다.\n",
    "    :param print_url:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    user_agent = \"'Mozilla/5.0\"\n",
    "    headers ={\"User-Agent\" : user_agent}\n",
    "\n",
    "    response = requests.get(print_url, headers=headers)\n",
    "    html = response.text\n",
    "\n",
    "    return html\n",
    "\n",
    "def write_html(output_file, html):\n",
    "    \"\"\"\n",
    "    주어진 HTML 텍스트를 출력 파일에 쓴다.\n",
    "    :param output_file:\n",
    "    :param html:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    output_file.write(\"{}\\n\".format(html))\n",
    "    output_file.write(\"@@@@@ ARTICLE DELMITER @@@@\\n\")\n",
    "\n",
    "def pause():\n",
    "    \"\"\"\n",
    "    3초동안 쉰다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    time.sleep(3)\n",
    "\n",
    "def close_output_file(output_file):\n",
    "    \"\"\"\n",
    "    출력 파일을 닫는다.\n",
    "    :param output_file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "def close_url_file(url_file):\n",
    "    \"\"\"\n",
    "    URL 파일을 닫는다.\n",
    "    :param url_file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    url_file.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    네이버 뉴스기사를 수집한다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    url_file_name = get_url_file_name()\n",
    "    output_file_name = get_output_file_name()\n",
    "\n",
    "    url_file = open_url_file(url_file_name)\n",
    "    output_file = create_output_file(output_file_name)\n",
    "\n",
    "    for line in url_file:\n",
    "        print_url = gen_print_url(line)\n",
    "        html = get_html(print_url)\n",
    "        write_html(output_file,html)\n",
    "\n",
    "    close_output_file(output_file)\n",
    "    close_url_file(url_file)\n",
    "\n",
    "main()\n",
    "#[출처] 증권뉴스데이터 수집(2/3편)|작성자 엉드루"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter HTML File name : society-20190511.txt\n",
      "Enter text file name : society-20190511contentsall.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "네이버 뉴스 기사 HTML에서 순수 텍스트 기사를 추출한다.\n",
    "\"\"\"\n",
    "\n",
    "import bs4\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "ARTICLE_DELIMITER = \"@@@@@ ARTICLE DELMITER @@@@\\n\"\n",
    "TITLE_START_PAT = '<h3 class=\"tit_view\" data-translation=\"\">'\n",
    "TITLE_END_PAT = '</h3>'\n",
    "DATE_TIME_START_PAT = '<span class=\"txt_info\">입력 </span>'\n",
    "BODY_START_PAT = '<p dmcf-pid=\"\" dmcf-ptype=\"\">'\n",
    "BODY_END_PAT = '</p>'\n",
    "TIDYUP_START_PAT = '<div class=\"foot_view\">'\n",
    "\n",
    "def get_html_file_name():\n",
    "    \"\"\"\n",
    "    사용자로 부터 HTML 파일 이름을 입력받아 돌려준다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    html_file_name = input(\"Enter HTML File name : \")\n",
    "\n",
    "    return html_file_name\n",
    "\n",
    "def get_text_file_name():\n",
    "    \"\"\"\n",
    "    사용자로부터 텍스트 파일 이름을 입력받아 돌려준다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    text_file_name = input(\"Enter text file name : \")\n",
    "\n",
    "    return text_file_name\n",
    "\n",
    "def open_html_file(html_file_name):\n",
    "    \"\"\"\n",
    "    HTML 기사 파일을 열어서 파일 객체를 돌려준다.\n",
    "    :param html_file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    html_file = open(html_file_name, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    return html_file\n",
    "\n",
    "def create_text_file(text_file_name):\n",
    "    \"\"\"\n",
    "    텍스트 기사 파일을 만들어 파일 객체를 돌려준다.\n",
    "    :param text_file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    text_file = open(text_file_name, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    return text_file\n",
    "\n",
    "def read_html_article(html_file):\n",
    "    \"\"\"\n",
    "    HTML 파일에서 기사 하나를 읽어서 돌려준다.\n",
    "    :param html_file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "    for line in html_file:\n",
    "        if line.startswith(ARTICLE_DELIMITER):\n",
    "            html_text = \"\".join(lines).strip()\n",
    "            return html_text\n",
    "        lines.append(line)\n",
    "\n",
    "    return None\n",
    "\n",
    "def ext_title(html_text):\n",
    "    \"\"\"\n",
    "    HTML 기사에서 제목을 추출하여 돌려준다.\n",
    "    :param html_text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    p = html_text.find(TITLE_START_PAT)\n",
    "    q = html_text.find(TITLE_END_PAT)\n",
    "    title = html_text[p + len(TITLE_START_PAT):q]\n",
    "    title = title.strip()\n",
    "\n",
    "    return title\n",
    "\n",
    "\n",
    "def ext_date_time(html_text):\n",
    "    \"\"\"\n",
    "    HTML 기사에서 날짜와 시간을 추출하여 돌려준다.\n",
    "    :param html_text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    start_p = html_text.find(DATE_TIME_START_PAT)+len(DATE_TIME_START_PAT)\n",
    "    end_p = start_p + 10\n",
    "    date_time = html_text[start_p:end_p]\n",
    "    date_time = date_time.strip()\n",
    "\n",
    "    return date_time\n",
    "\n",
    "def strip_html(html_body):\n",
    "    \"\"\"\n",
    "    HTML 본문에서 HTML 태그를 제거하고 돌려준다.\n",
    "    :param html_body:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    page = bs4.BeautifulSoup(html_body, \"html.parser\")\n",
    "    body = page.text\n",
    "\n",
    "    return body\n",
    "\n",
    "def tidyup(body):\n",
    "    \"\"\"\n",
    "    본문에서 필요없는 부분을 자르고 돌려준다.\n",
    "    :param body:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    p = body.find(TIDYUP_START_PAT)\n",
    "    body = body[:p]\n",
    "    body = body.strip()\n",
    "\n",
    "    return body\n",
    "\n",
    "def ext_body(html_text):\n",
    "    \"\"\"\n",
    "    HTML 기사에서 본문을 추출하여 돌려준다.\n",
    "    :param html_text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    p = html_text.find(BODY_START_PAT)\n",
    "    q = html_text.find(BODY_END_PAT)\n",
    "    html_body = html_text[p + len(BODY_START_PAT):q]\n",
    "    html_body = html_body.replace(\"<br />\",\"\\n\")\n",
    "    html_body = html_body.strip()\n",
    "    body = strip_html(html_body)\n",
    "    body = tidyup(body)\n",
    "\n",
    "    return body\n",
    "\n",
    "def write_article(text_file, title, date_time, body):\n",
    "    \"\"\"\n",
    "    텍스트 파일에 항목이 구분된 기사를 출력한다.\n",
    "    :param text_file:\n",
    "    :param title:\n",
    "    :param date_time:\n",
    "    :param body:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    text_file.write(\"{}\\n\".format(title))\n",
    "    text_file.write(\"{}\\n\".format(date_time))\n",
    "    text_file.write(\"{}\\n\".format(body))\n",
    "    text_file.write(\"{}\\n\".format(ARTICLE_DELIMITER))\n",
    "\n",
    "def main():{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"^C\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"!pip install newspaper3k\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 112,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"[포토] G2 '밀키' 미하엘, \\\"이제 IG만 남았어요\\\"\\n\",\n",
    "      \"13일 오후 베트남 하노이 국립 컨벤션센터에서 열린 MSI 그룹 스테이지 4일차 G2 e스포츠와 플래시 울브즈의 경기서 승리한 G2 '밀키' 미하엘 메흘레가 베트남어 방송 인터뷰를 하고 있다.\\n\",\n",
    "      \"\\n\",\n",
    "      \"하노이(베트남) ㅣ 김용우 기자 kenzi@fomos.co.kr\\n\",\n",
    "      \"\\n\",\n",
    "      \"포모스와 함께 즐기는 e스포츠, 게임 그 이상을 향해!\\n\",\n",
    "      \"\\n\",\n",
    "      \"Copyrights ⓒ FOMOS(http://www.fomos.kr) 무단 전재 및 재배포 금지\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from newspaper import Article\\n\",\n",
    "    \"\\n\",\n",
    "    \"'''\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513202543774\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513202526771\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513202442768\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513202100733\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201951713\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201912711\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201708688\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201646686\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201515670\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201343654\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201042627\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513200900613\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513200731602\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513200601595\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513200601594\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513201012624\\n\",\n",
    "    \"http://v.media.daum.net/v/20190513200300564\\n\",\n",
    "    \"'''\\n\",\n",
    "    \"\\n\",\n",
    "    \"url = 'http://v.media.daum.net/v/20190513202526771'\\n\",\n",
    "    \"a = Article(url, language='ko')\\n\",\n",
    "    \"a.download()\\n\",\n",
    "    \"a.parse()\\n\",\n",
    "    \"print(a.title)\\n\",\n",
    "    \"print(a.text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(\\\"F:/daumnews/sports/02.txt\\\", \\\"w\\\") as f:\\n\",\n",
    "    \"    f.write(a.text)\\n\",\n",
    "    \"f.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from newspaper import Article\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 29,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from bs4 import BeautifulSoup\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"\\n\",\n",
    "    \"html = download(\\\"https://media.daum.net/breakingnews/culture\\\")\\n\",\n",
    "    \"daumnews = BeautifulSoup(html.text, \\\"lxml\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 30,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"daumnewstitellists = daumnews.select(\\\"div > strong > a\\\")\\n\",\n",
    "    \"k = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"t = 18\\n\",\n",
    "    \"\\n\",\n",
    "    \"for links in daumnewstitellists:\\n\",\n",
    "    \"    l = links.get('href')\\n\",\n",
    "    \"    k.append(l)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i in range(0,17):\\n\",\n",
    "    \"    url = k[i]\\n\",\n",
    "    \"    a = Article(url, language='ko')\\n\",\n",
    "    \"    a.download()\\n\",\n",
    "    \"    a.parse()\\n\",\n",
    "    \"    with open(\\\"F:/daumnews/culture/%d.txt\\\" % int(i+t), \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"        f.write(a.title)\\n\",\n",
    "    \"        f.write(a.text)\\n\",\n",
    "    \"        f.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from bs4 import BeautifulSoup\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"\\n\",\n",
    "    \"html = download(\\\"https://media.daum.net/breakingnews/sports\\\")\\n\",\n",
    "    \"daumnews = BeautifulSoup(html.text, \\\"lxml\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"daumnewstitellists = daumnews.select(\\\"div > strong > a\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for links in daumnewstitellists:\\n\",\n",
    "    \"    #print(links.text)\\n\",\n",
    "    \"    print(links.get('href'))\\n\",\n",
    "    \"    #print()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def download(url, params={}, retries=3):\\n\",\n",
    "    \"    resp = None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    header = {\\\"user-agent\\\": \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36\\\"}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        resp = requests.get(url, params=params, headers = header)\\n\",\n",
    "    \"        resp.raise_for_status()\\n\",\n",
    "    \"    except requests.exceptions.HTTPError as e:\\n\",\n",
    "    \"        if 500 <= e.response.status_code < 600 and retries > 0:\\n\",\n",
    "    \"            print(retries)\\n\",\n",
    "    \"            resp = download(url, params, retries - 1)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(e.response.status_code)\\n\",\n",
    "    \"            print(e.response.reason)\\n\",\n",
    "    \"            print(e.request.headers)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return resp\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 117,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from newspaper import Article\\n\",\n",
    "    \"from bs4 import BeautifulSoup\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"\\n\",\n",
    "    \"html = download(\\\"https://media.daum.net/breakingnews/sports\\\")\\n\",\n",
    "    \"daumnews = BeautifulSoup(html.text, \\\"lxml\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 139,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"daumnewstitellists = daumnews.select(\\\"div > strong > a\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for links in daumnewstitellists:\\n\",\n",
    "    \"        b = links.get('href')\\n\",\n",
    "    \"        a = Article(b, language='ko')\\n\",\n",
    "    \"        a.download()\\n\",\n",
    "    \"        a.parse()        \\n\",\n",
    "    \"        with open(\\\"F:/daumnews/sports/01.txt\\\", \\\"w\\\") as f:\\n\",\n",
    "    \"            f.write(a.text)\\n\",\n",
    "    \"            f.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.7.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n",
    "\n",
    "    \"\"\"\n",
    "    네이트 뉴스 기사 HTML에서 순수 텍스트 기사를 추출한다.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    html_file_name = get_html_file_name()\n",
    "    text_file_name = get_text_file_name()\n",
    "    html_file = open_html_file(html_file_name)\n",
    "    text_file = create_text_file(text_file_name)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        html_text = read_html_article(html_file)\n",
    "\n",
    "        if not html_text:\n",
    "            break\n",
    "\n",
    "        title = ext_title(html_text)\n",
    "        date_time = ext_date_time(html_text)\n",
    "        body = ext_body(html_text)\n",
    "        write_article(text_file, title, date_time, body)\n",
    "\n",
    "    html_file.close()\n",
    "    text_file.close()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
