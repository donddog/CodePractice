{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 내용: Transformer 모델링\n",
    "\n",
    "## 1. Vocab: 한국어 음절 단위\n",
    "\n",
    "## 2. 데이터: 한국어 Q&A 문장\n",
    " - ex.)공무원 시험 죽을 거 같아 --> 철밥통 되기가 어디 쉽겠어요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation (초기 환경 세팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 데이터 확인\n",
    " - 출처: https://github.com/eagle705/pytorch-transformer-chatbot/tree/master/data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/train_chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/valid_chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 데이터 확보 \n",
    "# (2) 데이터 검증 \n",
    "# (3) 데이터 전처리 (Text Normalization, 띄어쓰기, 오타교정)\n",
    "# (4) Vocab 생성 (Tokenizer, Token 정의) # Token: 음절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 어휘사전 (Vocab) 생성 // (음절 단위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "SOS_TOKEN_ID = 2\n",
    "EOS_TOKEN_ID = 3\n",
    "\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "def create_vocab(train_path, valid_path, vocab_path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    with open(valid_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    vocab = set()\n",
    "    for sent in data:\n",
    "        for char in sent:\n",
    "            vocab.add(char)\n",
    "            \n",
    "    vocab_list = list(sorted(vocab))\n",
    "    \n",
    "    vocab_list.insert(0, PAD_TOKEN)\n",
    "    vocab_list.insert(1, UNK_TOKEN)\n",
    "    vocab_list.insert(2, SOS_TOKEN)\n",
    "    vocab_list.insert(3, EOS_TOKEN)\n",
    "    \n",
    "    print(vocab_list)\n",
    "\n",
    "    # 파일로 어휘사전 저장\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(vocab_list, indent=4, ensure_ascii=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<sos>', '<eos>', ' ', '!', '%', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'L', 'N', 'O', 'P', 'S', 'X', '_', 'a', 'c', 'g', 'j', 'k', 'n', 'o', 's', '~', '…', 'ㅊ', 'ㅋ', 'ㅎ', 'ㅜ', 'ㅠ', '가', '각', '간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '갚', '개', '객', '갠', '갯', '갱', '걍', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '겉', '게', '겐', '겜', '겟', '겠', '겨', '격', '겪', '견', '결', '겹', '겼', '경', '곁', '계', '곗', '고', '곡', '곤', '곧', '골', '곰', '곱', '곳', '공', '과', '관', '광', '괘', '괜', '괴', '교', '구', '국', '군', '굳', '굴', '굶', '굽', '굿', '궁', '궈', '권', '궜', '귀', '귄', '귈', '귐', '규', '균', '귤', '그', '극', '근', '글', '긁', '금', '급', '긋', '긍', '기', '긴', '길', '김', '깃', '깅', '깊', '까', '깍', '깎', '깐', '깔', '깜', '깝', '깠', '깡', '깨', '깬', '깰', '깼', '꺼', '꺽', '껀', '껄', '껏', '께', '껴', '꼈', '꼬', '꼭', '꼰', '꼴', '꼼', '꼿', '꽁', '꽂', '꽃', '꽈', '꽉', '꽝', '꽤', '꾸', '꾹', '꾼', '꿀', '꿈', '꿎', '꿔', '꿧', '꿨', '꿩', '꿰', '뀌', '뀐', '뀔', '끄', '끈', '끊', '끌', '끓', '끔', '끗', '끝', '끼', '낀', '낄', '낌', '나', '낙', '낚', '난', '날', '남', '납', '낫', '났', '낭', '낮', '낳', '내', '낸', '낼', '냄', '냅', '냈', '냉', '냐', '냥', '너', '넋', '넌', '널', '넓', '넘', '넛', '넣', '네', '넥', '넷', '넹', '녀', '녁', '년', '념', '녔', '녕', '노', '녹', '논', '놀', '놈', '놉', '농', '높', '놓', '놔', '놨', '뇌', '뇨', '뇽', '누', '눅', '눈', '눌', '눕', '눠', '눴', '뉴', '느', '는', '늘', '늙', '늠', '능', '늦', '늪', '니', '닉', '닌', '닐', '님', '닙', '닝', '다', '닥', '닦', '단', '닫', '달', '닭', '닮', '닳', '담', '답', '닷', '당', '닿', '대', '댄', '댈', '댓', '댔', '댜', '더', '덕', '던', '덜', '덤', '덥', '덧', '덩', '덮', '데', '덴', '델', '뎌', '뎠', '도', '독', '돈', '돋', '돌', '돕', '동', '돼', '됐', '되', '된', '될', '됨', '됩', '됬', '두', '둑', '둔', '둘', '둠', '둥', '둬', '뒀', '뒤', '뒷', '뒹', '듀', '드', '득', '든', '듣', '들', '듦', '듬', '듭', '듯', '등', '디', '딘', '딛', '딜', '딧', '딨', '딩', '딪', '따', '딱', '딴', '딸', '땀', '땅', '때', '땐', '땜', '땠', '땡', '떄', '떠', '떡', '떤', '떨', '떴', '떻', '떼', '뗄', '또', '똑', '똥', '뚝', '뚫', '뚱', '뛰', '뛴', '뜁', '뜨', '뜩', '뜬', '뜰', '뜸', '뜻', '띄', '띠', '띰', '띵', '라', '락', '란', '랄', '람', '랍', '랐', '랑', '랖', '래', '랙', '랜', '랠', '램', '랩', '랫', '랬', '랭', '량', '러', '런', '럴', '럼', '럽', '럿', '렀', '렁', '렇', '레', '렉', '렌', '렐', '렘', '렛', '렜', '려', '력', '련', '렬', '렴', '렵', '렷', '렸', '령', '례', '로', '록', '론', '롤', '롭', '롯', '롱', '뢰', '료', '루', '룩', '룰', '룸', '룽', '뤄', '류', '륜', '률', '르', '륵', '른', '를', '름', '릅', '릇', '릎', '리', '릭', '린', '릴', '림', '립', '릿', '링', '마', '막', '만', '많', '말', '맘', '맙', '맛', '망', '맞', '맡', '매', '맥', '맨', '맴', '맷', '맹', '맺', '머', '먹', '먼', '멀', '멈', '멋', '멍', '메', '멘', '멤', '며', '면', '명', '몇', '모', '목', '몫', '몬', '몰', '몸', '몹', '못', '몽', '묘', '무', '묵', '문', '묻', '물', '뭇', '뭉', '뭐', '뭔', '뭘', '뭣', '뮤', '미', '민', '믿', '밀', '밉', '밌', '밍', '밑', '바', '박', '밖', '반', '받', '발', '밝', '밟', '밤', '밥', '방', '밭', '배', '백', '밸', '뱃', '뱄', '뱉', '버', '벅', '번', '벋', '벌', '범', '법', '벗', '벚', '베', '벤', '벨', '벴', '벼', '벽', '변', '별', '볍', '병', '볕', '보', '복', '볶', '본', '볼', '봄', '봅', '봇', '봉', '봐', '봤', '뵈', '부', '북', '분', '불', '붓', '붕', '붙', '뷔', '브', '블', '비', '빈', '빌', '빔', '빗', '빙', '빚', '빛', '빠', '빡', '빨', '빴', '빵', '빼', '빽', '뺄', '뺏', '뺴', '뻐', '뻑', '뻔', '뻘', '뻣', '뻤', '뻥', '뽀', '뽑', '뽕', '뿅', '뿌', '뿍', '뿐', '쁘', '쁜', '쁠', '쁨', '삐', '삔', '사', '삭', '산', '살', '삶', '삼', '삽', '삿', '샀', '상', '새', '색', '샌', '샐', '샘', '샜', '생', '샤', '서', '석', '섞', '선', '섣', '설', '섬', '섭', '섯', '섰', '성', '세', '섹', '센', '셀', '셔', '션', '셥', '셨', '소', '속', '손', '솔', '솜', '송', '쇠', '쇼', '숍', '숏', '수', '숙', '순', '술', '숨', '숫', '숭', '쉬', '쉴', '쉼', '쉽', '슈', '스', '슨', '슬', '슴', '습', '슷', '승', '시', '식', '신', '실', '싫', '심', '십', '싱', '싶', '싸', '싹', '싼', '쌀', '쌈', '쌍', '쌓', '쌤', '쌩', '써', '썩', '썰', '썸', '썹', '썼', '쎄', '쎈', '쎌', '쏘', '쏜', '쏟', '쏠', '쐬', '쑤', '쑥', '쓰', '쓴', '쓸', '씀', '씁', '씌', '씨', '씩', '씬', '씰', '씸', '씹', '씻', '씽', '아', '악', '안', '앉', '않', '알', '압', '앗', '았', '앙', '앞', '애', '액', '앨', '앱', '야', '약', '얄', '얇', '양', '얘', '어', '억', '언', '얻', '얼', '얽', '엄', '업', '없', '엇', '었', '엉', '엊', '에', '엔', '엘', '엠', '엣', '여', '역', '엮', '연', '열', '염', '엽', '엿', '였', '영', '옆', '옇', '예', '옛', '오', '옥', '온', '올', '옮', '옳', '옴', '옵', '옷', '와', '완', '왓', '왔', '왕', '왜', '왠', '왤', '외', '왼', '욌', '요', '욕', '욜', '용', '우', '욱', '운', '울', '움', '웁', '웃', '워', '원', '월', '웠', '웨', '웬', '웹', '웽', '위', '윗', '윙', '유', '육', '윤', '율', '으', '은', '을', '음', '응', '의', '이', '익', '인', '일', '읽', '잃', '임', '입', '잇', '있', '잊', '잌', '자', '작', '잔', '잖', '잘', '잠', '잡', '잤', '장', '잦', '재', '잼', '잿', '쟁', '저', '적', '전', '절', '젊', '점', '접', '젔', '정', '젖', '제', '젝', '젠', '젤', '젯', '져', '젹', '졋', '졌', '조', '족', '존', '졸', '좀', '좁', '종', '좋', '좌', '죄', '죠', '주', '죽', '준', '줄', '줌', '줍', '중', '줘', '줬', '쥐', '쥬', '즈', '즉', '즐', '즘', '즙', '증', '지', '직', '진', '질', '짐', '집', '짓', '징', '짚', '짜', '짝', '짠', '짤', '짧', '짬', '짰', '짱', '째', '짼', '쨌', '쩌', '쩍', '쩐', '쩔', '쩝', '쩡', '쪄', '쪘', '쪼', '쪽', '쫄', '쫌', '쫙', '쭈', '쭉', '쭤', '쯤', '찌', '찍', '찐', '찔', '찜', '찝', '찡', '찢', '차', '착', '찬', '찮', '찰', '참', '찹', '찼', '창', '찾', '채', '책', '챔', '챗', '챘', '챙', '처', '척', '천', '철', '첨', '첩', '첫', '청', '체', '쳇', '쳐', '쳤', '초', '촉', '촌', '총', '촬', '최', '추', '축', '춘', '출', '춤', '춥', '충', '춰', '췄', '취', '츄', '츠', '측', '츤', '층', '치', '칙', '친', '칠', '침', '칩', '칫', '칭', '카', '칼', '캄', '캐', '캔', '캬', '커', '컥', '컨', '컴', '컷', '컸', '컹', '케', '켓', '켜', '켠', '켰', '코', '콕', '콘', '콜', '콤', '콧', '콩', '쾌', '쿠', '쿨', '쿵', '쿼', '퀴', '큐', '크', '큰', '클', '큼', '킁', '키', '킥', '킨', '킬', '킴', '킹', '타', '탁', '탄', '탈', '탐', '탑', '탓', '탔', '탕', '태', '택', '탱', '터', '턱', '턴', '털', '텀', '텁', '텄', '텅', '테', '텍', '텐', '텔', '템', '텨', '텻', '텼', '토', '톡', '톤', '톱', '통', '퇴', '투', '툭', '툰', '툴', '툼', '퉁', '퉜', '튀', '튜', '트', '특', '틀', '틈', '틋', '티', '틱', '팀', '팁', '팅', '파', '팍', '판', '팔', '팠', '패', '팩', '팬', '팸', '퍼', '펑', '페', '펜', '펨', '펭', '펴', '편', '펼', '평', '폐', '포', '폭', '폰', '폼', '퐈', '표', '푸', '푹', '푼', '풀', '품', '풋', '풍', '퓨', '프', '픈', '플', '픔', '픕', '피', '픽', '핀', '필', '핍', '핏', '핑', '하', '학', '한', '할', '함', '합', '핫', '항', '해', '핸', '햇', '했', '행', '햐', '향', '허', '헉', '헌', '헐', '험', '헛', '헤', '헥', '헬', '헷', '헹', '혀', '현', '혈', '혐', '협', '혔', '형', '혜', '호', '혹', '혼', '홀', '홈', '화', '확', '환', '활', '홧', '황', '회', '획', '효', '후', '훅', '훈', '훌', '훔', '훨', '휘', '휙', '휨', '휴', '흐', '흑', '흔', '흘', '흠', '흡', '흥', '희', '흰', '히', '힌', '힐', '힘', '힙']\n"
     ]
    }
   ],
   "source": [
    "create_vocab(train_path=\"./data/train_chatbot.txt\",\n",
    "             valid_path=\"./data/valid_chatbot.txt\",\n",
    "             vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnADataset(Dataset):\n",
    "    def __init__(self, data_path, vocab_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 전처리 단계\n",
    "        self.char2index, self.index2char = self._read_vocab(vocab_path)\n",
    "        self.data = self._preprocess(data_path)\n",
    "    \n",
    "    def _read_vocab(self, vocab_path):\n",
    "        with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "            labels = json.load(f)\n",
    "            char2index = dict()\n",
    "            index2char = dict()\n",
    "\n",
    "            for index, char in enumerate(labels):\n",
    "                char2index[char] = index\n",
    "                index2char[index] = char\n",
    "            \n",
    "        return char2index, index2char\n",
    "    \n",
    "    def _preprocess(self, data_path):\n",
    "        data = []\n",
    "        \n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                sents = line.strip().split('\\t')\n",
    "                assert len(sents) == 2, \"data error!!\"\n",
    "                question_sent, answer_sent = sents[0], sents[1]\n",
    "                \n",
    "                data.append((question_sent, answer_sent))\n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.char2index)\n",
    "    \n",
    "    # 필수 구현\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # 필수 구현\n",
    "    def __getitem__(self, index):\n",
    "        qna = self.data[index]\n",
    "        q_sent, a_sent = qna[0], qna[1]\n",
    "        \n",
    "        src = [self.char2index.get(SOS_TOKEN)]\n",
    "        src += [self.char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        tgt = [self.char2index.get(SOS_TOKEN)]\n",
    "        tgt += [self.char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        tgt += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        return torch.LongTensor(src), torch.LongTensor(tgt)\n",
    "    \n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    xs = [x for x, y in batch]\n",
    "    xs_pad = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    xs_lengths = [x.size(0) for x, y in batch]\n",
    "    xs_lengths = torch.LongTensor(xs_lengths)\n",
    "\n",
    "    ys = [y for x, y in batch]\n",
    "    ys_pad = torch.nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    ys_lengths = [y.size(0) for x, y in batch]\n",
    "    ys_lengths = torch.LongTensor(ys_lengths)\n",
    "\n",
    "    return xs_pad, xs_lengths, ys_pad, ys_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QnADataset(data_path=\"./data/train_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")\n",
    "\n",
    "valid_dataset = QnADataset(data_path=\"./data/valid_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]  # train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   69,  936,  ...,    0,    0,    0],\n",
      "        [   2,  779,  478,  ...,    0,    0,    0],\n",
      "        [   2,  822,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  205,  465,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,    0,    0,    0],\n",
      "        [   2,  805,  268,  ...,    0,    0,    0]]) tensor([[   2,  932,  707,  ...,    0,    0,    0],\n",
      "        [   2,  704,  773,  ...,    0,    0,    0],\n",
      "        [   2,  648,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  647,  908,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,  825,   10,    3],\n",
      "        [   2,  129,  480,  ...,    0,    0,    0]])\n",
      "tensor([14, 14, 15, 15, 15, 12, 11, 14,  9, 11, 10,  8, 10, 16, 13, 40, 17, 13,\n",
      "        24, 11, 11,  4, 13,  8, 16, 35, 27, 20, 17, 19, 11, 12]) tensor([20, 18, 17, 10, 21, 17, 14, 24, 10, 13, 15, 14, 15, 12, 24, 20, 13, 22,\n",
      "        25, 12, 15, 13, 14, 24, 11, 22, 21, 30, 15, 10, 32, 19])\n"
     ]
    }
   ],
   "source": [
    "for x, x_len, y, y_len in train_loader:\n",
    "    print(x, y)\n",
    "    print(x_len, y_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_id=0):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(vocab_size=vocab_size, \n",
    "                                              embed_size=embed_size, \n",
    "                                              pad_id=pad_id)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embed_size, \n",
    "                                                 max_len=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_enc = self.pos_encoding(x)\n",
    "        return token_emb + pos_enc\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_id):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.token_embedding(x)\n",
    "        return x_embed\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    ref: https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, pad_id, device):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        src_vocab_size = input_size\n",
    "        tgt_vocab_size = output_size\n",
    "\n",
    "        d_model = 512\n",
    "        n_head = 8\n",
    "        num_encoder_layers = 6\n",
    "        num_decoder_layers = 6\n",
    "        dim_feedforward = 2048\n",
    "        dropout = 0.1\n",
    "\n",
    "        self.encoder_dropout = nn.Dropout(dropout)\n",
    "        self.decoder_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder_embedding = Embeddings(vocab_size=src_vocab_size, embed_size=d_model, pad_id=pad_id) # embed_size = d_model\n",
    "        self.decoder_embedding = Embeddings(vocab_size=tgt_vocab_size, embed_size=d_model, pad_id=pad_id)\n",
    "        \n",
    "        self.transformer = torch.nn.Transformer(d_model=d_model,\n",
    "                                                nhead=n_head,\n",
    "                                                num_encoder_layers=num_encoder_layers,\n",
    "                                                num_decoder_layers=num_decoder_layers,\n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout)\n",
    "\n",
    "        self.proj_vocab_layer = nn.Linear(in_features=d_model, out_features=tgt_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def _generate_mask(self, x, length):\n",
    "        mask = []\n",
    "\n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # If the length is shorter than maximum length among samples,\n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                    x.new_ones(1, (max_length - l))\n",
    "                                    ], dim=-1)]\n",
    "            else:\n",
    "                # If the length of the sample equals to maximum length among samples,\n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "        mask = torch.cat(mask, dim=0).bool()\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, enc_input, dec_input, enc_input_len, dec_input_len):\n",
    "        \n",
    "        x_enc_embed = self.encoder_dropout(self.encoder_embedding(enc_input.long()))\n",
    "        x_dec_embed = self.decoder_dropout(self.decoder_embedding(dec_input.long()))\n",
    "        # print(x_enc_embed.size(), x_enc_embed)\n",
    "        # print(x_dec_embed.size(), x_dec_embed)\n",
    "\n",
    "        src_key_padding_mask = self._generate_mask(enc_input, enc_input_len).to(self.device)\n",
    "        tgt_key_padding_mask = self._generate_mask(dec_input, dec_input_len).to(self.device)\n",
    "        # print(enc_input_len, src_key_padding_mask)\n",
    "        # print(dec_input_len, tgt_key_padding_mask)\n",
    "        \n",
    "        memory_key_padding_mask = src_key_padding_mask\n",
    "        # src_mask = self.transformer.generate_square_subsequent_mask(x_enc_embed.size(1)).to(self.device)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(x_dec_embed.size(1)).to(self.device)\n",
    "        \n",
    "        # print(x_enc_embed.size(1), src_mask)\n",
    "        # print(x_dec_embed.size(1), tgt_mask)\n",
    "        \n",
    "        x_enc_embed = x_enc_embed.transpose(0, 1)\n",
    "        x_dec_embed = x_dec_embed.transpose(0, 1)\n",
    "        \n",
    "\n",
    "        feature = self.transformer(src=x_enc_embed,\n",
    "                                   tgt=x_dec_embed,\n",
    "                                   src_key_padding_mask=src_key_padding_mask,\n",
    "                                   tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                   memory_key_padding_mask=memory_key_padding_mask,\n",
    "                                   src_mask=None,\n",
    "                                   tgt_mask=tgt_mask)\n",
    "\n",
    "        logits = self.proj_vocab_layer(feature)\n",
    "        logits = self.softmax(logits)\n",
    "        logits = logits.transpose(0, 1)\n",
    "        # print(logits.size(), logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def search(self, enc_input, max_length=255, sos_id=2, eos_id=3):\n",
    "        \n",
    "        SOS_token = sos_id\n",
    "        EOS_token = eos_id\n",
    "        \n",
    "        y_hats, indice = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # ENCODER\n",
    "            x_enc_embed = self.encoder_dropout(self.encoder_embedding(enc_input.long()))\n",
    "            # src_mask = self.transformer.generate_square_subsequent_mask(x_enc_embed.size(1)).to(self.device)\n",
    "            \n",
    "            enc_input_len = torch.LongTensor([enc_input.size(1)])\n",
    "            src_key_padding_mask = self._generate_mask(enc_input, enc_input_len).to(self.device)\n",
    "            \n",
    "            memory_key_padding_mask = src_key_padding_mask\n",
    "            \n",
    "            x_enc_embed = x_enc_embed.transpose(0, 1)\n",
    "            src = x_enc_embed\n",
    "            \n",
    "            memory = self.transformer.encoder(src,\n",
    "                                              mask=None,\n",
    "                                              src_key_padding_mask=src_key_padding_mask)\n",
    "            \n",
    "            # DECODER\n",
    "            dec_input = torch.LongTensor([[SOS_token]]).to(self.device)\n",
    "            dec_input_len = torch.LongTensor([dec_input.size(-1)]).to(self.device)\n",
    "            \n",
    "            for di in range(max_length):\n",
    "                x_dec_embed = self.decoder_dropout(self.decoder_embedding(dec_input.long()))\n",
    "                tgt_mask = self.transformer.generate_square_subsequent_mask(x_dec_embed.size(1)).to(self.device)\n",
    "                tgt_key_padding_mask = self._generate_mask(dec_input, dec_input_len).to(self.device)\n",
    "\n",
    "                x_dec_embed = x_dec_embed.transpose(0, 1)\n",
    "                tgt = x_dec_embed\n",
    "\n",
    "                output = self.transformer.decoder(tgt,\n",
    "                                                  memory,\n",
    "                                                  tgt_mask=tgt_mask,\n",
    "                                                  memory_mask=None,\n",
    "                                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "                logits = self.proj_vocab_layer(output)\n",
    "                logits = self.softmax(logits)\n",
    "                y_pred = logits.transpose(0, 1)\n",
    "                y_hats += [y_pred]\n",
    "\n",
    "                y_pred_ids = y_pred.max(dim=-1)[1]\n",
    "                # print(\"y_pred_ids : \", y_pred_ids)\n",
    "                # indice += [y_pred_ids]\n",
    "\n",
    "                dec_input = torch.cat([dec_input, y_pred_ids[0,-1].unsqueeze(0).unsqueeze(0)], dim=-1).to(self.device)\n",
    "                # print(\"({}) dec_input: {}\".format(di, dec_input))\n",
    "\n",
    "                dec_input_len = torch.LongTensor([dec_input.size(-1)]).to(self.device)\n",
    "                \n",
    "                if y_pred_ids[0,-1].item() == EOS_token:\n",
    "                    break\n",
    "            \n",
    "            y_hats = torch.cat(y_hats, dim=1)\n",
    "            \n",
    "            indice = dec_input[:, 1:]\n",
    "    \n",
    "        return y_hats, indice\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246 1246\n",
      "Transformer(\n",
      "  (encoder_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (decoder_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (encoder_embedding): Embeddings(\n",
      "    (token_embedding): TokenEmbedding(\n",
      "      (token_embedding): Embedding(1246, 512, padding_idx=0)\n",
      "    )\n",
      "    (pos_encoding): PositionalEncoding()\n",
      "  )\n",
      "  (decoder_embedding): Embeddings(\n",
      "    (token_embedding): TokenEmbedding(\n",
      "      (token_embedding): Embedding(1246, 512, padding_idx=0)\n",
      "    )\n",
      "    (pos_encoding): PositionalEncoding()\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (4): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (5): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_vocab_layer): Linear(in_features=512, out_features=1246, bias=True)\n",
      "  (softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = train_dataset.vocab_size\n",
    "output_size = train_dataset.vocab_size\n",
    "\n",
    "print(input_size, output_size)\n",
    "\n",
    "model = Transformer(input_size=input_size, \n",
    "                    output_size=output_size,\n",
    "                    pad_id=PAD_TOKEN_ID, \n",
    "                    device=device).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.0001  # 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=PAD_TOKEN_ID).to(device)\n",
    "\n",
    "# log_softmax + NLLLoss == CrossEntropyLoss\n",
    "criterion = torch.nn.NLLLoss(reduction='mean', ignore_index=PAD_TOKEN_ID).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, clip, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        \n",
    "        src, src_len, trg, trg_len = batch\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        src_len = src_len.to(device)\n",
    "        trg_len = trg_len.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(enc_input=src,\n",
    "                       dec_input=trg[:, :-1],\n",
    "                       enc_input_len=src_len,\n",
    "                       dec_input_len=(trg_len-1))\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        log_interval = 100\n",
    "        if i % log_interval == 0 and i >= 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | loss {:.4f}'.format(epoch+1, i+1, len(data_loader), loss.detach().item()))\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "\n",
    "            src, src_len, trg, trg_len = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            src_len = src_len.to(device)\n",
    "            trg_len = trg_len.to(device)\n",
    "\n",
    "            output = model(enc_input=src,\n",
    "                           dec_input=trg[:, :-1],\n",
    "                           enc_input_len=src_len,\n",
    "                           dec_input_len=(trg_len-1))\n",
    "\n",
    "            #trg = [batch size, trg len]\n",
    "            #output = [batch size, trg len, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "        \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  351 batches | loss 7.3256\n",
      "| epoch   1 |   101/  351 batches | loss 3.4596\n",
      "| epoch   1 |   201/  351 batches | loss 3.2762\n",
      "| epoch   1 |   301/  351 batches | loss 2.8678\n",
      "Epoch: 01 | train.loss.best: 1\n",
      "Epoch: 01 | valid.loss.best: 1\n",
      "Epoch: 01 | Time: 0m 50s\n",
      "\tTrain Loss: 3.4564 | Train PPL:  31.7038\n",
      "\t Val. Loss: 2.7499 |  Val. PPL:  15.6408\n",
      "####################################################################################################\n",
      "| epoch   2 |     1/  351 batches | loss 2.7533\n",
      "| epoch   2 |   101/  351 batches | loss 2.7647\n",
      "| epoch   2 |   201/  351 batches | loss 2.5173\n",
      "| epoch   2 |   301/  351 batches | loss 2.7535\n",
      "Epoch: 02 | train.loss.best: 2\n",
      "Epoch: 02 | valid.loss.best: 2\n",
      "Epoch: 02 | Time: 0m 49s\n",
      "\tTrain Loss: 2.6090 | Train PPL:  13.5851\n",
      "\t Val. Loss: 2.4009 |  Val. PPL:  11.0330\n",
      "####################################################################################################\n",
      "| epoch   3 |     1/  351 batches | loss 2.3137\n",
      "| epoch   3 |   101/  351 batches | loss 2.3309\n",
      "| epoch   3 |   201/  351 batches | loss 2.2465\n",
      "| epoch   3 |   301/  351 batches | loss 2.2806\n",
      "Epoch: 03 | train.loss.best: 3\n",
      "Epoch: 03 | valid.loss.best: 3\n",
      "Epoch: 03 | Time: 0m 49s\n",
      "\tTrain Loss: 2.3301 | Train PPL:  10.2791\n",
      "\t Val. Loss: 2.2264 |  Val. PPL:   9.2663\n",
      "####################################################################################################\n",
      "| epoch   4 |     1/  351 batches | loss 2.1316\n",
      "| epoch   4 |   101/  351 batches | loss 2.2272\n",
      "| epoch   4 |   201/  351 batches | loss 2.1177\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100 # 25\n",
    "CLIP = 5\n",
    "\n",
    "best_train_loss = float('inf')\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, epoch)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), './models/transformer/train.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | train.loss.best: {epoch+1}')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/transformer/valid.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | valid.loss.best: {epoch+1}')\n",
    "        \n",
    "    # lr_scheduler.step(valid_loss)\n",
    "    lr_scheduler.step(train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):8.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {math.exp(valid_loss):8.4f}')\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/transformer/train.loss.best.pt'))\n",
    "\n",
    "test_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):8.4f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference (검증용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, q_sent=\"\", a_sent=None, char2index=None, index2char=None):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        src = [char2index.get(SOS_TOKEN)]\n",
    "        src += [char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        trg = [char2index.get(SOS_TOKEN)]\n",
    "        trg += [char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        trg += [char2index.get(EOS_TOKEN)]\n",
    "\n",
    "        src = torch.LongTensor([src]).to(device)\n",
    "        trg = torch.LongTensor([trg]).to(device)\n",
    "\n",
    "        hyp_ys, hyp_indice = model.search(enc_input=src, max_length=80)\n",
    "        \n",
    "        pred = hyp_indice[0].detach().cpu().numpy()\n",
    "        print(pred)\n",
    "        \n",
    "        pred_sent = [index2char[token_id] for token_id in pred]\n",
    "        pred_sent = ''.join(pred_sent)\n",
    "        \n",
    "        print(f\"H: ({pred_sent})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(train_dataset), size=1)[0]\n",
    "print(\"random idx : \", idx)\n",
    "\n",
    "q_sent = train_dataset.data[idx][0]\n",
    "a_sent = train_dataset.data[idx][1]\n",
    "print(\"Q: \", q_sent)\n",
    "print(\"A: \", a_sent)\n",
    "\n",
    "inference(model, q_sent, a_sent, char2index=train_dataset.char2index, index2char=train_dataset.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
