{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 내용: Transformer 모델링\n",
    "\n",
    "## 1. Vocab: 한국어 음절 단위\n",
    "\n",
    "## 2. 데이터: 한국어 Q&A 문장\n",
    " - ex.)공무원 시험 죽을 거 같아 --> 철밥통 되기가 어디 쉽겠어요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation (초기 환경 세팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 데이터 확인\n",
    " - 출처: https://github.com/eagle705/pytorch-transformer-chatbot/tree/master/data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/train_chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/valid_chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 데이터 확보 \n",
    "# (2) 데이터 검증 \n",
    "# (3) 데이터 전처리 (Text Normalization, 띄어쓰기, 오타교정)\n",
    "# (4) Vocab 생성 (Tokenizer, Token 정의) # Token: 음절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 어휘사전 (Vocab) 생성 // (음절 단위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "SOS_TOKEN_ID = 2\n",
    "EOS_TOKEN_ID = 3\n",
    "\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "def create_vocab(train_path, valid_path, vocab_path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    with open(valid_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    vocab = set()\n",
    "    for sent in data:\n",
    "        for char in sent:\n",
    "            vocab.add(char)\n",
    "            \n",
    "    vocab_list = list(sorted(vocab))\n",
    "    \n",
    "    vocab_list.insert(0, PAD_TOKEN)\n",
    "    vocab_list.insert(1, UNK_TOKEN)\n",
    "    vocab_list.insert(2, SOS_TOKEN)\n",
    "    vocab_list.insert(3, EOS_TOKEN)\n",
    "    \n",
    "    print(vocab_list)\n",
    "\n",
    "    # 파일로 어휘사전 저장\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(vocab_list, indent=4, ensure_ascii=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<sos>', '<eos>', ' ', '!', '%', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'L', 'N', 'O', 'P', 'S', 'X', '_', 'a', 'c', 'g', 'j', 'k', 'n', 'o', 's', '~', '…', 'ㅊ', 'ㅋ', 'ㅎ', 'ㅜ', 'ㅠ', '가', '각', '간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '갚', '개', '객', '갠', '갯', '갱', '걍', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '겉', '게', '겐', '겜', '겟', '겠', '겨', '격', '겪', '견', '결', '겹', '겼', '경', '곁', '계', '곗', '고', '곡', '곤', '곧', '골', '곰', '곱', '곳', '공', '과', '관', '광', '괘', '괜', '괴', '교', '구', '국', '군', '굳', '굴', '굶', '굽', '굿', '궁', '궈', '권', '궜', '귀', '귄', '귈', '귐', '규', '균', '귤', '그', '극', '근', '글', '긁', '금', '급', '긋', '긍', '기', '긴', '길', '김', '깃', '깅', '깊', '까', '깍', '깎', '깐', '깔', '깜', '깝', '깠', '깡', '깨', '깬', '깰', '깼', '꺼', '꺽', '껀', '껄', '껏', '께', '껴', '꼈', '꼬', '꼭', '꼰', '꼴', '꼼', '꼿', '꽁', '꽂', '꽃', '꽈', '꽉', '꽝', '꽤', '꾸', '꾹', '꾼', '꿀', '꿈', '꿎', '꿔', '꿧', '꿨', '꿩', '꿰', '뀌', '뀐', '뀔', '끄', '끈', '끊', '끌', '끓', '끔', '끗', '끝', '끼', '낀', '낄', '낌', '나', '낙', '낚', '난', '날', '남', '납', '낫', '났', '낭', '낮', '낳', '내', '낸', '낼', '냄', '냅', '냈', '냉', '냐', '냥', '너', '넋', '넌', '널', '넓', '넘', '넛', '넣', '네', '넥', '넷', '넹', '녀', '녁', '년', '념', '녔', '녕', '노', '녹', '논', '놀', '놈', '놉', '농', '높', '놓', '놔', '놨', '뇌', '뇨', '뇽', '누', '눅', '눈', '눌', '눕', '눠', '눴', '뉴', '느', '는', '늘', '늙', '늠', '능', '늦', '늪', '니', '닉', '닌', '닐', '님', '닙', '닝', '다', '닥', '닦', '단', '닫', '달', '닭', '닮', '닳', '담', '답', '닷', '당', '닿', '대', '댄', '댈', '댓', '댔', '댜', '더', '덕', '던', '덜', '덤', '덥', '덧', '덩', '덮', '데', '덴', '델', '뎌', '뎠', '도', '독', '돈', '돋', '돌', '돕', '동', '돼', '됐', '되', '된', '될', '됨', '됩', '됬', '두', '둑', '둔', '둘', '둠', '둥', '둬', '뒀', '뒤', '뒷', '뒹', '듀', '드', '득', '든', '듣', '들', '듦', '듬', '듭', '듯', '등', '디', '딘', '딛', '딜', '딧', '딨', '딩', '딪', '따', '딱', '딴', '딸', '땀', '땅', '때', '땐', '땜', '땠', '땡', '떄', '떠', '떡', '떤', '떨', '떴', '떻', '떼', '뗄', '또', '똑', '똥', '뚝', '뚫', '뚱', '뛰', '뛴', '뜁', '뜨', '뜩', '뜬', '뜰', '뜸', '뜻', '띄', '띠', '띰', '띵', '라', '락', '란', '랄', '람', '랍', '랐', '랑', '랖', '래', '랙', '랜', '랠', '램', '랩', '랫', '랬', '랭', '량', '러', '런', '럴', '럼', '럽', '럿', '렀', '렁', '렇', '레', '렉', '렌', '렐', '렘', '렛', '렜', '려', '력', '련', '렬', '렴', '렵', '렷', '렸', '령', '례', '로', '록', '론', '롤', '롭', '롯', '롱', '뢰', '료', '루', '룩', '룰', '룸', '룽', '뤄', '류', '륜', '률', '르', '륵', '른', '를', '름', '릅', '릇', '릎', '리', '릭', '린', '릴', '림', '립', '릿', '링', '마', '막', '만', '많', '말', '맘', '맙', '맛', '망', '맞', '맡', '매', '맥', '맨', '맴', '맷', '맹', '맺', '머', '먹', '먼', '멀', '멈', '멋', '멍', '메', '멘', '멤', '며', '면', '명', '몇', '모', '목', '몫', '몬', '몰', '몸', '몹', '못', '몽', '묘', '무', '묵', '문', '묻', '물', '뭇', '뭉', '뭐', '뭔', '뭘', '뭣', '뮤', '미', '민', '믿', '밀', '밉', '밌', '밍', '밑', '바', '박', '밖', '반', '받', '발', '밝', '밟', '밤', '밥', '방', '밭', '배', '백', '밸', '뱃', '뱄', '뱉', '버', '벅', '번', '벋', '벌', '범', '법', '벗', '벚', '베', '벤', '벨', '벴', '벼', '벽', '변', '별', '볍', '병', '볕', '보', '복', '볶', '본', '볼', '봄', '봅', '봇', '봉', '봐', '봤', '뵈', '부', '북', '분', '불', '붓', '붕', '붙', '뷔', '브', '블', '비', '빈', '빌', '빔', '빗', '빙', '빚', '빛', '빠', '빡', '빨', '빴', '빵', '빼', '빽', '뺄', '뺏', '뺴', '뻐', '뻑', '뻔', '뻘', '뻣', '뻤', '뻥', '뽀', '뽑', '뽕', '뿅', '뿌', '뿍', '뿐', '쁘', '쁜', '쁠', '쁨', '삐', '삔', '사', '삭', '산', '살', '삶', '삼', '삽', '삿', '샀', '상', '새', '색', '샌', '샐', '샘', '샜', '생', '샤', '서', '석', '섞', '선', '섣', '설', '섬', '섭', '섯', '섰', '성', '세', '섹', '센', '셀', '셔', '션', '셥', '셨', '소', '속', '손', '솔', '솜', '송', '쇠', '쇼', '숍', '숏', '수', '숙', '순', '술', '숨', '숫', '숭', '쉬', '쉴', '쉼', '쉽', '슈', '스', '슨', '슬', '슴', '습', '슷', '승', '시', '식', '신', '실', '싫', '심', '십', '싱', '싶', '싸', '싹', '싼', '쌀', '쌈', '쌍', '쌓', '쌤', '쌩', '써', '썩', '썰', '썸', '썹', '썼', '쎄', '쎈', '쎌', '쏘', '쏜', '쏟', '쏠', '쐬', '쑤', '쑥', '쓰', '쓴', '쓸', '씀', '씁', '씌', '씨', '씩', '씬', '씰', '씸', '씹', '씻', '씽', '아', '악', '안', '앉', '않', '알', '압', '앗', '았', '앙', '앞', '애', '액', '앨', '앱', '야', '약', '얄', '얇', '양', '얘', '어', '억', '언', '얻', '얼', '얽', '엄', '업', '없', '엇', '었', '엉', '엊', '에', '엔', '엘', '엠', '엣', '여', '역', '엮', '연', '열', '염', '엽', '엿', '였', '영', '옆', '옇', '예', '옛', '오', '옥', '온', '올', '옮', '옳', '옴', '옵', '옷', '와', '완', '왓', '왔', '왕', '왜', '왠', '왤', '외', '왼', '욌', '요', '욕', '욜', '용', '우', '욱', '운', '울', '움', '웁', '웃', '워', '원', '월', '웠', '웨', '웬', '웹', '웽', '위', '윗', '윙', '유', '육', '윤', '율', '으', '은', '을', '음', '응', '의', '이', '익', '인', '일', '읽', '잃', '임', '입', '잇', '있', '잊', '잌', '자', '작', '잔', '잖', '잘', '잠', '잡', '잤', '장', '잦', '재', '잼', '잿', '쟁', '저', '적', '전', '절', '젊', '점', '접', '젔', '정', '젖', '제', '젝', '젠', '젤', '젯', '져', '젹', '졋', '졌', '조', '족', '존', '졸', '좀', '좁', '종', '좋', '좌', '죄', '죠', '주', '죽', '준', '줄', '줌', '줍', '중', '줘', '줬', '쥐', '쥬', '즈', '즉', '즐', '즘', '즙', '증', '지', '직', '진', '질', '짐', '집', '짓', '징', '짚', '짜', '짝', '짠', '짤', '짧', '짬', '짰', '짱', '째', '짼', '쨌', '쩌', '쩍', '쩐', '쩔', '쩝', '쩡', '쪄', '쪘', '쪼', '쪽', '쫄', '쫌', '쫙', '쭈', '쭉', '쭤', '쯤', '찌', '찍', '찐', '찔', '찜', '찝', '찡', '찢', '차', '착', '찬', '찮', '찰', '참', '찹', '찼', '창', '찾', '채', '책', '챔', '챗', '챘', '챙', '처', '척', '천', '철', '첨', '첩', '첫', '청', '체', '쳇', '쳐', '쳤', '초', '촉', '촌', '총', '촬', '최', '추', '축', '춘', '출', '춤', '춥', '충', '춰', '췄', '취', '츄', '츠', '측', '츤', '층', '치', '칙', '친', '칠', '침', '칩', '칫', '칭', '카', '칼', '캄', '캐', '캔', '캬', '커', '컥', '컨', '컴', '컷', '컸', '컹', '케', '켓', '켜', '켠', '켰', '코', '콕', '콘', '콜', '콤', '콧', '콩', '쾌', '쿠', '쿨', '쿵', '쿼', '퀴', '큐', '크', '큰', '클', '큼', '킁', '키', '킥', '킨', '킬', '킴', '킹', '타', '탁', '탄', '탈', '탐', '탑', '탓', '탔', '탕', '태', '택', '탱', '터', '턱', '턴', '털', '텀', '텁', '텄', '텅', '테', '텍', '텐', '텔', '템', '텨', '텻', '텼', '토', '톡', '톤', '톱', '통', '퇴', '투', '툭', '툰', '툴', '툼', '퉁', '퉜', '튀', '튜', '트', '특', '틀', '틈', '틋', '티', '틱', '팀', '팁', '팅', '파', '팍', '판', '팔', '팠', '패', '팩', '팬', '팸', '퍼', '펑', '페', '펜', '펨', '펭', '펴', '편', '펼', '평', '폐', '포', '폭', '폰', '폼', '퐈', '표', '푸', '푹', '푼', '풀', '품', '풋', '풍', '퓨', '프', '픈', '플', '픔', '픕', '피', '픽', '핀', '필', '핍', '핏', '핑', '하', '학', '한', '할', '함', '합', '핫', '항', '해', '핸', '햇', '했', '행', '햐', '향', '허', '헉', '헌', '헐', '험', '헛', '헤', '헥', '헬', '헷', '헹', '혀', '현', '혈', '혐', '협', '혔', '형', '혜', '호', '혹', '혼', '홀', '홈', '화', '확', '환', '활', '홧', '황', '회', '획', '효', '후', '훅', '훈', '훌', '훔', '훨', '휘', '휙', '휨', '휴', '흐', '흑', '흔', '흘', '흠', '흡', '흥', '희', '흰', '히', '힌', '힐', '힘', '힙']\n"
     ]
    }
   ],
   "source": [
    "create_vocab(train_path=\"./data/train_chatbot.txt\",\n",
    "             valid_path=\"./data/valid_chatbot.txt\",\n",
    "             vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnADataset(Dataset):\n",
    "    def __init__(self, data_path, vocab_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 전처리 단계\n",
    "        self.char2index, self.index2char = self._read_vocab(vocab_path)\n",
    "        self.data = self._preprocess(data_path)\n",
    "    \n",
    "    def _read_vocab(self, vocab_path):\n",
    "        with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "            labels = json.load(f)\n",
    "            char2index = dict()\n",
    "            index2char = dict()\n",
    "\n",
    "            for index, char in enumerate(labels):\n",
    "                char2index[char] = index\n",
    "                index2char[index] = char\n",
    "            \n",
    "        return char2index, index2char\n",
    "    \n",
    "    def _preprocess(self, data_path):\n",
    "        data = []\n",
    "        \n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                sents = line.strip().split('\\t')\n",
    "                assert len(sents) == 2, \"data error!!\"\n",
    "                question_sent, answer_sent = sents[0], sents[1]\n",
    "                \n",
    "                data.append((question_sent, answer_sent))\n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.char2index)\n",
    "    \n",
    "    # 필수 구현\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # 필수 구현\n",
    "    def __getitem__(self, index):\n",
    "        qna = self.data[index]\n",
    "        q_sent, a_sent = qna[0], qna[1]\n",
    "        \n",
    "        src = [self.char2index.get(SOS_TOKEN)]\n",
    "        src += [self.char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        tgt = [self.char2index.get(SOS_TOKEN)]\n",
    "        tgt += [self.char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        tgt += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        return torch.LongTensor(src), torch.LongTensor(tgt)\n",
    "    \n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    xs = [x for x, y in batch]\n",
    "    xs_pad = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    xs_lengths = [x.size(0) for x, y in batch]\n",
    "    xs_lengths = torch.LongTensor(xs_lengths)\n",
    "\n",
    "    ys = [y for x, y in batch]\n",
    "    ys_pad = torch.nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    ys_lengths = [y.size(0) for x, y in batch]\n",
    "    ys_lengths = torch.LongTensor(ys_lengths)\n",
    "\n",
    "    return xs_pad, xs_lengths, ys_pad, ys_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QnADataset(data_path=\"./data/train_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")\n",
    "\n",
    "valid_dataset = QnADataset(data_path=\"./data/valid_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]  # train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   69,  936,  ...,    0,    0,    0],\n",
      "        [   2,  779,  478,  ...,    0,    0,    0],\n",
      "        [   2,  822,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  205,  465,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,    0,    0,    0],\n",
      "        [   2,  805,  268,  ...,    0,    0,    0]]) tensor([[   2,  932,  707,  ...,    0,    0,    0],\n",
      "        [   2,  704,  773,  ...,    0,    0,    0],\n",
      "        [   2,  648,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  647,  908,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,  825,   10,    3],\n",
      "        [   2,  129,  480,  ...,    0,    0,    0]])\n",
      "tensor([14, 14, 15, 15, 15, 12, 11, 14,  9, 11, 10,  8, 10, 16, 13, 40, 17, 13,\n",
      "        24, 11, 11,  4, 13,  8, 16, 35, 27, 20, 17, 19, 11, 12]) tensor([20, 18, 17, 10, 21, 17, 14, 24, 10, 13, 15, 14, 15, 12, 24, 20, 13, 22,\n",
      "        25, 12, 15, 13, 14, 24, 11, 22, 21, 30, 15, 10, 32, 19])\n"
     ]
    }
   ],
   "source": [
    "for x, x_len, y, y_len in train_loader:\n",
    "    print(x, y)\n",
    "    print(x_len, y_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_id=0):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(vocab_size=vocab_size, \n",
    "                                              embed_size=embed_size, \n",
    "                                              pad_id=pad_id)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embed_size, \n",
    "                                                 max_len=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_enc = self.pos_encoding(x)\n",
    "        return token_emb + pos_enc\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_id):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.token_embedding(x)\n",
    "        return x_embed\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    ref: https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, pad_id, device):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        src_vocab_size = input_size\n",
    "        tgt_vocab_size = output_size\n",
    "\n",
    "        d_model = 512\n",
    "        n_head = 8\n",
    "        num_encoder_layers = 6\n",
    "        num_decoder_layers = 6\n",
    "        dim_feedforward = 2048\n",
    "        dropout = 0.1\n",
    "\n",
    "        self.encoder_dropout = nn.Dropout(dropout)\n",
    "        self.decoder_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder_embedding = Embeddings(vocab_size=src_vocab_size, embed_size=d_model, pad_id=pad_id) # embed_size = d_model\n",
    "        self.decoder_embedding = Embeddings(vocab_size=tgt_vocab_size, embed_size=d_model, pad_id=pad_id)\n",
    "        \n",
    "        self.transformer = torch.nn.Transformer(d_model=d_model,\n",
    "                                                nhead=n_head,\n",
    "                                                num_encoder_layers=num_encoder_layers,\n",
    "                                                num_decoder_layers=num_decoder_layers,\n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout)\n",
    "\n",
    "        self.proj_vocab_layer = nn.Linear(in_features=d_model, out_features=tgt_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def _generate_mask(self, x, length):\n",
    "        mask = []\n",
    "\n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # If the length is shorter than maximum length among samples,\n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                    x.new_ones(1, (max_length - l))\n",
    "                                    ], dim=-1)]\n",
    "            else:\n",
    "                # If the length of the sample equals to maximum length among samples,\n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "        mask = torch.cat(mask, dim=0).bool()\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, enc_input, dec_input, enc_input_len, dec_input_len):\n",
    "        \n",
    "        x_enc_embed = self.encoder_dropout(self.encoder_embedding(enc_input.long()))\n",
    "        x_dec_embed = self.decoder_dropout(self.decoder_embedding(dec_input.long()))\n",
    "        # print(x_enc_embed.size(), x_enc_embed)\n",
    "        # print(x_dec_embed.size(), x_dec_embed)\n",
    "\n",
    "        src_key_padding_mask = self._generate_mask(enc_input, enc_input_len).to(self.device)\n",
    "        tgt_key_padding_mask = self._generate_mask(dec_input, dec_input_len).to(self.device)\n",
    "        # print(enc_input_len, src_key_padding_mask)\n",
    "        # print(dec_input_len, tgt_key_padding_mask)\n",
    "        \n",
    "        memory_key_padding_mask = src_key_padding_mask\n",
    "        # src_mask = self.transformer.generate_square_subsequent_mask(x_enc_embed.size(1)).to(self.device)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(x_dec_embed.size(1)).to(self.device)\n",
    "        \n",
    "        # print(x_enc_embed.size(1), src_mask)\n",
    "        # print(x_dec_embed.size(1), tgt_mask)\n",
    "        \n",
    "        x_enc_embed = x_enc_embed.transpose(0, 1)\n",
    "        x_dec_embed = x_dec_embed.transpose(0, 1)\n",
    "        \n",
    "\n",
    "        feature = self.transformer(src=x_enc_embed,\n",
    "                                   tgt=x_dec_embed,\n",
    "                                   src_key_padding_mask=src_key_padding_mask,\n",
    "                                   tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                   memory_key_padding_mask=memory_key_padding_mask,\n",
    "                                   src_mask=None,\n",
    "                                   tgt_mask=tgt_mask)\n",
    "\n",
    "        logits = self.proj_vocab_layer(feature)\n",
    "        logits = self.softmax(logits)\n",
    "        logits = logits.transpose(0, 1)\n",
    "        # print(logits.size(), logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def search(self, enc_input, max_length=255, sos_id=2, eos_id=3):\n",
    "        \n",
    "        SOS_token = sos_id\n",
    "        EOS_token = eos_id\n",
    "        \n",
    "        y_hats, indice = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # ENCODER\n",
    "            x_enc_embed = self.encoder_dropout(self.encoder_embedding(enc_input.long()))\n",
    "            # src_mask = self.transformer.generate_square_subsequent_mask(x_enc_embed.size(1)).to(self.device)\n",
    "            \n",
    "            enc_input_len = torch.LongTensor([enc_input.size(1)])\n",
    "            src_key_padding_mask = self._generate_mask(enc_input, enc_input_len).to(self.device)\n",
    "            \n",
    "            memory_key_padding_mask = src_key_padding_mask\n",
    "            \n",
    "            x_enc_embed = x_enc_embed.transpose(0, 1)\n",
    "            src = x_enc_embed\n",
    "            \n",
    "            memory = self.transformer.encoder(src,\n",
    "                                              mask=None,\n",
    "                                              src_key_padding_mask=src_key_padding_mask)\n",
    "            \n",
    "            # DECODER\n",
    "            dec_input = torch.LongTensor([[SOS_token]]).to(self.device)\n",
    "            dec_input_len = torch.LongTensor([dec_input.size(-1)]).to(self.device)\n",
    "            \n",
    "            for di in range(max_length):\n",
    "                x_dec_embed = self.decoder_dropout(self.decoder_embedding(dec_input.long()))\n",
    "                tgt_mask = self.transformer.generate_square_subsequent_mask(x_dec_embed.size(1)).to(self.device)\n",
    "                tgt_key_padding_mask = self._generate_mask(dec_input, dec_input_len).to(self.device)\n",
    "\n",
    "                x_dec_embed = x_dec_embed.transpose(0, 1)\n",
    "                tgt = x_dec_embed\n",
    "\n",
    "                output = self.transformer.decoder(tgt,\n",
    "                                                  memory,\n",
    "                                                  tgt_mask=tgt_mask,\n",
    "                                                  memory_mask=None,\n",
    "                                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "                logits = self.proj_vocab_layer(output)\n",
    "                logits = self.softmax(logits)\n",
    "                y_pred = logits.transpose(0, 1)\n",
    "                y_hats += [y_pred]\n",
    "\n",
    "                y_pred_ids = y_pred.max(dim=-1)[1]\n",
    "                # print(\"y_pred_ids : \", y_pred_ids)\n",
    "                # indice += [y_pred_ids]\n",
    "\n",
    "                dec_input = torch.cat([dec_input, y_pred_ids[0,-1].unsqueeze(0).unsqueeze(0)], dim=-1).to(self.device)\n",
    "                # print(\"({}) dec_input: {}\".format(di, dec_input))\n",
    "\n",
    "                dec_input_len = torch.LongTensor([dec_input.size(-1)]).to(self.device)\n",
    "                \n",
    "                if y_pred_ids[0,-1].item() == EOS_token:\n",
    "                    break\n",
    "            \n",
    "            y_hats = torch.cat(y_hats, dim=1)\n",
    "            \n",
    "            indice = dec_input[:, 1:]\n",
    "    \n",
    "        return y_hats, indice\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246 1246\n",
      "Transformer(\n",
      "  (encoder_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (decoder_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (encoder_embedding): Embeddings(\n",
      "    (token_embedding): TokenEmbedding(\n",
      "      (token_embedding): Embedding(1246, 512, padding_idx=0)\n",
      "    )\n",
      "    (pos_encoding): PositionalEncoding()\n",
      "  )\n",
      "  (decoder_embedding): Embeddings(\n",
      "    (token_embedding): TokenEmbedding(\n",
      "      (token_embedding): Embedding(1246, 512, padding_idx=0)\n",
      "    )\n",
      "    (pos_encoding): PositionalEncoding()\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (4): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (5): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_vocab_layer): Linear(in_features=512, out_features=1246, bias=True)\n",
      "  (softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = train_dataset.vocab_size\n",
    "output_size = train_dataset.vocab_size\n",
    "\n",
    "print(input_size, output_size)\n",
    "\n",
    "model = Transformer(input_size=input_size, \n",
    "                    output_size=output_size,\n",
    "                    pad_id=PAD_TOKEN_ID, \n",
    "                    device=device).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.0001  # 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=PAD_TOKEN_ID).to(device)\n",
    "\n",
    "# log_softmax + NLLLoss == CrossEntropyLoss\n",
    "criterion = torch.nn.NLLLoss(reduction='mean', ignore_index=PAD_TOKEN_ID).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, clip, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        \n",
    "        src, src_len, trg, trg_len = batch\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        src_len = src_len.to(device)\n",
    "        trg_len = trg_len.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(enc_input=src,\n",
    "                       dec_input=trg[:, :-1],\n",
    "                       enc_input_len=src_len,\n",
    "                       dec_input_len=(trg_len-1))\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        log_interval = 100\n",
    "        if i % log_interval == 0 and i >= 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | loss {:.4f}'.format(epoch+1, i+1, len(data_loader), loss.detach().item()))\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "\n",
    "            src, src_len, trg, trg_len = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            src_len = src_len.to(device)\n",
    "            trg_len = trg_len.to(device)\n",
    "\n",
    "            output = model(enc_input=src,\n",
    "                           dec_input=trg[:, :-1],\n",
    "                           enc_input_len=src_len,\n",
    "                           dec_input_len=(trg_len-1))\n",
    "\n",
    "            #trg = [batch size, trg len]\n",
    "            #output = [batch size, trg len, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "        \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  351 batches | loss 7.3256\n",
      "| epoch   1 |   101/  351 batches | loss 3.4596\n",
      "| epoch   1 |   201/  351 batches | loss 3.2762\n",
      "| epoch   1 |   301/  351 batches | loss 2.8678\n",
      "Epoch: 01 | train.loss.best: 1\n",
      "Epoch: 01 | valid.loss.best: 1\n",
      "Epoch: 01 | Time: 0m 50s\n",
      "\tTrain Loss: 3.4564 | Train PPL:  31.7038\n",
      "\t Val. Loss: 2.7499 |  Val. PPL:  15.6408\n",
      "####################################################################################################\n",
      "| epoch   2 |     1/  351 batches | loss 2.7533\n",
      "| epoch   2 |   101/  351 batches | loss 2.7647\n",
      "| epoch   2 |   201/  351 batches | loss 2.5173\n",
      "| epoch   2 |   301/  351 batches | loss 2.7535\n",
      "Epoch: 02 | train.loss.best: 2\n",
      "Epoch: 02 | valid.loss.best: 2\n",
      "Epoch: 02 | Time: 0m 49s\n",
      "\tTrain Loss: 2.6090 | Train PPL:  13.5851\n",
      "\t Val. Loss: 2.4009 |  Val. PPL:  11.0330\n",
      "####################################################################################################\n",
      "| epoch   3 |     1/  351 batches | loss 2.3137\n",
      "| epoch   3 |   101/  351 batches | loss 2.3309\n",
      "| epoch   3 |   201/  351 batches | loss 2.2465\n",
      "| epoch   3 |   301/  351 batches | loss 2.2806\n",
      "Epoch: 03 | train.loss.best: 3\n",
      "Epoch: 03 | valid.loss.best: 3\n",
      "Epoch: 03 | Time: 0m 49s\n",
      "\tTrain Loss: 2.3301 | Train PPL:  10.2791\n",
      "\t Val. Loss: 2.2264 |  Val. PPL:   9.2663\n",
      "####################################################################################################\n",
      "| epoch   4 |     1/  351 batches | loss 2.1316\n",
      "| epoch   4 |   101/  351 batches | loss 2.2272\n",
      "| epoch   4 |   201/  351 batches | loss 2.1177\n",
      "| epoch   4 |   301/  351 batches | loss 2.1675\n",
      "Epoch: 04 | train.loss.best: 4\n",
      "Epoch: 04 | valid.loss.best: 4\n",
      "Epoch: 04 | Time: 0m 50s\n",
      "\tTrain Loss: 2.1555 | Train PPL:   8.6323\n",
      "\t Val. Loss: 2.0948 |  Val. PPL:   8.1237\n",
      "####################################################################################################\n",
      "| epoch   5 |     1/  351 batches | loss 2.1404\n",
      "| epoch   5 |   101/  351 batches | loss 2.0981\n",
      "| epoch   5 |   201/  351 batches | loss 2.0056\n",
      "| epoch   5 |   301/  351 batches | loss 1.9528\n",
      "Epoch: 05 | train.loss.best: 5\n",
      "Epoch: 05 | valid.loss.best: 5\n",
      "Epoch: 05 | Time: 0m 50s\n",
      "\tTrain Loss: 2.0191 | Train PPL:   7.5318\n",
      "\t Val. Loss: 2.0066 |  Val. PPL:   7.4380\n",
      "####################################################################################################\n",
      "| epoch   6 |     1/  351 batches | loss 2.0696\n",
      "| epoch   6 |   101/  351 batches | loss 1.7329\n",
      "| epoch   6 |   201/  351 batches | loss 1.9554\n",
      "| epoch   6 |   301/  351 batches | loss 1.7045\n",
      "Epoch: 06 | train.loss.best: 6\n",
      "Epoch: 06 | valid.loss.best: 6\n",
      "Epoch: 06 | Time: 0m 50s\n",
      "\tTrain Loss: 1.9061 | Train PPL:   6.7268\n",
      "\t Val. Loss: 1.9258 |  Val. PPL:   6.8606\n",
      "####################################################################################################\n",
      "| epoch   7 |     1/  351 batches | loss 1.8195\n",
      "| epoch   7 |   101/  351 batches | loss 1.7548\n",
      "| epoch   7 |   201/  351 batches | loss 1.7539\n",
      "| epoch   7 |   301/  351 batches | loss 1.7474\n",
      "Epoch: 07 | train.loss.best: 7\n",
      "Epoch: 07 | valid.loss.best: 7\n",
      "Epoch: 07 | Time: 0m 51s\n",
      "\tTrain Loss: 1.8004 | Train PPL:   6.0518\n",
      "\t Val. Loss: 1.8566 |  Val. PPL:   6.4017\n",
      "####################################################################################################\n",
      "| epoch   8 |     1/  351 batches | loss 1.6511\n",
      "| epoch   8 |   101/  351 batches | loss 1.7441\n",
      "| epoch   8 |   201/  351 batches | loss 1.7109\n",
      "| epoch   8 |   301/  351 batches | loss 1.7211\n",
      "Epoch: 08 | train.loss.best: 8\n",
      "Epoch: 08 | valid.loss.best: 8\n",
      "Epoch: 08 | Time: 0m 52s\n",
      "\tTrain Loss: 1.7106 | Train PPL:   5.5320\n",
      "\t Val. Loss: 1.8029 |  Val. PPL:   6.0675\n",
      "####################################################################################################\n",
      "| epoch   9 |     1/  351 batches | loss 1.5210\n",
      "| epoch   9 |   101/  351 batches | loss 1.6317\n",
      "| epoch   9 |   201/  351 batches | loss 1.7490\n",
      "| epoch   9 |   301/  351 batches | loss 1.5776\n",
      "Epoch: 09 | train.loss.best: 9\n",
      "Epoch: 09 | valid.loss.best: 9\n",
      "Epoch: 09 | Time: 0m 50s\n",
      "\tTrain Loss: 1.6220 | Train PPL:   5.0634\n",
      "\t Val. Loss: 1.7657 |  Val. PPL:   5.8457\n",
      "####################################################################################################\n",
      "| epoch  10 |     1/  351 batches | loss 1.4558\n",
      "| epoch  10 |   101/  351 batches | loss 1.5765\n",
      "| epoch  10 |   201/  351 batches | loss 1.4675\n",
      "| epoch  10 |   301/  351 batches | loss 1.5443\n",
      "Epoch: 10 | train.loss.best: 10\n",
      "Epoch: 10 | valid.loss.best: 10\n",
      "Epoch: 10 | Time: 0m 49s\n",
      "\tTrain Loss: 1.5433 | Train PPL:   4.6799\n",
      "\t Val. Loss: 1.7228 |  Val. PPL:   5.5999\n",
      "####################################################################################################\n",
      "| epoch  11 |     1/  351 batches | loss 1.2813\n",
      "| epoch  11 |   101/  351 batches | loss 1.4913\n",
      "| epoch  11 |   201/  351 batches | loss 1.4741\n",
      "| epoch  11 |   301/  351 batches | loss 1.3527\n",
      "Epoch: 11 | train.loss.best: 11\n",
      "Epoch: 11 | valid.loss.best: 11\n",
      "Epoch: 11 | Time: 0m 49s\n",
      "\tTrain Loss: 1.4686 | Train PPL:   4.3433\n",
      "\t Val. Loss: 1.6852 |  Val. PPL:   5.3936\n",
      "####################################################################################################\n",
      "| epoch  12 |     1/  351 batches | loss 1.4021\n",
      "| epoch  12 |   101/  351 batches | loss 1.3728\n",
      "| epoch  12 |   201/  351 batches | loss 1.5376\n",
      "| epoch  12 |   301/  351 batches | loss 1.4110\n",
      "Epoch: 12 | train.loss.best: 12\n",
      "Epoch: 12 | valid.loss.best: 12\n",
      "Epoch: 12 | Time: 0m 49s\n",
      "\tTrain Loss: 1.3993 | Train PPL:   4.0522\n",
      "\t Val. Loss: 1.6369 |  Val. PPL:   5.1390\n",
      "####################################################################################################\n",
      "| epoch  13 |     1/  351 batches | loss 1.3216\n",
      "| epoch  13 |   101/  351 batches | loss 1.3162\n",
      "| epoch  13 |   201/  351 batches | loss 1.4842\n",
      "| epoch  13 |   301/  351 batches | loss 1.3335\n",
      "Epoch: 13 | train.loss.best: 13\n",
      "Epoch: 13 | valid.loss.best: 13\n",
      "Epoch: 13 | Time: 0m 49s\n",
      "\tTrain Loss: 1.3344 | Train PPL:   3.7976\n",
      "\t Val. Loss: 1.6161 |  Val. PPL:   5.0335\n",
      "####################################################################################################\n",
      "| epoch  14 |     1/  351 batches | loss 1.1942\n",
      "| epoch  14 |   101/  351 batches | loss 1.2446\n",
      "| epoch  14 |   201/  351 batches | loss 1.3409\n",
      "| epoch  14 |   301/  351 batches | loss 1.3540\n",
      "Epoch: 14 | train.loss.best: 14\n",
      "Epoch: 14 | valid.loss.best: 14\n",
      "Epoch: 14 | Time: 0m 49s\n",
      "\tTrain Loss: 1.2716 | Train PPL:   3.5664\n",
      "\t Val. Loss: 1.5851 |  Val. PPL:   4.8799\n",
      "####################################################################################################\n",
      "| epoch  15 |     1/  351 batches | loss 1.1542\n",
      "| epoch  15 |   101/  351 batches | loss 1.2317\n",
      "| epoch  15 |   201/  351 batches | loss 1.2072\n",
      "| epoch  15 |   301/  351 batches | loss 1.2763\n",
      "Epoch: 15 | train.loss.best: 15\n",
      "Epoch: 15 | valid.loss.best: 15\n",
      "Epoch: 15 | Time: 0m 48s\n",
      "\tTrain Loss: 1.2160 | Train PPL:   3.3736\n",
      "\t Val. Loss: 1.5550 |  Val. PPL:   4.7353\n",
      "####################################################################################################\n",
      "| epoch  16 |     1/  351 batches | loss 1.0737\n",
      "| epoch  16 |   101/  351 batches | loss 1.1989\n",
      "| epoch  16 |   201/  351 batches | loss 1.0902\n",
      "| epoch  16 |   301/  351 batches | loss 1.1773\n",
      "Epoch: 16 | train.loss.best: 16\n",
      "Epoch: 16 | valid.loss.best: 16\n",
      "Epoch: 16 | Time: 0m 48s\n",
      "\tTrain Loss: 1.1623 | Train PPL:   3.1973\n",
      "\t Val. Loss: 1.5258 |  Val. PPL:   4.5988\n",
      "####################################################################################################\n",
      "| epoch  17 |     1/  351 batches | loss 1.1482\n",
      "| epoch  17 |   101/  351 batches | loss 1.1502\n",
      "| epoch  17 |   201/  351 batches | loss 1.1788\n",
      "| epoch  17 |   301/  351 batches | loss 1.1538\n",
      "Epoch: 17 | train.loss.best: 17\n",
      "Epoch: 17 | valid.loss.best: 17\n",
      "Epoch: 17 | Time: 0m 48s\n",
      "\tTrain Loss: 1.1140 | Train PPL:   3.0466\n",
      "\t Val. Loss: 1.5166 |  Val. PPL:   4.5567\n",
      "####################################################################################################\n",
      "| epoch  18 |     1/  351 batches | loss 1.0099\n",
      "| epoch  18 |   101/  351 batches | loss 0.9867\n",
      "| epoch  18 |   201/  351 batches | loss 0.9707\n",
      "| epoch  18 |   301/  351 batches | loss 1.1183\n",
      "Epoch: 18 | train.loss.best: 18\n",
      "Epoch: 18 | valid.loss.best: 18\n",
      "Epoch: 18 | Time: 0m 47s\n",
      "\tTrain Loss: 1.0623 | Train PPL:   2.8930\n",
      "\t Val. Loss: 1.4948 |  Val. PPL:   4.4584\n",
      "####################################################################################################\n",
      "| epoch  19 |     1/  351 batches | loss 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |   101/  351 batches | loss 1.1675\n",
      "| epoch  19 |   201/  351 batches | loss 1.1103\n",
      "| epoch  19 |   301/  351 batches | loss 0.9882\n",
      "Epoch: 19 | train.loss.best: 19\n",
      "Epoch: 19 | valid.loss.best: 19\n",
      "Epoch: 19 | Time: 0m 47s\n",
      "\tTrain Loss: 1.0176 | Train PPL:   2.7666\n",
      "\t Val. Loss: 1.4759 |  Val. PPL:   4.3749\n",
      "####################################################################################################\n",
      "| epoch  20 |     1/  351 batches | loss 0.9503\n",
      "| epoch  20 |   101/  351 batches | loss 0.9377\n",
      "| epoch  20 |   201/  351 batches | loss 0.9543\n",
      "| epoch  20 |   301/  351 batches | loss 0.9824\n",
      "Epoch: 20 | train.loss.best: 20\n",
      "Epoch: 20 | valid.loss.best: 20\n",
      "Epoch: 20 | Time: 0m 48s\n",
      "\tTrain Loss: 0.9751 | Train PPL:   2.6514\n",
      "\t Val. Loss: 1.4724 |  Val. PPL:   4.3595\n",
      "####################################################################################################\n",
      "| epoch  21 |     1/  351 batches | loss 1.0032\n",
      "| epoch  21 |   101/  351 batches | loss 0.8949\n",
      "| epoch  21 |   201/  351 batches | loss 0.9210\n",
      "| epoch  21 |   301/  351 batches | loss 0.9909\n",
      "Epoch: 21 | train.loss.best: 21\n",
      "Epoch: 21 | valid.loss.best: 21\n",
      "Epoch: 21 | Time: 0m 47s\n",
      "\tTrain Loss: 0.9345 | Train PPL:   2.5460\n",
      "\t Val. Loss: 1.4553 |  Val. PPL:   4.2858\n",
      "####################################################################################################\n",
      "| epoch  22 |     1/  351 batches | loss 0.8325\n",
      "| epoch  22 |   101/  351 batches | loss 0.8752\n",
      "| epoch  22 |   201/  351 batches | loss 0.9525\n",
      "| epoch  22 |   301/  351 batches | loss 0.9445\n",
      "Epoch: 22 | train.loss.best: 22\n",
      "Epoch: 22 | valid.loss.best: 22\n",
      "Epoch: 22 | Time: 0m 48s\n",
      "\tTrain Loss: 0.8959 | Train PPL:   2.4496\n",
      "\t Val. Loss: 1.4508 |  Val. PPL:   4.2667\n",
      "####################################################################################################\n",
      "| epoch  23 |     1/  351 batches | loss 0.6693\n",
      "| epoch  23 |   101/  351 batches | loss 0.9104\n",
      "| epoch  23 |   201/  351 batches | loss 0.8244\n",
      "| epoch  23 |   301/  351 batches | loss 0.9249\n",
      "Epoch: 23 | train.loss.best: 23\n",
      "Epoch: 23 | Time: 0m 48s\n",
      "\tTrain Loss: 0.8622 | Train PPL:   2.3683\n",
      "\t Val. Loss: 1.4595 |  Val. PPL:   4.3038\n",
      "####################################################################################################\n",
      "| epoch  24 |     1/  351 batches | loss 0.7689\n",
      "| epoch  24 |   101/  351 batches | loss 0.8513\n",
      "| epoch  24 |   201/  351 batches | loss 0.8524\n",
      "| epoch  24 |   301/  351 batches | loss 0.8656\n",
      "Epoch: 24 | train.loss.best: 24\n",
      "Epoch: 24 | valid.loss.best: 24\n",
      "Epoch: 24 | Time: 0m 47s\n",
      "\tTrain Loss: 0.8250 | Train PPL:   2.2818\n",
      "\t Val. Loss: 1.4436 |  Val. PPL:   4.2358\n",
      "####################################################################################################\n",
      "| epoch  25 |     1/  351 batches | loss 0.7685\n",
      "| epoch  25 |   101/  351 batches | loss 0.8338\n",
      "| epoch  25 |   201/  351 batches | loss 0.7704\n",
      "| epoch  25 |   301/  351 batches | loss 0.8353\n",
      "Epoch: 25 | train.loss.best: 25\n",
      "Epoch: 25 | Time: 0m 48s\n",
      "\tTrain Loss: 0.7877 | Train PPL:   2.1982\n",
      "\t Val. Loss: 1.4545 |  Val. PPL:   4.2823\n",
      "####################################################################################################\n",
      "| epoch  26 |     1/  351 batches | loss 0.6575\n",
      "| epoch  26 |   101/  351 batches | loss 0.6782\n",
      "| epoch  26 |   201/  351 batches | loss 0.7598\n",
      "| epoch  26 |   301/  351 batches | loss 0.7657\n",
      "Epoch: 26 | train.loss.best: 26\n",
      "Epoch: 26 | Time: 0m 47s\n",
      "\tTrain Loss: 0.7534 | Train PPL:   2.1243\n",
      "\t Val. Loss: 1.4498 |  Val. PPL:   4.2621\n",
      "####################################################################################################\n",
      "| epoch  27 |     1/  351 batches | loss 0.6913\n",
      "| epoch  27 |   101/  351 batches | loss 0.6316\n",
      "| epoch  27 |   201/  351 batches | loss 0.6518\n",
      "| epoch  27 |   301/  351 batches | loss 0.7663\n",
      "Epoch: 27 | train.loss.best: 27\n",
      "Epoch: 27 | Time: 0m 47s\n",
      "\tTrain Loss: 0.7199 | Train PPL:   2.0542\n",
      "\t Val. Loss: 1.4602 |  Val. PPL:   4.3069\n",
      "####################################################################################################\n",
      "| epoch  28 |     1/  351 batches | loss 0.6570\n",
      "| epoch  28 |   101/  351 batches | loss 0.7169\n",
      "| epoch  28 |   201/  351 batches | loss 0.7270\n",
      "| epoch  28 |   301/  351 batches | loss 0.6341\n",
      "Epoch: 28 | train.loss.best: 28\n",
      "Epoch: 28 | Time: 0m 47s\n",
      "\tTrain Loss: 0.6848 | Train PPL:   1.9833\n",
      "\t Val. Loss: 1.4619 |  Val. PPL:   4.3140\n",
      "####################################################################################################\n",
      "| epoch  29 |     1/  351 batches | loss 0.5804\n",
      "| epoch  29 |   101/  351 batches | loss 0.6064\n",
      "| epoch  29 |   201/  351 batches | loss 0.6205\n",
      "| epoch  29 |   301/  351 batches | loss 0.7228\n",
      "Epoch: 29 | train.loss.best: 29\n",
      "Epoch: 29 | Time: 0m 48s\n",
      "\tTrain Loss: 0.6501 | Train PPL:   1.9157\n",
      "\t Val. Loss: 1.4837 |  Val. PPL:   4.4092\n",
      "####################################################################################################\n",
      "| epoch  30 |     1/  351 batches | loss 0.5478\n",
      "| epoch  30 |   101/  351 batches | loss 0.5993\n",
      "| epoch  30 |   201/  351 batches | loss 0.6227\n",
      "| epoch  30 |   301/  351 batches | loss 0.7134\n",
      "Epoch: 30 | train.loss.best: 30\n",
      "Epoch: 30 | Time: 0m 48s\n",
      "\tTrain Loss: 0.6227 | Train PPL:   1.8640\n",
      "\t Val. Loss: 1.4918 |  Val. PPL:   4.4452\n",
      "####################################################################################################\n",
      "| epoch  31 |     1/  351 batches | loss 0.5721\n",
      "| epoch  31 |   101/  351 batches | loss 0.5286\n",
      "| epoch  31 |   201/  351 batches | loss 0.6331\n",
      "| epoch  31 |   301/  351 batches | loss 0.5630\n",
      "Epoch: 31 | train.loss.best: 31\n",
      "Epoch: 31 | Time: 0m 48s\n",
      "\tTrain Loss: 0.5829 | Train PPL:   1.7913\n",
      "\t Val. Loss: 1.4994 |  Val. PPL:   4.4791\n",
      "####################################################################################################\n",
      "| epoch  32 |     1/  351 batches | loss 0.5444\n",
      "| epoch  32 |   101/  351 batches | loss 0.5033\n",
      "| epoch  32 |   201/  351 batches | loss 0.5481\n",
      "| epoch  32 |   301/  351 batches | loss 0.6083\n",
      "Epoch: 32 | train.loss.best: 32\n",
      "Epoch: 32 | Time: 0m 48s\n",
      "\tTrain Loss: 0.5540 | Train PPL:   1.7401\n",
      "\t Val. Loss: 1.5301 |  Val. PPL:   4.6186\n",
      "####################################################################################################\n",
      "| epoch  33 |     1/  351 batches | loss 0.4562\n",
      "| epoch  33 |   101/  351 batches | loss 0.5099\n",
      "| epoch  33 |   201/  351 batches | loss 0.5690\n",
      "| epoch  33 |   301/  351 batches | loss 0.4808\n",
      "Epoch: 33 | train.loss.best: 33\n",
      "Epoch: 33 | Time: 0m 48s\n",
      "\tTrain Loss: 0.5197 | Train PPL:   1.6816\n",
      "\t Val. Loss: 1.5357 |  Val. PPL:   4.6446\n",
      "####################################################################################################\n",
      "| epoch  34 |     1/  351 batches | loss 0.4209\n",
      "| epoch  34 |   101/  351 batches | loss 0.4534\n",
      "| epoch  34 |   201/  351 batches | loss 0.5182\n",
      "| epoch  34 |   301/  351 batches | loss 0.4498\n",
      "Epoch: 34 | train.loss.best: 34\n",
      "Epoch: 34 | Time: 0m 48s\n",
      "\tTrain Loss: 0.4879 | Train PPL:   1.6289\n",
      "\t Val. Loss: 1.5470 |  Val. PPL:   4.6973\n",
      "####################################################################################################\n",
      "| epoch  35 |     1/  351 batches | loss 0.4669\n",
      "| epoch  35 |   101/  351 batches | loss 0.4419\n",
      "| epoch  35 |   201/  351 batches | loss 0.4350\n",
      "| epoch  35 |   301/  351 batches | loss 0.5233\n",
      "Epoch: 35 | train.loss.best: 35\n",
      "Epoch: 35 | Time: 0m 48s\n",
      "\tTrain Loss: 0.4563 | Train PPL:   1.5782\n",
      "\t Val. Loss: 1.5766 |  Val. PPL:   4.8385\n",
      "####################################################################################################\n",
      "| epoch  36 |     1/  351 batches | loss 0.4123\n",
      "| epoch  36 |   101/  351 batches | loss 0.3668\n",
      "| epoch  36 |   201/  351 batches | loss 0.5173\n",
      "| epoch  36 |   301/  351 batches | loss 0.4855\n",
      "Epoch: 36 | train.loss.best: 36\n",
      "Epoch: 36 | Time: 0m 48s\n",
      "\tTrain Loss: 0.4223 | Train PPL:   1.5254\n",
      "\t Val. Loss: 1.6196 |  Val. PPL:   5.0510\n",
      "####################################################################################################\n",
      "| epoch  37 |     1/  351 batches | loss 0.2615\n",
      "| epoch  37 |   101/  351 batches | loss 0.3299\n",
      "| epoch  37 |   201/  351 batches | loss 0.4619\n",
      "| epoch  37 |   301/  351 batches | loss 0.4206\n",
      "Epoch: 37 | train.loss.best: 37\n",
      "Epoch: 37 | Time: 0m 48s\n",
      "\tTrain Loss: 0.3896 | Train PPL:   1.4764\n",
      "\t Val. Loss: 1.6540 |  Val. PPL:   5.2280\n",
      "####################################################################################################\n",
      "| epoch  38 |     1/  351 batches | loss 0.3391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  38 |   101/  351 batches | loss 0.2925\n",
      "| epoch  38 |   201/  351 batches | loss 0.3077\n",
      "| epoch  38 |   301/  351 batches | loss 0.4195\n",
      "Epoch: 38 | train.loss.best: 38\n",
      "Epoch: 38 | Time: 0m 48s\n",
      "\tTrain Loss: 0.3637 | Train PPL:   1.4387\n",
      "\t Val. Loss: 1.6419 |  Val. PPL:   5.1648\n",
      "####################################################################################################\n",
      "| epoch  39 |     1/  351 batches | loss 0.3023\n",
      "| epoch  39 |   101/  351 batches | loss 0.2780\n",
      "| epoch  39 |   201/  351 batches | loss 0.3616\n",
      "| epoch  39 |   301/  351 batches | loss 0.3718\n",
      "Epoch: 39 | train.loss.best: 39\n",
      "Epoch: 39 | Time: 0m 48s\n",
      "\tTrain Loss: 0.3315 | Train PPL:   1.3931\n",
      "\t Val. Loss: 1.6902 |  Val. PPL:   5.4204\n",
      "####################################################################################################\n",
      "| epoch  40 |     1/  351 batches | loss 0.2578\n",
      "| epoch  40 |   101/  351 batches | loss 0.3102\n",
      "| epoch  40 |   201/  351 batches | loss 0.2900\n",
      "| epoch  40 |   301/  351 batches | loss 0.3063\n",
      "Epoch: 40 | train.loss.best: 40\n",
      "Epoch: 40 | Time: 0m 48s\n",
      "\tTrain Loss: 0.2969 | Train PPL:   1.3457\n",
      "\t Val. Loss: 1.7309 |  Val. PPL:   5.6459\n",
      "####################################################################################################\n",
      "| epoch  41 |     1/  351 batches | loss 0.2736\n",
      "| epoch  41 |   101/  351 batches | loss 0.2194\n",
      "| epoch  41 |   201/  351 batches | loss 0.2968\n",
      "| epoch  41 |   301/  351 batches | loss 0.3336\n",
      "Epoch: 41 | train.loss.best: 41\n",
      "Epoch: 41 | Time: 0m 48s\n",
      "\tTrain Loss: 0.2733 | Train PPL:   1.3143\n",
      "\t Val. Loss: 1.7388 |  Val. PPL:   5.6904\n",
      "####################################################################################################\n",
      "| epoch  42 |     1/  351 batches | loss 0.1808\n",
      "| epoch  42 |   101/  351 batches | loss 0.2118\n",
      "| epoch  42 |   201/  351 batches | loss 0.2255\n",
      "| epoch  42 |   301/  351 batches | loss 0.2495\n",
      "Epoch: 42 | train.loss.best: 42\n",
      "Epoch: 42 | Time: 0m 48s\n",
      "\tTrain Loss: 0.2496 | Train PPL:   1.2835\n",
      "\t Val. Loss: 1.7532 |  Val. PPL:   5.7729\n",
      "####################################################################################################\n",
      "| epoch  43 |     1/  351 batches | loss 0.1844\n",
      "| epoch  43 |   101/  351 batches | loss 0.2169\n",
      "| epoch  43 |   201/  351 batches | loss 0.2476\n",
      "| epoch  43 |   301/  351 batches | loss 0.2448\n",
      "Epoch: 43 | train.loss.best: 43\n",
      "Epoch: 43 | Time: 0m 48s\n",
      "\tTrain Loss: 0.2313 | Train PPL:   1.2602\n",
      "\t Val. Loss: 1.7751 |  Val. PPL:   5.9011\n",
      "####################################################################################################\n",
      "| epoch  44 |     1/  351 batches | loss 0.1938\n",
      "| epoch  44 |   101/  351 batches | loss 0.1565\n",
      "| epoch  44 |   201/  351 batches | loss 0.1998\n",
      "| epoch  44 |   301/  351 batches | loss 0.1644\n",
      "Epoch: 44 | train.loss.best: 44\n",
      "Epoch: 44 | Time: 0m 48s\n",
      "\tTrain Loss: 0.2141 | Train PPL:   1.2387\n",
      "\t Val. Loss: 1.7919 |  Val. PPL:   6.0006\n",
      "####################################################################################################\n",
      "| epoch  45 |     1/  351 batches | loss 0.1437\n",
      "| epoch  45 |   101/  351 batches | loss 0.1959\n",
      "| epoch  45 |   201/  351 batches | loss 0.2034\n",
      "| epoch  45 |   301/  351 batches | loss 0.2223\n",
      "Epoch: 45 | train.loss.best: 45\n",
      "Epoch: 45 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1942 | Train PPL:   1.2143\n",
      "\t Val. Loss: 1.8076 |  Val. PPL:   6.0961\n",
      "####################################################################################################\n",
      "| epoch  46 |     1/  351 batches | loss 0.2142\n",
      "| epoch  46 |   101/  351 batches | loss 0.1529\n",
      "| epoch  46 |   201/  351 batches | loss 0.1809\n",
      "| epoch  46 |   301/  351 batches | loss 0.1704\n",
      "Epoch: 46 | train.loss.best: 46\n",
      "Epoch: 46 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1792 | Train PPL:   1.1962\n",
      "\t Val. Loss: 1.8530 |  Val. PPL:   6.3791\n",
      "####################################################################################################\n",
      "| epoch  47 |     1/  351 batches | loss 0.1765\n",
      "| epoch  47 |   101/  351 batches | loss 0.1391\n",
      "| epoch  47 |   201/  351 batches | loss 0.1683\n",
      "| epoch  47 |   301/  351 batches | loss 0.1703\n",
      "Epoch: 47 | train.loss.best: 47\n",
      "Epoch: 47 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1697 | Train PPL:   1.1849\n",
      "\t Val. Loss: 1.8483 |  Val. PPL:   6.3491\n",
      "####################################################################################################\n",
      "| epoch  48 |     1/  351 batches | loss 0.1039\n",
      "| epoch  48 |   101/  351 batches | loss 0.1602\n",
      "| epoch  48 |   201/  351 batches | loss 0.2040\n",
      "| epoch  48 |   301/  351 batches | loss 0.1962\n",
      "Epoch: 48 | train.loss.best: 48\n",
      "Epoch: 48 | Time: 0m 47s\n",
      "\tTrain Loss: 0.1581 | Train PPL:   1.1713\n",
      "\t Val. Loss: 1.8968 |  Val. PPL:   6.6643\n",
      "####################################################################################################\n",
      "| epoch  49 |     1/  351 batches | loss 0.1251\n",
      "| epoch  49 |   101/  351 batches | loss 0.1675\n",
      "| epoch  49 |   201/  351 batches | loss 0.1687\n",
      "| epoch  49 |   301/  351 batches | loss 0.2133\n",
      "Epoch: 49 | train.loss.best: 49\n",
      "Epoch: 49 | Time: 0m 47s\n",
      "\tTrain Loss: 0.1517 | Train PPL:   1.1639\n",
      "\t Val. Loss: 1.9072 |  Val. PPL:   6.7343\n",
      "####################################################################################################\n",
      "| epoch  50 |     1/  351 batches | loss 0.1292\n",
      "| epoch  50 |   101/  351 batches | loss 0.1045\n",
      "| epoch  50 |   201/  351 batches | loss 0.1281\n",
      "| epoch  50 |   301/  351 batches | loss 0.1979\n",
      "Epoch: 50 | train.loss.best: 50\n",
      "Epoch: 50 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1432 | Train PPL:   1.1539\n",
      "\t Val. Loss: 1.9255 |  Val. PPL:   6.8586\n",
      "####################################################################################################\n",
      "| epoch  51 |     1/  351 batches | loss 0.1488\n",
      "| epoch  51 |   101/  351 batches | loss 0.1630\n",
      "| epoch  51 |   201/  351 batches | loss 0.0988\n",
      "| epoch  51 |   301/  351 batches | loss 0.1648\n",
      "Epoch: 51 | train.loss.best: 51\n",
      "Epoch: 51 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1335 | Train PPL:   1.1429\n",
      "\t Val. Loss: 1.9468 |  Val. PPL:   7.0060\n",
      "####################################################################################################\n",
      "| epoch  52 |     1/  351 batches | loss 0.0910\n",
      "| epoch  52 |   101/  351 batches | loss 0.1197\n",
      "| epoch  52 |   201/  351 batches | loss 0.1187\n",
      "| epoch  52 |   301/  351 batches | loss 0.1490\n",
      "Epoch: 52 | train.loss.best: 52\n",
      "Epoch: 52 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1296 | Train PPL:   1.1384\n",
      "\t Val. Loss: 1.9542 |  Val. PPL:   7.0584\n",
      "####################################################################################################\n",
      "| epoch  53 |     1/  351 batches | loss 0.1015\n",
      "| epoch  53 |   101/  351 batches | loss 0.1342\n",
      "| epoch  53 |   201/  351 batches | loss 0.1023\n",
      "| epoch  53 |   301/  351 batches | loss 0.1500\n",
      "Epoch: 53 | train.loss.best: 53\n",
      "Epoch: 53 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1242 | Train PPL:   1.1323\n",
      "\t Val. Loss: 1.9838 |  Val. PPL:   7.2705\n",
      "####################################################################################################\n",
      "| epoch  54 |     1/  351 batches | loss 0.1132\n",
      "| epoch  54 |   101/  351 batches | loss 0.1132\n",
      "| epoch  54 |   201/  351 batches | loss 0.1215\n",
      "| epoch  54 |   301/  351 batches | loss 0.1145\n",
      "Epoch: 54 | train.loss.best: 54\n",
      "Epoch: 54 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1222 | Train PPL:   1.1300\n",
      "\t Val. Loss: 1.9712 |  Val. PPL:   7.1790\n",
      "####################################################################################################\n",
      "| epoch  55 |     1/  351 batches | loss 0.1116\n",
      "| epoch  55 |   101/  351 batches | loss 0.0905\n",
      "| epoch  55 |   201/  351 batches | loss 0.1166\n",
      "| epoch  55 |   301/  351 batches | loss 0.1076\n",
      "Epoch: 55 | train.loss.best: 55\n",
      "Epoch: 55 | Time: 0m 47s\n",
      "\tTrain Loss: 0.1156 | Train PPL:   1.1225\n",
      "\t Val. Loss: 1.9969 |  Val. PPL:   7.3660\n",
      "####################################################################################################\n",
      "| epoch  56 |     1/  351 batches | loss 0.0855\n",
      "| epoch  56 |   101/  351 batches | loss 0.1077\n",
      "| epoch  56 |   201/  351 batches | loss 0.0924\n",
      "| epoch  56 |   301/  351 batches | loss 0.1098\n",
      "Epoch: 56 | train.loss.best: 56\n",
      "Epoch: 56 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1106 | Train PPL:   1.1169\n",
      "\t Val. Loss: 1.9934 |  Val. PPL:   7.3408\n",
      "####################################################################################################\n",
      "| epoch  57 |     1/  351 batches | loss 0.0771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  57 |   101/  351 batches | loss 0.0962\n",
      "| epoch  57 |   201/  351 batches | loss 0.0700\n",
      "| epoch  57 |   301/  351 batches | loss 0.0949\n",
      "Epoch: 57 | train.loss.best: 57\n",
      "Epoch: 57 | Time: 0m 47s\n",
      "\tTrain Loss: 0.1078 | Train PPL:   1.1138\n",
      "\t Val. Loss: 2.0340 |  Val. PPL:   7.6448\n",
      "####################################################################################################\n",
      "| epoch  58 |     1/  351 batches | loss 0.0908\n",
      "| epoch  58 |   101/  351 batches | loss 0.1020\n",
      "| epoch  58 |   201/  351 batches | loss 0.0854\n",
      "| epoch  58 |   301/  351 batches | loss 0.1123\n",
      "Epoch: 58 | train.loss.best: 58\n",
      "Epoch: 58 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1058 | Train PPL:   1.1116\n",
      "\t Val. Loss: 2.0500 |  Val. PPL:   7.7678\n",
      "####################################################################################################\n",
      "| epoch  59 |     1/  351 batches | loss 0.0801\n",
      "| epoch  59 |   101/  351 batches | loss 0.0962\n",
      "| epoch  59 |   201/  351 batches | loss 0.1272\n",
      "| epoch  59 |   301/  351 batches | loss 0.0963\n",
      "Epoch: 59 | train.loss.best: 59\n",
      "Epoch: 59 | Time: 0m 48s\n",
      "\tTrain Loss: 0.1042 | Train PPL:   1.1098\n",
      "\t Val. Loss: 2.0783 |  Val. PPL:   7.9910\n",
      "####################################################################################################\n",
      "| epoch  60 |     1/  351 batches | loss 0.0945\n",
      "| epoch  60 |   101/  351 batches | loss 0.1131\n",
      "| epoch  60 |   201/  351 batches | loss 0.1135\n",
      "| epoch  60 |   301/  351 batches | loss 0.1194\n",
      "Epoch: 60 | train.loss.best: 60\n",
      "Epoch: 60 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0984 | Train PPL:   1.1034\n",
      "\t Val. Loss: 2.0904 |  Val. PPL:   8.0882\n",
      "####################################################################################################\n",
      "| epoch  61 |     1/  351 batches | loss 0.0693\n",
      "| epoch  61 |   101/  351 batches | loss 0.0990\n",
      "| epoch  61 |   201/  351 batches | loss 0.0905\n",
      "| epoch  61 |   301/  351 batches | loss 0.0992\n",
      "Epoch: 61 | train.loss.best: 61\n",
      "Epoch: 61 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0932 | Train PPL:   1.0977\n",
      "\t Val. Loss: 2.0952 |  Val. PPL:   8.1270\n",
      "####################################################################################################\n",
      "| epoch  62 |     1/  351 batches | loss 0.1041\n",
      "| epoch  62 |   101/  351 batches | loss 0.0995\n",
      "| epoch  62 |   201/  351 batches | loss 0.1190\n",
      "| epoch  62 |   301/  351 batches | loss 0.1186\n",
      "Epoch: 62 | train.loss.best: 62\n",
      "Epoch: 62 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0930 | Train PPL:   1.0974\n",
      "\t Val. Loss: 2.0839 |  Val. PPL:   8.0361\n",
      "####################################################################################################\n",
      "| epoch  63 |     1/  351 batches | loss 0.0812\n",
      "| epoch  63 |   101/  351 batches | loss 0.0755\n",
      "| epoch  63 |   201/  351 batches | loss 0.1098\n",
      "| epoch  63 |   301/  351 batches | loss 0.1127\n",
      "Epoch: 63 | train.loss.best: 63\n",
      "Epoch: 63 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0922 | Train PPL:   1.0966\n",
      "\t Val. Loss: 2.1106 |  Val. PPL:   8.2529\n",
      "####################################################################################################\n",
      "| epoch  64 |     1/  351 batches | loss 0.0712\n",
      "| epoch  64 |   101/  351 batches | loss 0.0814\n",
      "| epoch  64 |   201/  351 batches | loss 0.0702\n",
      "| epoch  64 |   301/  351 batches | loss 0.0807\n",
      "Epoch: 64 | train.loss.best: 64\n",
      "Epoch: 64 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0880 | Train PPL:   1.0919\n",
      "\t Val. Loss: 2.0794 |  Val. PPL:   7.9995\n",
      "####################################################################################################\n",
      "| epoch  65 |     1/  351 batches | loss 0.0734\n",
      "| epoch  65 |   101/  351 batches | loss 0.1281\n",
      "| epoch  65 |   201/  351 batches | loss 0.1185\n",
      "| epoch  65 |   301/  351 batches | loss 0.0818\n",
      "Epoch: 65 | train.loss.best: 65\n",
      "Epoch: 65 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0879 | Train PPL:   1.0919\n",
      "\t Val. Loss: 2.1282 |  Val. PPL:   8.3996\n",
      "####################################################################################################\n",
      "| epoch  66 |     1/  351 batches | loss 0.1115\n",
      "| epoch  66 |   101/  351 batches | loss 0.0848\n",
      "| epoch  66 |   201/  351 batches | loss 0.0781\n",
      "| epoch  66 |   301/  351 batches | loss 0.0899\n",
      "Epoch: 66 | train.loss.best: 66\n",
      "Epoch: 66 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0854 | Train PPL:   1.0892\n",
      "\t Val. Loss: 2.1308 |  Val. PPL:   8.4219\n",
      "####################################################################################################\n",
      "| epoch  67 |     1/  351 batches | loss 0.0815\n",
      "| epoch  67 |   101/  351 batches | loss 0.0658\n",
      "| epoch  67 |   201/  351 batches | loss 0.0571\n",
      "| epoch  67 |   301/  351 batches | loss 0.0744\n",
      "Epoch: 67 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0866 | Train PPL:   1.0905\n",
      "\t Val. Loss: 2.1336 |  Val. PPL:   8.4448\n",
      "####################################################################################################\n",
      "| epoch  68 |     1/  351 batches | loss 0.0488\n",
      "| epoch  68 |   101/  351 batches | loss 0.0657\n",
      "| epoch  68 |   201/  351 batches | loss 0.1063\n",
      "| epoch  68 |   301/  351 batches | loss 0.0929\n",
      "Epoch: 68 | train.loss.best: 68\n",
      "Epoch: 68 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0827 | Train PPL:   1.0862\n",
      "\t Val. Loss: 2.1463 |  Val. PPL:   8.5533\n",
      "####################################################################################################\n",
      "| epoch  69 |     1/  351 batches | loss 0.0521\n",
      "| epoch  69 |   101/  351 batches | loss 0.0376\n",
      "| epoch  69 |   201/  351 batches | loss 0.0979\n",
      "| epoch  69 |   301/  351 batches | loss 0.1283\n",
      "Epoch: 69 | train.loss.best: 69\n",
      "Epoch: 69 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0789 | Train PPL:   1.0821\n",
      "\t Val. Loss: 2.1326 |  Val. PPL:   8.4371\n",
      "####################################################################################################\n",
      "| epoch  70 |     1/  351 batches | loss 0.0816\n",
      "| epoch  70 |   101/  351 batches | loss 0.0837\n",
      "| epoch  70 |   201/  351 batches | loss 0.0827\n",
      "| epoch  70 |   301/  351 batches | loss 0.0911\n",
      "Epoch: 70 | train.loss.best: 70\n",
      "Epoch: 70 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0778 | Train PPL:   1.0809\n",
      "\t Val. Loss: 2.1739 |  Val. PPL:   8.7929\n",
      "####################################################################################################\n",
      "| epoch  71 |     1/  351 batches | loss 0.0675\n",
      "| epoch  71 |   101/  351 batches | loss 0.0541\n",
      "| epoch  71 |   201/  351 batches | loss 0.0634\n",
      "| epoch  71 |   301/  351 batches | loss 0.0641\n",
      "Epoch: 71 | train.loss.best: 71\n",
      "Epoch: 71 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0758 | Train PPL:   1.0788\n",
      "\t Val. Loss: 2.1972 |  Val. PPL:   9.0002\n",
      "####################################################################################################\n",
      "| epoch  72 |     1/  351 batches | loss 0.0779\n",
      "| epoch  72 |   101/  351 batches | loss 0.0575\n",
      "| epoch  72 |   201/  351 batches | loss 0.0576\n",
      "| epoch  72 |   301/  351 batches | loss 0.0675\n",
      "Epoch: 72 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0763 | Train PPL:   1.0793\n",
      "\t Val. Loss: 2.2102 |  Val. PPL:   9.1175\n",
      "####################################################################################################\n",
      "| epoch  73 |     1/  351 batches | loss 0.0666\n",
      "| epoch  73 |   101/  351 batches | loss 0.0547\n",
      "| epoch  73 |   201/  351 batches | loss 0.1019\n",
      "| epoch  73 |   301/  351 batches | loss 0.1005\n",
      "Epoch: 73 | train.loss.best: 73\n",
      "Epoch: 73 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0748 | Train PPL:   1.0777\n",
      "\t Val. Loss: 2.2068 |  Val. PPL:   9.0869\n",
      "####################################################################################################\n",
      "| epoch  74 |     1/  351 batches | loss 0.0575\n",
      "| epoch  74 |   101/  351 batches | loss 0.0754\n",
      "| epoch  74 |   201/  351 batches | loss 0.0834\n",
      "| epoch  74 |   301/  351 batches | loss 0.0791\n",
      "Epoch: 74 | train.loss.best: 74\n",
      "Epoch: 74 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0716 | Train PPL:   1.0742\n",
      "\t Val. Loss: 2.1946 |  Val. PPL:   8.9763\n",
      "####################################################################################################\n",
      "| epoch  75 |     1/  351 batches | loss 0.0850\n",
      "| epoch  75 |   101/  351 batches | loss 0.0693\n",
      "| epoch  75 |   201/  351 batches | loss 0.0549\n",
      "| epoch  75 |   301/  351 batches | loss 0.0659\n",
      "Epoch: 75 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0730 | Train PPL:   1.0757\n",
      "\t Val. Loss: 2.1901 |  Val. PPL:   8.9364\n",
      "####################################################################################################\n",
      "| epoch  76 |     1/  351 batches | loss 0.0433\n",
      "| epoch  76 |   101/  351 batches | loss 0.0791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  76 |   201/  351 batches | loss 0.0654\n",
      "| epoch  76 |   301/  351 batches | loss 0.1071\n",
      "Epoch: 76 | train.loss.best: 76\n",
      "Epoch: 76 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0691 | Train PPL:   1.0716\n",
      "\t Val. Loss: 2.1882 |  Val. PPL:   8.9187\n",
      "####################################################################################################\n",
      "| epoch  77 |     1/  351 batches | loss 0.0908\n",
      "| epoch  77 |   101/  351 batches | loss 0.0775\n",
      "| epoch  77 |   201/  351 batches | loss 0.0558\n",
      "| epoch  77 |   301/  351 batches | loss 0.0852\n",
      "Epoch: 77 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0692 | Train PPL:   1.0716\n",
      "\t Val. Loss: 2.2230 |  Val. PPL:   9.2350\n",
      "####################################################################################################\n",
      "| epoch  78 |     1/  351 batches | loss 0.0658\n",
      "| epoch  78 |   101/  351 batches | loss 0.0370\n",
      "| epoch  78 |   201/  351 batches | loss 0.0597\n",
      "| epoch  78 |   301/  351 batches | loss 0.0804\n",
      "Epoch: 78 | train.loss.best: 78\n",
      "Epoch: 78 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0685 | Train PPL:   1.0709\n",
      "\t Val. Loss: 2.2372 |  Val. PPL:   9.3667\n",
      "####################################################################################################\n",
      "| epoch  79 |     1/  351 batches | loss 0.0406\n",
      "| epoch  79 |   101/  351 batches | loss 0.0831\n",
      "| epoch  79 |   201/  351 batches | loss 0.0961\n",
      "| epoch  79 |   301/  351 batches | loss 0.0639\n",
      "Epoch: 79 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0701 | Train PPL:   1.0726\n",
      "\t Val. Loss: 2.2293 |  Val. PPL:   9.2930\n",
      "####################################################################################################\n",
      "| epoch  80 |     1/  351 batches | loss 0.0732\n",
      "| epoch  80 |   101/  351 batches | loss 0.0587\n",
      "| epoch  80 |   201/  351 batches | loss 0.0888\n",
      "| epoch  80 |   301/  351 batches | loss 0.0647\n",
      "Epoch: 80 | train.loss.best: 80\n",
      "Epoch: 80 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0633 | Train PPL:   1.0654\n",
      "\t Val. Loss: 2.2218 |  Val. PPL:   9.2235\n",
      "####################################################################################################\n",
      "| epoch  81 |     1/  351 batches | loss 0.1114\n",
      "| epoch  81 |   101/  351 batches | loss 0.0369\n",
      "| epoch  81 |   201/  351 batches | loss 0.0504\n",
      "| epoch  81 |   301/  351 batches | loss 0.0521\n",
      "Epoch: 81 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0656 | Train PPL:   1.0678\n",
      "\t Val. Loss: 2.2092 |  Val. PPL:   9.1086\n",
      "####################################################################################################\n",
      "| epoch  82 |     1/  351 batches | loss 0.0790\n",
      "| epoch  82 |   101/  351 batches | loss 0.0622\n",
      "| epoch  82 |   201/  351 batches | loss 0.0560\n",
      "| epoch  82 |   301/  351 batches | loss 0.0698\n",
      "Epoch: 82 | train.loss.best: 82\n",
      "Epoch: 82 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0627 | Train PPL:   1.0647\n",
      "\t Val. Loss: 2.2585 |  Val. PPL:   9.5692\n",
      "####################################################################################################\n",
      "| epoch  83 |     1/  351 batches | loss 0.0496\n",
      "| epoch  83 |   101/  351 batches | loss 0.1013\n",
      "| epoch  83 |   201/  351 batches | loss 0.0433\n",
      "| epoch  83 |   301/  351 batches | loss 0.0465\n",
      "Epoch: 83 | train.loss.best: 83\n",
      "Epoch: 83 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0622 | Train PPL:   1.0642\n",
      "\t Val. Loss: 2.2656 |  Val. PPL:   9.6372\n",
      "####################################################################################################\n",
      "| epoch  84 |     1/  351 batches | loss 0.0497\n",
      "| epoch  84 |   101/  351 batches | loss 0.0933\n",
      "| epoch  84 |   201/  351 batches | loss 0.0385\n",
      "| epoch  84 |   301/  351 batches | loss 0.0912\n",
      "Epoch: 84 | train.loss.best: 84\n",
      "Epoch: 84 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0622 | Train PPL:   1.0642\n",
      "\t Val. Loss: 2.2152 |  Val. PPL:   9.1629\n",
      "####################################################################################################\n",
      "| epoch  85 |     1/  351 batches | loss 0.0565\n",
      "| epoch  85 |   101/  351 batches | loss 0.0232\n",
      "| epoch  85 |   201/  351 batches | loss 0.0595\n",
      "| epoch  85 |   301/  351 batches | loss 0.0654\n",
      "Epoch: 85 | train.loss.best: 85\n",
      "Epoch: 85 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0618 | Train PPL:   1.0637\n",
      "\t Val. Loss: 2.2356 |  Val. PPL:   9.3519\n",
      "####################################################################################################\n",
      "| epoch  86 |     1/  351 batches | loss 0.0692\n",
      "| epoch  86 |   101/  351 batches | loss 0.0495\n",
      "| epoch  86 |   201/  351 batches | loss 0.0524\n",
      "| epoch  86 |   301/  351 batches | loss 0.0928\n",
      "Epoch: 86 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0622 | Train PPL:   1.0641\n",
      "\t Val. Loss: 2.2473 |  Val. PPL:   9.4622\n",
      "####################################################################################################\n",
      "| epoch  87 |     1/  351 batches | loss 0.0619\n",
      "| epoch  87 |   101/  351 batches | loss 0.0934\n",
      "| epoch  87 |   201/  351 batches | loss 0.0588\n",
      "| epoch  87 |   301/  351 batches | loss 0.0901\n",
      "Epoch: 87 | train.loss.best: 87\n",
      "Epoch: 87 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0589 | Train PPL:   1.0607\n",
      "\t Val. Loss: 2.2696 |  Val. PPL:   9.6759\n",
      "####################################################################################################\n",
      "| epoch  88 |     1/  351 batches | loss 0.0563\n",
      "| epoch  88 |   101/  351 batches | loss 0.0369\n",
      "| epoch  88 |   201/  351 batches | loss 0.0434\n",
      "| epoch  88 |   301/  351 batches | loss 0.0671\n",
      "Epoch: 88 | train.loss.best: 88\n",
      "Epoch: 88 | Time: 0m 47s\n",
      "\tTrain Loss: 0.0581 | Train PPL:   1.0598\n",
      "\t Val. Loss: 2.2557 |  Val. PPL:   9.5415\n",
      "####################################################################################################\n",
      "| epoch  89 |     1/  351 batches | loss 0.0402\n",
      "| epoch  89 |   101/  351 batches | loss 0.0691\n",
      "| epoch  89 |   201/  351 batches | loss 0.0409\n",
      "| epoch  89 |   301/  351 batches | loss 0.0810\n",
      "Epoch: 89 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0590 | Train PPL:   1.0607\n",
      "\t Val. Loss: 2.2799 |  Val. PPL:   9.7757\n",
      "####################################################################################################\n",
      "| epoch  90 |     1/  351 batches | loss 0.0307\n",
      "| epoch  90 |   101/  351 batches | loss 0.0734\n",
      "| epoch  90 |   201/  351 batches | loss 0.0768\n",
      "| epoch  90 |   301/  351 batches | loss 0.0587\n",
      "Epoch    90: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 90 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0597 | Train PPL:   1.0615\n",
      "\t Val. Loss: 2.2598 |  Val. PPL:   9.5811\n",
      "####################################################################################################\n",
      "| epoch  91 |     1/  351 batches | loss 0.0536\n",
      "| epoch  91 |   101/  351 batches | loss 0.0396\n",
      "| epoch  91 |   201/  351 batches | loss 0.0249\n",
      "| epoch  91 |   301/  351 batches | loss 0.0309\n",
      "Epoch: 91 | train.loss.best: 91\n",
      "Epoch: 91 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0328 | Train PPL:   1.0333\n",
      "\t Val. Loss: 2.2487 |  Val. PPL:   9.4750\n",
      "####################################################################################################\n",
      "| epoch  92 |     1/  351 batches | loss 0.0181\n",
      "| epoch  92 |   101/  351 batches | loss 0.0178\n",
      "| epoch  92 |   201/  351 batches | loss 0.0263\n",
      "| epoch  92 |   301/  351 batches | loss 0.0154\n",
      "Epoch: 92 | train.loss.best: 92\n",
      "Epoch: 92 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0224 | Train PPL:   1.0227\n",
      "\t Val. Loss: 2.2545 |  Val. PPL:   9.5302\n",
      "####################################################################################################\n",
      "| epoch  93 |     1/  351 batches | loss 0.0169\n",
      "| epoch  93 |   101/  351 batches | loss 0.0128\n",
      "| epoch  93 |   201/  351 batches | loss 0.0175\n",
      "| epoch  93 |   301/  351 batches | loss 0.0136\n",
      "Epoch: 93 | train.loss.best: 93\n",
      "Epoch: 93 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0192 | Train PPL:   1.0194\n",
      "\t Val. Loss: 2.2578 |  Val. PPL:   9.5616\n",
      "####################################################################################################\n",
      "| epoch  94 |     1/  351 batches | loss 0.0185\n",
      "| epoch  94 |   101/  351 batches | loss 0.0291\n",
      "| epoch  94 |   201/  351 batches | loss 0.0241\n",
      "| epoch  94 |   301/  351 batches | loss 0.0281\n",
      "Epoch: 94 | train.loss.best: 94\n",
      "Epoch: 94 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0182 | Train PPL:   1.0184\n",
      "\t Val. Loss: 2.2815 |  Val. PPL:   9.7913\n",
      "####################################################################################################\n",
      "| epoch  95 |     1/  351 batches | loss 0.0247\n",
      "| epoch  95 |   101/  351 batches | loss 0.0160\n",
      "| epoch  95 |   201/  351 batches | loss 0.0381\n",
      "| epoch  95 |   301/  351 batches | loss 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0189 | Train PPL:   1.0191\n",
      "\t Val. Loss: 2.2706 |  Val. PPL:   9.6849\n",
      "####################################################################################################\n",
      "| epoch  96 |     1/  351 batches | loss 0.0356\n",
      "| epoch  96 |   101/  351 batches | loss 0.0108\n",
      "| epoch  96 |   201/  351 batches | loss 0.0301\n",
      "| epoch  96 |   301/  351 batches | loss 0.0100\n",
      "Epoch    96: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch: 96 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0199 | Train PPL:   1.0201\n",
      "\t Val. Loss: 2.2739 |  Val. PPL:   9.7172\n",
      "####################################################################################################\n",
      "| epoch  97 |     1/  351 batches | loss 0.0256\n",
      "| epoch  97 |   101/  351 batches | loss 0.0209\n",
      "| epoch  97 |   201/  351 batches | loss 0.0043\n",
      "| epoch  97 |   301/  351 batches | loss 0.0075\n",
      "Epoch: 97 | train.loss.best: 97\n",
      "Epoch: 97 | Time: 0m 49s\n",
      "\tTrain Loss: 0.0153 | Train PPL:   1.0154\n",
      "\t Val. Loss: 2.2537 |  Val. PPL:   9.5233\n",
      "####################################################################################################\n",
      "| epoch  98 |     1/  351 batches | loss 0.0097\n",
      "| epoch  98 |   101/  351 batches | loss 0.0157\n",
      "| epoch  98 |   201/  351 batches | loss 0.0071\n",
      "| epoch  98 |   301/  351 batches | loss 0.0052\n",
      "Epoch: 98 | train.loss.best: 98\n",
      "Epoch: 98 | Time: 0m 49s\n",
      "\tTrain Loss: 0.0117 | Train PPL:   1.0117\n",
      "\t Val. Loss: 2.2532 |  Val. PPL:   9.5180\n",
      "####################################################################################################\n",
      "| epoch  99 |     1/  351 batches | loss 0.0069\n",
      "| epoch  99 |   101/  351 batches | loss 0.0083\n",
      "| epoch  99 |   201/  351 batches | loss 0.0048\n",
      "| epoch  99 |   301/  351 batches | loss 0.0140\n",
      "Epoch: 99 | train.loss.best: 99\n",
      "Epoch: 99 | Time: 0m 48s\n",
      "\tTrain Loss: 0.0109 | Train PPL:   1.0109\n",
      "\t Val. Loss: 2.2706 |  Val. PPL:   9.6848\n",
      "####################################################################################################\n",
      "| epoch 100 |     1/  351 batches | loss 0.0031\n",
      "| epoch 100 |   101/  351 batches | loss 0.0040\n",
      "| epoch 100 |   201/  351 batches | loss 0.0065\n",
      "| epoch 100 |   301/  351 batches | loss 0.0080\n",
      "Epoch: 100 | train.loss.best: 100\n",
      "Epoch: 100 | Time: 0m 49s\n",
      "\tTrain Loss: 0.0108 | Train PPL:   1.0109\n",
      "\t Val. Loss: 2.2774 |  Val. PPL:   9.7518\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100 # 25\n",
    "CLIP = 5\n",
    "\n",
    "best_train_loss = float('inf')\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, epoch)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), './models/transformer/train.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | train.loss.best: {epoch+1}')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/transformer/valid.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | valid.loss.best: {epoch+1}')\n",
    "        \n",
    "    # lr_scheduler.step(valid_loss)\n",
    "    lr_scheduler.step(train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):8.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {math.exp(valid_loss):8.4f}')\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.2774 | Test PPL:   9.7518 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/transformer/train.loss.best.pt'))\n",
    "\n",
    "test_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):8.4f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference (검증용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, q_sent=\"\", a_sent=None, char2index=None, index2char=None):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        src = [char2index.get(SOS_TOKEN)]\n",
    "        src += [char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        trg = [char2index.get(SOS_TOKEN)]\n",
    "        trg += [char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        trg += [char2index.get(EOS_TOKEN)]\n",
    "\n",
    "        src = torch.LongTensor([src]).to(device)\n",
    "        trg = torch.LongTensor([trg]).to(device)\n",
    "\n",
    "        hyp_ys, hyp_indice = model.search(enc_input=src, max_length=80)\n",
    "        \n",
    "        pred = hyp_indice[0].detach().cpu().numpy()\n",
    "        print(pred)\n",
    "        \n",
    "        pred_sent = [index2char[token_id] for token_id in pred]\n",
    "        pred_sent = ''.join(pred_sent)\n",
    "        \n",
    "        print(f\"H: ({pred_sent})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random idx :  2732\n",
      "Q:  사랑이 밥 먹여주나\n",
      "A:  사랑이 밥은 먹여주지 않지만 행복을 줘요.\n",
      "[ 638  406  857    4  549  852    4  497  791  913  930    4  756  930\n",
      "  480    4 1186  579  853    4  920  825   10    3]\n",
      "H: (사랑이 밥은 먹여주지 않지만 행복을 줘요.<eos>)\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(train_dataset), size=1)[0]\n",
    "print(\"random idx : \", idx)\n",
    "\n",
    "q_sent = train_dataset.data[idx][0]\n",
    "a_sent = train_dataset.data[idx][1]\n",
    "print(\"Q: \", q_sent)\n",
    "print(\"A: \", a_sent)\n",
    "\n",
    "inference(model, q_sent, a_sent, char2index=train_dataset.char2index, index2char=train_dataset.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
