{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 내용: SEQ2SEQ 모델링\n",
    "\n",
    "### 1. Vocab: 한국어 음절 단위\n",
    "\n",
    "### 2. 데이터: 한국어 Q&A 문장\n",
    " - ex.)공무원 시험 죽을 거 같아 --> 철밥통 되기가 어디 쉽겠어요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation (초기 환경 세팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 데이터 확인\n",
    " - 출처: https://github.com/eagle705/pytorch-transformer-chatbot/tree/master/data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/train_chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/valid_chatbot.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 어휘사전 (Vocab) 생성 // (음절 단위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "def create_vocab(train_path, valid_path, vocab_path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    with open(valid_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    vocab = set()\n",
    "    for sent in data:\n",
    "        for char in sent:\n",
    "            vocab.add(char)\n",
    "            \n",
    "    vocab_list = list(sorted(vocab))\n",
    "    \n",
    "    vocab_list.insert(0, PAD_TOKEN)\n",
    "    vocab_list.insert(1, UNK_TOKEN)\n",
    "    vocab_list.insert(2, SOS_TOKEN)\n",
    "    vocab_list.insert(3, EOS_TOKEN)\n",
    "    \n",
    "    print(vocab_list)\n",
    "\n",
    "    # 파일로 어휘사전 저장\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(vocab_list, indent=4, ensure_ascii=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<sos>', '<eos>', ' ', '!', '%', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'L', 'N', 'O', 'P', 'S', 'X', '_', 'a', 'c', 'g', 'j', 'k', 'n', 'o', 's', '~', '…', 'ㅊ', 'ㅋ', 'ㅎ', 'ㅜ', 'ㅠ', '가', '각', '간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '갚', '개', '객', '갠', '갯', '갱', '걍', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '겉', '게', '겐', '겜', '겟', '겠', '겨', '격', '겪', '견', '결', '겹', '겼', '경', '곁', '계', '곗', '고', '곡', '곤', '곧', '골', '곰', '곱', '곳', '공', '과', '관', '광', '괘', '괜', '괴', '교', '구', '국', '군', '굳', '굴', '굶', '굽', '굿', '궁', '궈', '권', '궜', '귀', '귄', '귈', '귐', '규', '균', '귤', '그', '극', '근', '글', '긁', '금', '급', '긋', '긍', '기', '긴', '길', '김', '깃', '깅', '깊', '까', '깍', '깎', '깐', '깔', '깜', '깝', '깠', '깡', '깨', '깬', '깰', '깼', '꺼', '꺽', '껀', '껄', '껏', '께', '껴', '꼈', '꼬', '꼭', '꼰', '꼴', '꼼', '꼿', '꽁', '꽂', '꽃', '꽈', '꽉', '꽝', '꽤', '꾸', '꾹', '꾼', '꿀', '꿈', '꿎', '꿔', '꿧', '꿨', '꿩', '꿰', '뀌', '뀐', '뀔', '끄', '끈', '끊', '끌', '끓', '끔', '끗', '끝', '끼', '낀', '낄', '낌', '나', '낙', '낚', '난', '날', '남', '납', '낫', '났', '낭', '낮', '낳', '내', '낸', '낼', '냄', '냅', '냈', '냉', '냐', '냥', '너', '넋', '넌', '널', '넓', '넘', '넛', '넣', '네', '넥', '넷', '넹', '녀', '녁', '년', '념', '녔', '녕', '노', '녹', '논', '놀', '놈', '놉', '농', '높', '놓', '놔', '놨', '뇌', '뇨', '뇽', '누', '눅', '눈', '눌', '눕', '눠', '눴', '뉴', '느', '는', '늘', '늙', '늠', '능', '늦', '늪', '니', '닉', '닌', '닐', '님', '닙', '닝', '다', '닥', '닦', '단', '닫', '달', '닭', '닮', '닳', '담', '답', '닷', '당', '닿', '대', '댄', '댈', '댓', '댔', '댜', '더', '덕', '던', '덜', '덤', '덥', '덧', '덩', '덮', '데', '덴', '델', '뎌', '뎠', '도', '독', '돈', '돋', '돌', '돕', '동', '돼', '됐', '되', '된', '될', '됨', '됩', '됬', '두', '둑', '둔', '둘', '둠', '둥', '둬', '뒀', '뒤', '뒷', '뒹', '듀', '드', '득', '든', '듣', '들', '듦', '듬', '듭', '듯', '등', '디', '딘', '딛', '딜', '딧', '딨', '딩', '딪', '따', '딱', '딴', '딸', '땀', '땅', '때', '땐', '땜', '땠', '땡', '떄', '떠', '떡', '떤', '떨', '떴', '떻', '떼', '뗄', '또', '똑', '똥', '뚝', '뚫', '뚱', '뛰', '뛴', '뜁', '뜨', '뜩', '뜬', '뜰', '뜸', '뜻', '띄', '띠', '띰', '띵', '라', '락', '란', '랄', '람', '랍', '랐', '랑', '랖', '래', '랙', '랜', '랠', '램', '랩', '랫', '랬', '랭', '량', '러', '런', '럴', '럼', '럽', '럿', '렀', '렁', '렇', '레', '렉', '렌', '렐', '렘', '렛', '렜', '려', '력', '련', '렬', '렴', '렵', '렷', '렸', '령', '례', '로', '록', '론', '롤', '롭', '롯', '롱', '뢰', '료', '루', '룩', '룰', '룸', '룽', '뤄', '류', '륜', '률', '르', '륵', '른', '를', '름', '릅', '릇', '릎', '리', '릭', '린', '릴', '림', '립', '릿', '링', '마', '막', '만', '많', '말', '맘', '맙', '맛', '망', '맞', '맡', '매', '맥', '맨', '맴', '맷', '맹', '맺', '머', '먹', '먼', '멀', '멈', '멋', '멍', '메', '멘', '멤', '며', '면', '명', '몇', '모', '목', '몫', '몬', '몰', '몸', '몹', '못', '몽', '묘', '무', '묵', '문', '묻', '물', '뭇', '뭉', '뭐', '뭔', '뭘', '뭣', '뮤', '미', '민', '믿', '밀', '밉', '밌', '밍', '밑', '바', '박', '밖', '반', '받', '발', '밝', '밟', '밤', '밥', '방', '밭', '배', '백', '밸', '뱃', '뱄', '뱉', '버', '벅', '번', '벋', '벌', '범', '법', '벗', '벚', '베', '벤', '벨', '벴', '벼', '벽', '변', '별', '볍', '병', '볕', '보', '복', '볶', '본', '볼', '봄', '봅', '봇', '봉', '봐', '봤', '뵈', '부', '북', '분', '불', '붓', '붕', '붙', '뷔', '브', '블', '비', '빈', '빌', '빔', '빗', '빙', '빚', '빛', '빠', '빡', '빨', '빴', '빵', '빼', '빽', '뺄', '뺏', '뺴', '뻐', '뻑', '뻔', '뻘', '뻣', '뻤', '뻥', '뽀', '뽑', '뽕', '뿅', '뿌', '뿍', '뿐', '쁘', '쁜', '쁠', '쁨', '삐', '삔', '사', '삭', '산', '살', '삶', '삼', '삽', '삿', '샀', '상', '새', '색', '샌', '샐', '샘', '샜', '생', '샤', '서', '석', '섞', '선', '섣', '설', '섬', '섭', '섯', '섰', '성', '세', '섹', '센', '셀', '셔', '션', '셥', '셨', '소', '속', '손', '솔', '솜', '송', '쇠', '쇼', '숍', '숏', '수', '숙', '순', '술', '숨', '숫', '숭', '쉬', '쉴', '쉼', '쉽', '슈', '스', '슨', '슬', '슴', '습', '슷', '승', '시', '식', '신', '실', '싫', '심', '십', '싱', '싶', '싸', '싹', '싼', '쌀', '쌈', '쌍', '쌓', '쌤', '쌩', '써', '썩', '썰', '썸', '썹', '썼', '쎄', '쎈', '쎌', '쏘', '쏜', '쏟', '쏠', '쐬', '쑤', '쑥', '쓰', '쓴', '쓸', '씀', '씁', '씌', '씨', '씩', '씬', '씰', '씸', '씹', '씻', '씽', '아', '악', '안', '앉', '않', '알', '압', '앗', '았', '앙', '앞', '애', '액', '앨', '앱', '야', '약', '얄', '얇', '양', '얘', '어', '억', '언', '얻', '얼', '얽', '엄', '업', '없', '엇', '었', '엉', '엊', '에', '엔', '엘', '엠', '엣', '여', '역', '엮', '연', '열', '염', '엽', '엿', '였', '영', '옆', '옇', '예', '옛', '오', '옥', '온', '올', '옮', '옳', '옴', '옵', '옷', '와', '완', '왓', '왔', '왕', '왜', '왠', '왤', '외', '왼', '욌', '요', '욕', '욜', '용', '우', '욱', '운', '울', '움', '웁', '웃', '워', '원', '월', '웠', '웨', '웬', '웹', '웽', '위', '윗', '윙', '유', '육', '윤', '율', '으', '은', '을', '음', '응', '의', '이', '익', '인', '일', '읽', '잃', '임', '입', '잇', '있', '잊', '잌', '자', '작', '잔', '잖', '잘', '잠', '잡', '잤', '장', '잦', '재', '잼', '잿', '쟁', '저', '적', '전', '절', '젊', '점', '접', '젔', '정', '젖', '제', '젝', '젠', '젤', '젯', '져', '젹', '졋', '졌', '조', '족', '존', '졸', '좀', '좁', '종', '좋', '좌', '죄', '죠', '주', '죽', '준', '줄', '줌', '줍', '중', '줘', '줬', '쥐', '쥬', '즈', '즉', '즐', '즘', '즙', '증', '지', '직', '진', '질', '짐', '집', '짓', '징', '짚', '짜', '짝', '짠', '짤', '짧', '짬', '짰', '짱', '째', '짼', '쨌', '쩌', '쩍', '쩐', '쩔', '쩝', '쩡', '쪄', '쪘', '쪼', '쪽', '쫄', '쫌', '쫙', '쭈', '쭉', '쭤', '쯤', '찌', '찍', '찐', '찔', '찜', '찝', '찡', '찢', '차', '착', '찬', '찮', '찰', '참', '찹', '찼', '창', '찾', '채', '책', '챔', '챗', '챘', '챙', '처', '척', '천', '철', '첨', '첩', '첫', '청', '체', '쳇', '쳐', '쳤', '초', '촉', '촌', '총', '촬', '최', '추', '축', '춘', '출', '춤', '춥', '충', '춰', '췄', '취', '츄', '츠', '측', '츤', '층', '치', '칙', '친', '칠', '침', '칩', '칫', '칭', '카', '칼', '캄', '캐', '캔', '캬', '커', '컥', '컨', '컴', '컷', '컸', '컹', '케', '켓', '켜', '켠', '켰', '코', '콕', '콘', '콜', '콤', '콧', '콩', '쾌', '쿠', '쿨', '쿵', '쿼', '퀴', '큐', '크', '큰', '클', '큼', '킁', '키', '킥', '킨', '킬', '킴', '킹', '타', '탁', '탄', '탈', '탐', '탑', '탓', '탔', '탕', '태', '택', '탱', '터', '턱', '턴', '털', '텀', '텁', '텄', '텅', '테', '텍', '텐', '텔', '템', '텨', '텻', '텼', '토', '톡', '톤', '톱', '통', '퇴', '투', '툭', '툰', '툴', '툼', '퉁', '퉜', '튀', '튜', '트', '특', '틀', '틈', '틋', '티', '틱', '팀', '팁', '팅', '파', '팍', '판', '팔', '팠', '패', '팩', '팬', '팸', '퍼', '펑', '페', '펜', '펨', '펭', '펴', '편', '펼', '평', '폐', '포', '폭', '폰', '폼', '퐈', '표', '푸', '푹', '푼', '풀', '품', '풋', '풍', '퓨', '프', '픈', '플', '픔', '픕', '피', '픽', '핀', '필', '핍', '핏', '핑', '하', '학', '한', '할', '함', '합', '핫', '항', '해', '핸', '햇', '했', '행', '햐', '향', '허', '헉', '헌', '헐', '험', '헛', '헤', '헥', '헬', '헷', '헹', '혀', '현', '혈', '혐', '협', '혔', '형', '혜', '호', '혹', '혼', '홀', '홈', '화', '확', '환', '활', '홧', '황', '회', '획', '효', '후', '훅', '훈', '훌', '훔', '훨', '휘', '휙', '휨', '휴', '흐', '흑', '흔', '흘', '흠', '흡', '흥', '희', '흰', '히', '힌', '힐', '힘', '힙']\n"
     ]
    }
   ],
   "source": [
    "create_vocab(train_path=\"./data/train_chatbot.txt\",\n",
    "             valid_path=\"./data/valid_chatbot.txt\",\n",
    "             vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnADataset(Dataset):\n",
    "    def __init__(self, data_path, vocab_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char2index, self.index2char = self._read_vocab(vocab_path)\n",
    "        self.data = self._preprocess(data_path)\n",
    "    \n",
    "    def _read_vocab(self, vocab_path):\n",
    "        with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "            labels = json.load(f)\n",
    "            char2index = dict()\n",
    "            index2char = dict()\n",
    "\n",
    "            for index, char in enumerate(labels):\n",
    "                char2index[char] = index\n",
    "                index2char[index] = char\n",
    "            \n",
    "        return char2index, index2char\n",
    "    \n",
    "    def _preprocess(self, data_path):\n",
    "        data = []\n",
    "        \n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                sents = line.strip().split('\\t')\n",
    "                assert len(sents) == 2, \"data error!!\"\n",
    "                question_sent, answer_sent = sents[0], sents[1]\n",
    "                \n",
    "                data.append((question_sent, answer_sent))\n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.char2index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        qna = self.data[index]\n",
    "        q_sent, a_sent = qna[0], qna[1]\n",
    "        \n",
    "        src = [self.char2index.get(SOS_TOKEN)]\n",
    "        src += [self.char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        tgt = [self.char2index.get(SOS_TOKEN)]\n",
    "        tgt += [self.char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        tgt += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        return torch.LongTensor(src), torch.LongTensor(tgt)\n",
    "    \n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    xs = [x for x, y in batch]\n",
    "    xs_pad = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    xs_lengths = [x.size(0) for x, y in batch]\n",
    "    xs_lengths = torch.LongTensor(xs_lengths)\n",
    "\n",
    "    ys = [y for x, y in batch]\n",
    "    ys_pad = torch.nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    ys_lengths = [y.size(0) for x, y in batch]\n",
    "    ys_lengths = torch.LongTensor(ys_lengths)\n",
    "\n",
    "    return xs_pad, xs_lengths, ys_pad, ys_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QnADataset(data_path=\"./data/train_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")\n",
    "\n",
    "valid_dataset = QnADataset(data_path=\"./data/valid_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # 4\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   69,  936,  ...,    0,    0,    0],\n",
      "        [   2,  779,  478,  ...,    0,    0,    0],\n",
      "        [   2,  822,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  205,  465,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,    0,    0,    0],\n",
      "        [   2,  805,  268,  ...,    0,    0,    0]]) tensor([[   2,  932,  707,  ...,    0,    0,    0],\n",
      "        [   2,  704,  773,  ...,    0,    0,    0],\n",
      "        [   2,  648,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  647,  908,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,  825,   10,    3],\n",
      "        [   2,  129,  480,  ...,    0,    0,    0]])\n",
      "tensor([14, 14, 15, 15, 15, 12, 11, 14,  9, 11, 10,  8, 10, 16, 13, 40, 17, 13,\n",
      "        24, 11, 11,  4, 13,  8, 16, 35, 27, 20, 17, 19, 11, 12]) tensor([20, 18, 17, 10, 21, 17, 14, 24, 10, 13, 15, 14, 15, 12, 24, 20, 13, 22,\n",
      "        25, 12, 15, 13, 14, 24, 11, 22, 21, 30, 15, 10, 32, 19])\n"
     ]
    }
   ],
   "source": [
    "for x, x_len, y, y_len in train_loader:\n",
    "    print(x, y)\n",
    "    print(x_len, y_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- src = [batch size. src len]\n",
    "- embedded = [batch size, src len, emb dim]\n",
    "- outputs = [batch size, src len, hid dim * n directions]\n",
    "- hidden = [batch size, n layers * n directions, hid dim]\n",
    "- cell = [batch size, n layers * n directions, hid dim]\n",
    "- outputs are always from the top hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True) # batch_first=True\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input = [batch size]\n",
    "- hidden = [batch size, n layers * n directions, hid dim]\n",
    "- cell = [batch size, n layers * n directions, hid dim]\n",
    "        \n",
    "- n directions in the decoder will both always be 1, therefore:\n",
    "- hidden = [batch size, n layers, hid dim]\n",
    "- context = [batch size, n layers, hid dim]\n",
    "\n",
    "- input = [batch size, 1]\n",
    "- embedded = [batch size, 1, emb dim]\n",
    "- prediction = [batch size, output dim]\n",
    "- output = [batch size, seq len, hid dim * n directions]\n",
    "- hidden = [batch size, n layers * n directions, hid dim]\n",
    "- cell = [batch size, n layers * n directions, hid dim]\n",
    "        \n",
    "- seq len and n directions will always be 1 in the decoder, therefore:\n",
    "- output = [batch size, 1, hid dim]\n",
    "- hidden = [batch size, n layers, hid dim]\n",
    "- cell = [batch size, n layers, hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True) # batch_first=True\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- src = [batch size, src len]\n",
    "- trg = [batch size, trg len]\n",
    "- teacher_forcing_ratio is probability to use teacher forcing\n",
    "- e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "- tensor to store decoder outputs\n",
    "- last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "- first input to the decoder is the `<sos>` tokens\n",
    "- insert input token embedding, previous hidden and previous cell states\n",
    "- receive output tensor (predictions) and new hidden and cell states\n",
    "- place predictions in a tensor holding predictions for each token\n",
    "- decide if we are going to use teacher forcing or not\n",
    "- get the highest predicted token from our predictions\n",
    "- if teacher forcing, use actual next token as next input\n",
    "- if not, use predicted token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder mu st be equal!\"\n",
    "        \n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n",
    "        outputs = outputs.transpose(1, 0).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    " \n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(1246, 256)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(1246, 256)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=512, out_features=1246, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = train_dataset.vocab_size\n",
    "OUTPUT_DIM = train_dataset.vocab_size\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5 \n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trg = [batch size, trg len]\n",
    "- output = [batch size, trg len, output dim]\n",
    "- trg = [(trg len - 1) * batch size]\n",
    "- output = [(trg len - 1) * batch size, output dim]\n",
    "- trg = [batch size, trg len]\n",
    "- output = [batch size, trg len, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=PAD_TOKEN_ID).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, clip, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src, src_len, trg, trg_len = batch\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        \n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        log_interval = 100\n",
    "        \n",
    "        if i % log_interval == 0 and i >= 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | loss {:.4f}'.format(epoch+1, i+1, len(data_loader), loss.detach().item()))\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trg = [batch size, trg len]\n",
    "- output = [batch size, trg len, output dim]\n",
    "- trg = [(trg len - 1) * batch size]\n",
    "- output = [(trg len - 1) * batch size, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src, src_len, trg, trg_len = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  351 batches | loss 7.1277\n",
      "| epoch   1 |   101/  351 batches | loss 4.2795\n",
      "| epoch   1 |   201/  351 batches | loss 4.2831\n",
      "| epoch   1 |   301/  351 batches | loss 3.9870\n",
      "Epoch: 01 | train.loss.best: 1\n",
      "Epoch: 01 | valid.loss.best: 1\n",
      "Epoch: 01 | Time: 0m 25s\n",
      "\tTrain Loss: 4.2795 | Train PPL:  72.2058\n",
      "\t Val. Loss: 4.5660 |  Val. PPL:  96.1544\n",
      "####################################################################################################\n",
      "| epoch   2 |     1/  351 batches | loss 4.0984\n",
      "| epoch   2 |   101/  351 batches | loss 3.7460\n",
      "| epoch   2 |   201/  351 batches | loss 4.0170\n",
      "| epoch   2 |   301/  351 batches | loss 4.0312\n",
      "Epoch: 02 | train.loss.best: 2\n",
      "Epoch: 02 | Time: 0m 24s\n",
      "\tTrain Loss: 3.9178 | Train PPL:  50.2901\n",
      "\t Val. Loss: 4.5894 |  Val. PPL:  98.4327\n",
      "####################################################################################################\n",
      "| epoch   3 |     1/  351 batches | loss 3.9413\n",
      "| epoch   3 |   101/  351 batches | loss 3.8912\n",
      "| epoch   3 |   201/  351 batches | loss 3.7123\n",
      "| epoch   3 |   301/  351 batches | loss 3.5094\n",
      "Epoch: 03 | train.loss.best: 3\n",
      "Epoch     3: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: 03 | Time: 0m 25s\n",
      "\tTrain Loss: 3.7988 | Train PPL:  44.6470\n",
      "\t Val. Loss: 4.5861 |  Val. PPL:  98.1096\n",
      "####################################################################################################\n",
      "| epoch   4 |     1/  351 batches | loss 3.7564\n",
      "| epoch   4 |   101/  351 batches | loss 3.7155\n",
      "| epoch   4 |   201/  351 batches | loss 3.4148\n",
      "| epoch   4 |   301/  351 batches | loss 3.6483\n",
      "Epoch: 04 | train.loss.best: 4\n",
      "Epoch: 04 | valid.loss.best: 4\n",
      "Epoch: 04 | Time: 0m 25s\n",
      "\tTrain Loss: 3.6792 | Train PPL:  39.6146\n",
      "\t Val. Loss: 4.5214 |  Val. PPL:  91.9656\n",
      "####################################################################################################\n",
      "| epoch   5 |     1/  351 batches | loss 3.3576\n",
      "| epoch   5 |   101/  351 batches | loss 3.5334\n",
      "| epoch   5 |   201/  351 batches | loss 3.9055\n",
      "| epoch   5 |   301/  351 batches | loss 3.2564\n",
      "Epoch: 05 | train.loss.best: 5\n",
      "Epoch: 05 | Time: 0m 25s\n",
      "\tTrain Loss: 3.6316 | Train PPL:  37.7716\n",
      "\t Val. Loss: 4.5505 |  Val. PPL:  94.6822\n",
      "####################################################################################################\n",
      "| epoch   6 |     1/  351 batches | loss 3.8136\n",
      "| epoch   6 |   101/  351 batches | loss 3.8080\n",
      "| epoch   6 |   201/  351 batches | loss 3.4913\n",
      "| epoch   6 |   301/  351 batches | loss 3.6055\n",
      "Epoch: 06 | train.loss.best: 6\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 06 | Time: 0m 25s\n",
      "\tTrain Loss: 3.5991 | Train PPL:  36.5647\n",
      "\t Val. Loss: 4.5289 |  Val. PPL:  92.6531\n",
      "####################################################################################################\n",
      "| epoch   7 |     1/  351 batches | loss 3.5079\n",
      "| epoch   7 |   101/  351 batches | loss 3.8023\n",
      "| epoch   7 |   201/  351 batches | loss 3.4203\n",
      "| epoch   7 |   301/  351 batches | loss 3.7126\n",
      "Epoch: 07 | train.loss.best: 7\n",
      "Epoch: 07 | Time: 0m 25s\n",
      "\tTrain Loss: 3.5518 | Train PPL:  34.8754\n",
      "\t Val. Loss: 4.5391 |  Val. PPL:  93.6107\n",
      "####################################################################################################\n",
      "| epoch   8 |     1/  351 batches | loss 3.3769\n",
      "| epoch   8 |   101/  351 batches | loss 3.3910\n",
      "| epoch   8 |   201/  351 batches | loss 3.6927\n",
      "| epoch   8 |   301/  351 batches | loss 3.2375\n",
      "Epoch: 08 | train.loss.best: 8\n",
      "Epoch     8: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: 08 | Time: 0m 25s\n",
      "\tTrain Loss: 3.5218 | Train PPL:  33.8439\n",
      "\t Val. Loss: 4.5339 |  Val. PPL:  93.1197\n",
      "####################################################################################################\n",
      "| epoch   9 |     1/  351 batches | loss 3.8336\n",
      "| epoch   9 |   101/  351 batches | loss 3.7801\n",
      "| epoch   9 |   201/  351 batches | loss 3.3532\n",
      "| epoch   9 |   301/  351 batches | loss 3.2750\n",
      "Epoch: 09 | Time: 0m 25s\n",
      "\tTrain Loss: 3.5261 | Train PPL:  33.9916\n",
      "\t Val. Loss: 4.5810 |  Val. PPL:  97.6116\n",
      "####################################################################################################\n",
      "| epoch  10 |     1/  351 batches | loss 3.4825\n",
      "| epoch  10 |   101/  351 batches | loss 3.5922\n",
      "| epoch  10 |   201/  351 batches | loss 3.6619\n",
      "| epoch  10 |   301/  351 batches | loss 3.3598\n",
      "Epoch: 10 | train.loss.best: 10\n",
      "Epoch    10: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: 10 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4996 | Train PPL:  33.1023\n",
      "\t Val. Loss: 4.5302 |  Val. PPL:  92.7743\n",
      "####################################################################################################\n",
      "| epoch  11 |     1/  351 batches | loss 3.7758\n",
      "| epoch  11 |   101/  351 batches | loss 3.3318\n",
      "| epoch  11 |   201/  351 batches | loss 3.9128\n",
      "| epoch  11 |   301/  351 batches | loss 3.2977\n",
      "Epoch: 11 | train.loss.best: 11\n",
      "Epoch: 11 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4912 | Train PPL:  32.8259\n",
      "\t Val. Loss: 4.5328 |  Val. PPL:  93.0229\n",
      "####################################################################################################\n",
      "| epoch  12 |     1/  351 batches | loss 3.2781\n",
      "| epoch  12 |   101/  351 batches | loss 3.4652\n",
      "| epoch  12 |   201/  351 batches | loss 3.2209\n",
      "| epoch  12 |   301/  351 batches | loss 3.9257\n",
      "Epoch: 12 | train.loss.best: 12\n",
      "Epoch    12: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch: 12 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4679 | Train PPL:  32.0707\n",
      "\t Val. Loss: 4.5251 |  Val. PPL:  92.3013\n",
      "####################################################################################################\n",
      "| epoch  13 |     1/  351 batches | loss 3.2453\n",
      "| epoch  13 |   101/  351 batches | loss 3.3018\n",
      "| epoch  13 |   201/  351 batches | loss 3.1992\n",
      "| epoch  13 |   301/  351 batches | loss 3.6548\n",
      "Epoch: 13 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4777 | Train PPL:  32.3861\n",
      "\t Val. Loss: 4.5272 |  Val. PPL:  92.5031\n",
      "####################################################################################################\n",
      "| epoch  14 |     1/  351 batches | loss 3.5705\n",
      "| epoch  14 |   101/  351 batches | loss 3.4055\n",
      "| epoch  14 |   201/  351 batches | loss 3.5920\n",
      "| epoch  14 |   301/  351 batches | loss 3.1561\n",
      "Epoch: 14 | train.loss.best: 14\n",
      "Epoch: 14 | valid.loss.best: 14\n",
      "Epoch: 14 | Time: 0m 24s\n",
      "\tTrain Loss: 3.4659 | Train PPL:  32.0047\n",
      "\t Val. Loss: 4.5185 |  Val. PPL:  91.6941\n",
      "####################################################################################################\n",
      "| epoch  15 |     1/  351 batches | loss 3.6618\n",
      "| epoch  15 |   101/  351 batches | loss 3.0766\n",
      "| epoch  15 |   201/  351 batches | loss 3.6535\n",
      "| epoch  15 |   301/  351 batches | loss 3.7850\n",
      "Epoch: 15 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4791 | Train PPL:  32.4298\n",
      "\t Val. Loss: 4.5205 |  Val. PPL:  91.8785\n",
      "####################################################################################################\n",
      "| epoch  16 |     1/  351 batches | loss 4.0221\n",
      "| epoch  16 |   101/  351 batches | loss 3.7716\n",
      "| epoch  16 |   201/  351 batches | loss 3.2898\n",
      "| epoch  16 |   301/  351 batches | loss 3.7247\n",
      "Epoch    16: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch: 16 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4746 | Train PPL:  32.2834\n",
      "\t Val. Loss: 4.5245 |  Val. PPL:  92.2528\n",
      "####################################################################################################\n",
      "| epoch  17 |     1/  351 batches | loss 3.2954\n",
      "| epoch  17 |   101/  351 batches | loss 3.4083\n",
      "| epoch  17 |   201/  351 batches | loss 3.4528\n",
      "| epoch  17 |   301/  351 batches | loss 3.4481\n",
      "Epoch: 17 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4695 | Train PPL:  32.1214\n",
      "\t Val. Loss: 4.5366 |  Val. PPL:  93.3748\n",
      "####################################################################################################\n",
      "| epoch  18 |     1/  351 batches | loss 3.4184\n",
      "| epoch  18 |   101/  351 batches | loss 3.6332\n",
      "| epoch  18 |   201/  351 batches | loss 3.6584\n",
      "| epoch  18 |   301/  351 batches | loss 3.6711\n",
      "Epoch    18: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch: 18 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4826 | Train PPL:  32.5445\n",
      "\t Val. Loss: 4.5229 |  Val. PPL:  92.1013\n",
      "####################################################################################################\n",
      "| epoch  19 |     1/  351 batches | loss 3.4999\n",
      "| epoch  19 |   101/  351 batches | loss 3.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |   201/  351 batches | loss 3.3776\n",
      "| epoch  19 |   301/  351 batches | loss 3.2502\n",
      "Epoch: 19 | train.loss.best: 19\n",
      "Epoch: 19 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4585 | Train PPL:  31.7687\n",
      "\t Val. Loss: 4.5301 |  Val. PPL:  92.7662\n",
      "####################################################################################################\n",
      "| epoch  20 |     1/  351 batches | loss 3.3811\n",
      "| epoch  20 |   101/  351 batches | loss 3.5513\n",
      "| epoch  20 |   201/  351 batches | loss 3.2378\n",
      "| epoch  20 |   301/  351 batches | loss 3.4636\n",
      "Epoch    20: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch: 20 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4602 | Train PPL:  31.8228\n",
      "\t Val. Loss: 4.5312 |  Val. PPL:  92.8677\n",
      "####################################################################################################\n",
      "| epoch  21 |     1/  351 batches | loss 3.3594\n",
      "| epoch  21 |   101/  351 batches | loss 3.6014\n",
      "| epoch  21 |   201/  351 batches | loss 3.3225\n",
      "| epoch  21 |   301/  351 batches | loss 3.6279\n",
      "Epoch: 21 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4743 | Train PPL:  32.2748\n",
      "\t Val. Loss: 4.5268 |  Val. PPL:  92.4590\n",
      "####################################################################################################\n",
      "| epoch  22 |     1/  351 batches | loss 4.0631\n",
      "| epoch  22 |   101/  351 batches | loss 3.8602\n",
      "| epoch  22 |   201/  351 batches | loss 3.8058\n",
      "| epoch  22 |   301/  351 batches | loss 3.2523\n",
      "Epoch    22: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch: 22 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4697 | Train PPL:  32.1282\n",
      "\t Val. Loss: 4.5264 |  Val. PPL:  92.4292\n",
      "####################################################################################################\n",
      "| epoch  23 |     1/  351 batches | loss 3.9475\n",
      "| epoch  23 |   101/  351 batches | loss 3.3757\n",
      "| epoch  23 |   201/  351 batches | loss 3.0752\n",
      "| epoch  23 |   301/  351 batches | loss 3.7202\n",
      "Epoch: 23 | train.loss.best: 23\n",
      "Epoch: 23 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4433 | Train PPL:  31.2915\n",
      "\t Val. Loss: 4.5272 |  Val. PPL:  92.5031\n",
      "####################################################################################################\n",
      "| epoch  24 |     1/  351 batches | loss 3.8414\n",
      "| epoch  24 |   101/  351 batches | loss 3.4349\n",
      "| epoch  24 |   201/  351 batches | loss 3.2579\n",
      "| epoch  24 |   301/  351 batches | loss 3.0524\n",
      "Epoch    24: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch: 24 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4749 | Train PPL:  32.2955\n",
      "\t Val. Loss: 4.5253 |  Val. PPL:  92.3280\n",
      "####################################################################################################\n",
      "| epoch  25 |     1/  351 batches | loss 3.3223\n",
      "| epoch  25 |   101/  351 batches | loss 3.5928\n",
      "| epoch  25 |   201/  351 batches | loss 3.1187\n",
      "| epoch  25 |   301/  351 batches | loss 3.1665\n",
      "Epoch: 25 | Time: 0m 25s\n",
      "\tTrain Loss: 3.4648 | Train PPL:  31.9703\n",
      "\t Val. Loss: 4.5249 |  Val. PPL:  92.2908\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 25\n",
    "CLIP = 5\n",
    "\n",
    "best_train_loss = float('inf')\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, epoch)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), './models/s2s/train.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | train.loss.best: {epoch+1}')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/s2s/valid.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | valid.loss.best: {epoch+1}')\n",
    "        \n",
    "    lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):8.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {math.exp(valid_loss):8.4f}')\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.5272 | Test PPL:  92.5031 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/s2s/train.loss.best.pt'))\n",
    "\n",
    "test_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):8.4f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, q_sent=\"\", a_sent=None, char2index=None, index2char=None):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        src = [char2index.get(SOS_TOKEN)]\n",
    "        src += [char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        trg = [char2index.get(SOS_TOKEN)]\n",
    "        trg += [char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        trg += [char2index.get(EOS_TOKEN)]\n",
    "\n",
    "        src = torch.LongTensor([src]).to(device)\n",
    "        trg = torch.LongTensor([trg]).to(device)\n",
    "\n",
    "        hyp_ys = model(src, trg, 1)\n",
    "        pred = torch.argmax(hyp_ys[0], dim=-1).detach().cpu().numpy()\n",
    "        \n",
    "        pred_sent = [index2char[token_id] for token_id in pred[1:]]\n",
    "        pred_sent = ''.join(pred_sent)\n",
    "        \n",
    "        print(pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random idx :  2732\n",
      "Q:  사랑이 밥 먹여주나\n",
      "A:  사랑이 밥은 먹여주지 않지만 행복을 줘요.\n",
      "그랑     사어 세 않아  좋복해 해보.<eos>\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(train_dataset), size=1)[0]\n",
    "\n",
    "print(\"random idx : \", idx)\n",
    "\n",
    "q_sent = train_dataset.data[idx][0]\n",
    "a_sent = train_dataset.data[idx][1]\n",
    "\n",
    "print(\"Q: \", q_sent)\n",
    "print(\"A: \", a_sent)\n",
    "\n",
    "inference(model, q_sent, a_sent, char2index=train_dataset.char2index, index2char=train_dataset.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
