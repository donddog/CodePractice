{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network Language Modeling (RNNLM)\n",
    "\n",
    "#### 목표: 한국어 철자(또는 음절) 인식 단위 언어모델링 \n",
    " - P('안녕하세요') = \n",
    "     P('안') * \n",
    "     P('녕'|'안') * \n",
    "     P('하'|'안', '녕') * \n",
    "     P('세'|'안', '녕', '하') * \n",
    "     P('요'|'안', '녕', '하', '세')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 데이터\n",
    " - 출처: https://github.com/eagle705/pytorch-transformer-chatbot/data_in\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " - examples (경로: './data/train_chatbot.txt')\n",
    "\n",
    "헤어지고 차단하는 이유는 뭘까?\t연락하지 않는 게 서로에게 좋으니가요.\n",
    "그 애도 내 생각을 할까?\t연락해 보는게 좋을 거예요.\n",
    "잘 맞는 사람 만나고 싶다\t많은 사람들은 살면서 맞춰가기도 해요.\n",
    "정말 제가 이해안되고 견딜수 없을만큼 아파\t사랑했던 만큼 아픈거라 생각해요.\n",
    "나랑 있는게 힘들었나봐\t상대방을 이해해 주세요.\n",
    "공무원 시험 죽을 거 같아\t철밥통 되기가 어디 쉽겠어요.\n",
    "몰랐구나\t저도 몰랐어요.\n",
    "성공할 수 있을까\t지금보다 더 잘 살 거예요.\n",
    "이 남자맘이 뭔지 ㅠ\t사람 맘은 알 길이 없어요.\n",
    "사랑하면 좋을까?\t사랑보다 좋은 건 없어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-Of-Vocabulary (OOV) // UNKNOWN (OOV, UNK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인식 단위: 철자(음절, Syllable) // ('가', '갸', '겨', ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/train.txt\", 'w', encoding='utf-8') as f_w:\n",
    "#     with open('./data/valid_train.txt', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             pair = line.strip().split('\\t')\n",
    "#             # print(pair[0])\n",
    "#             # print(pair[1])\n",
    "#             f_w.write(\"{}\\n\".format(pair[0]))\n",
    "#             f_w.write(\"{}\\n\".format(pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 인식 단위 VOCAB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_vocab(train_text_file, valid_text_file):\n",
    "    vocab = set()\n",
    "    \n",
    "    with open(train_text_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sent = line.strip() # \"안녕하세요\"\n",
    "            \n",
    "            for char in sent:\n",
    "                vocab.add(char)\n",
    "            \n",
    "    with open(valid_text_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sent = line.strip()\n",
    "            \n",
    "            for char in sent:\n",
    "                vocab.add(char)\n",
    "    \n",
    "    # print(vocab)\n",
    "    \n",
    "    vocab_list = sorted(vocab)\n",
    "    # print(vocab_list)\n",
    "    \n",
    "    vocab = {PAD_token: \"<pad>\", SOS_token: \"<sos>\", EOS_token: \"<eos>\"}\n",
    "    \n",
    "    for idx, v in enumerate(vocab_list):\n",
    "        vocab[int(idx+3)] = v\n",
    "        # print(int(idx+3), v)\n",
    "    \n",
    "    with open('./data/vocab.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(vocab, ensure_ascii=False))\n",
    "    \n",
    "create_vocab('./data/train.txt', './data/valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):    \n",
    "    def __init__(self, text_path, vocab_path):\n",
    "        self.text_path = text_path\n",
    "        self.vocab_path = vocab_path\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        self._read_vocab()\n",
    "        self._read_text()\n",
    "        \n",
    "    def _read_vocab(self):\n",
    "        with open(self.vocab_path, encoding='utf-8') as f:\n",
    "            self.vocab = json.load(f)\n",
    "    \n",
    "    def _read_text(self):\n",
    "        with open(self.text_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sent = line.strip()\n",
    "                self.data.append(sent)\n",
    "                \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "                \n",
    "    @property\n",
    "    def class_to_idx(self):\n",
    "        return {_class: _cid for _cid, _class in self.vocab.items()}\n",
    "    \n",
    "    @property\n",
    "    def idx_to_class(self):\n",
    "        return {_cid: _class for _cid, _class in self.vocab.items()}\n",
    "                \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    @lru_cache(maxsize=100000)\n",
    "    def __getitem__(self, index):\n",
    "        sent = []\n",
    "        sent += [int(self.class_to_idx['<sos>'])] \n",
    "        sent += [int(self.class_to_idx[char]) for char in self.data[index]]\n",
    "        sent += [int(self.class_to_idx['<eos>'])]         \n",
    "        \n",
    "        src = sent[:-1]\n",
    "        tgt = sent[1:]\n",
    "        \n",
    "#         print(\"sent: \", sent)\n",
    "#         print(\"src : \", src)\n",
    "#         print(\"tgt : \", tgt)\n",
    "        \n",
    "        return torch.LongTensor(src), torch.LongTensor(tgt)\n",
    "        \n",
    "    @classmethod\n",
    "    def text_collate_fn(cls, batch):\n",
    "        # batch = [ s1(src, tgt), s2(src, tgt), s3(src, tgt)]\n",
    "        batch_X = [x for x, y in batch]        \n",
    "        padded_batch_X = torch.nn.utils.rnn.pad_sequence(batch_X, batch_first=True)\n",
    "#         print(\"batch_X: \\n{}\".format(batch_X))\n",
    "#         print()\n",
    "#         print(\"padded_batch_X: \\n{}\".format(padded_batch_X))\n",
    "#         print()\n",
    "        \n",
    "        batch_Y = [y for x, y in batch]\n",
    "        padded_batch_Y = torch.nn.utils.rnn.pad_sequence(batch_Y, batch_first=True)\n",
    "        \n",
    "        return padded_batch_X, padded_batch_Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(text_path='./data/train.txt', vocab_path='./data/vocab.json')\n",
    "valid_dataset = TextDataset(text_path='./data/valid.txt', vocab_path='./data/vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "헤어지고 차단하는 이유는 뭘까?\n",
      "(tensor([   1, 1194,  772,  929,   93,    3,  974,  283, 1173,  266,    3,  856,\n",
      "         846,  266,    3,  528,  144,   21]), tensor([1194,  772,  929,   93,    3,  974,  283, 1173,  266,    3,  856,  846,\n",
      "         266,    3,  528,  144,   21,    2]))\n",
      "\n",
      "연락하지 않는 게 서로에게 좋으니가요.\n",
      "(tensor([   1,  793,  399, 1173,  929,    3,  755,  266,    3,   77,    3,  655,\n",
      "         443,  785,   77,    3,  908,  850,  273,   48,  824,    9]), tensor([ 793,  399, 1173,  929,    3,  755,  266,    3,   77,    3,  655,  443,\n",
      "         785,   77,    3,  908,  850,  273,   48,  824,    9,    2]))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.data[0])\n",
    "print(train_dataset[0])\n",
    "print()\n",
    "print(train_dataset.data[1])\n",
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           collate_fn=TextDataset.text_collate_fn,\n",
    "                                           drop_last=False)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           collate_fn=TextDataset.text_collate_fn,\n",
    "                                           drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X      : [[   1  647  443  830    3  309  856 1117    3 1049  696  464    3  983\n",
      "   751  577  666  824    9]\n",
      " [   1  128  418    3  637  402  345    3  851  130    3  480  751  824\n",
      "     9    0    0    0    0]]\n",
      "\n",
      "<sos> 새 로 운   데 이 트   코 스 를   찾 아 보 세 요 . \n",
      "<sos> 그 런   사 람 들   은 근   많 아 요 . <pad> <pad> <pad> <pad> \n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(\"X      : {}\".format(X.numpy()))\n",
    "    print()\n",
    "    \n",
    "    for sent in X.numpy():\n",
    "        for char_idx in sent:\n",
    "            print(\"{} \".format(train_dataset.idx_to_class[str(char_idx)]), end=\"\")\n",
    "        print()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, word_vec_dim=128, hidden_size=128, n_layers=2, dropout_p=0.1):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                  embedding_dim=word_vec_dim,\n",
    "                                  padding_idx=PAD_token)\n",
    "\n",
    "        self.rnn = nn.LSTM(word_vec_dim,\n",
    "                           hidden_size,\n",
    "                           num_layers=n_layers,\n",
    "                           dropout=dropout_p,\n",
    "                           bidirectional=False,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        rnn_output_dim = 2*hidden_size if self.rnn.bidirectional else hidden_size       \n",
    "        self.output = nn.Linear(rnn_output_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)        \n",
    "        y, h = self.rnn(x)       \n",
    "        y = self.output(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNLM(\n",
      "  (embed): Embedding(1245, 128, padding_idx=0)\n",
      "  (rnn): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (output): Linear(in_features=256, out_features=1245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNNLM(vocab_size=train_dataset.vocab_size,\n",
    "              word_vec_dim=128,\n",
    "              hidden_size=256,\n",
    "              n_layers=2,\n",
    "              dropout_p=0.1).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True, False, False, False])\n",
      "\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.Tensor([1, 2, 3, 4, 0, 0, 0])\n",
    "mask = (Y != 0)\n",
    "\n",
    "print(mask)\n",
    "print()\n",
    "\n",
    "Y2 = Y[mask]\n",
    "print(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started.\n",
      "Epoch: 1  |  time in 1 minutes, 42 seconds:          0 (sec.)\n",
      "Epoch: 1  |  (train)\tAvg.Loss: 3.94503719\t\tPPL: 51.67825983\n",
      "model saved as file name: ./model/model.ep_001.pt\n",
      "\n",
      "Epoch: 2  |  time in 2 minutes, 44 seconds\n",
      "Epoch: 2  |  (train)\tAvg.Loss: 3.86377718\t\tPPL: 47.64497538\n",
      "model saved as file name: ./model/model.ep_002.pt\n",
      "\n",
      "Epoch: 3  |  time in 3 minutes, 47 seconds\n",
      "Epoch: 3  |  (train)\tAvg.Loss: 3.85778780\t\tPPL: 47.36046475\n",
      "model saved as file name: ./model/model.ep_003.pt\n",
      "\n",
      "Epoch: 4  |  time in 4 minutes, 49 seconds\n",
      "Epoch: 4  |  (train)\tAvg.Loss: 3.85498315\t\tPPL: 47.22782125\n",
      "model saved as file name: ./model/model.ep_004.pt\n",
      "\n",
      "Epoch: 5  |  time in 5 minutes, 51 seconds\n",
      "Epoch: 5  |  (train)\tAvg.Loss: 3.84891557\t\tPPL: 46.94212993\n",
      "model saved as file name: ./model/model.ep_005.pt\n",
      "\n",
      "Epoch: 6  |  time in 6 minutes, 52 seconds\n",
      "Epoch: 6  |  (train)\tAvg.Loss: 3.84428895\t\tPPL: 46.72544821\n",
      "model saved as file name: ./model/model.ep_006.pt\n",
      "\n",
      "Epoch: 7  |  time in 7 minutes, 54 seconds\n",
      "Epoch: 7  |  (train)\tAvg.Loss: 3.84413764\t\tPPL: 46.71837908\n",
      "model saved as file name: ./model/model.ep_007.pt\n",
      "\n",
      "Epoch: 8  |  time in 9 minutes, 1 seconds\n",
      "Epoch: 8  |  (train)\tAvg.Loss: 3.84279812\t\tPPL: 46.65584066\n",
      "model saved as file name: ./model/model.ep_008.pt\n",
      "\n",
      "Epoch: 9  |  time in 10 minutes, 5 seconds\n",
      "Epoch: 9  |  (train)\tAvg.Loss: 3.84136111\t\tPPL: 46.58884384\n",
      "model saved as file name: ./model/model.ep_009.pt\n",
      "\n",
      "Epoch: 10  |  time in 11 minutes, 10 seconds\n",
      "Epoch: 10  |  (train)\tAvg.Loss: 3.84234113\t\tPPL: 46.63452433\n",
      "model saved as file name: ./model/model.ep_010.pt\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 10\n",
    "\n",
    "total_batch = len(train_loader)\n",
    "# print(total_batch)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('Learning started.')\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epochs):    \n",
    "    total_loss = 0.\n",
    "    total_num = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        \n",
    "        # print(X.size(), Y.size(), type(Y))\n",
    "        \n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        # print(hypothesis.size())\n",
    "        \n",
    "        y_pred = hypothesis.contiguous().view(-1, hypothesis.size(-1))\n",
    "        target = Y.contiguous().view(-1)\n",
    "        real_value_index = [target != PAD_token]\n",
    "\n",
    "        loss = criterion(y_pred[real_value_index], target[real_value_index])\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        #print(\"grad_norm: {:.8f}\".format(grad_norm))\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_num += target[real_value_index].size(0)\n",
    "        \n",
    "        # print(total_loss, total_num, avg_loss)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            secs = int(time.time() - start_time)\n",
    "            et = int((len(train_loader) - i) * (secs/(i+1)))\n",
    "            print(\"(train) : {:4d}/{:4d} # estimated end time: {:10d} (sec.)\".format(i+1, len(train_loader), et), end=\"\\r\", flush=True)\n",
    "        \n",
    "    avg_loss = total_loss / total_num\n",
    "    perplexity  = np.exp(avg_loss)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | \", \"time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print('Epoch: %d' %(epoch + 1), \" | \", f'(train)\\tAvg.Loss: {avg_loss:.8f}\\t\\tPPL: {perplexity:.8f}')\n",
    "    \n",
    "    model_fname = \"./model/model.ep_{:03d}.pt\".format(epoch+1)\n",
    "    torch.save(model.state_dict(), model_fname)\n",
    "    \n",
    "    print(\"model saved as file name: {}\".format(model_fname))\n",
    "    print()\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fname = \"./model/model.ep_{:03d}.pt\".format(10)\n",
    "state = torch.load(model_fname, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | valid loss  3.83 | test ppl    46.17\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0.\n",
    "    total_num = 0    \n",
    "    \n",
    "    for i, (X, Y) in enumerate(valid_loader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        hypothesis = model(X)\n",
    "        \n",
    "        y_pred = hypothesis.contiguous().view(-1, hypothesis.size(-1))\n",
    "        target = Y.contiguous().view(-1)\n",
    "        real_value_index = [target != PAD_token]\n",
    "        \n",
    "        loss = criterion(y_pred[real_value_index], target[real_value_index])\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_num += target[real_value_index].size(0)\n",
    "        \n",
    "    avg_loss = total_loss / total_num\n",
    "    perplexity  = np.exp(avg_loss)\n",
    "    \n",
    "    print('=' * 89)\n",
    "    print('| End of training | valid loss {:5.2f} | test ppl {:8.2f}'.format(avg_loss, perplexity))\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2i = train_dataset.class_to_idx\n",
    "\n",
    "def get_prob_sent(sent=\"\"):\n",
    "    sent_id = []\n",
    "    sent_id += [int(c2i['<sos>'])] \n",
    "    sent_id += [int(c2i[char]) for char in sent]\n",
    "    # sent_id += [int(c2i['<eos>'])] \n",
    "\n",
    "    src = torch.LongTensor([sent_id[:-1]])\n",
    "    tgt = torch.LongTensor([sent_id[1:]])\n",
    "    \n",
    "    X = src.to(device)\n",
    "    Y = tgt.to(device)\n",
    "    \n",
    "    # print(X.size())\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hypothesis = model(X)\n",
    "    \n",
    "    # print(hypothesis.size(), hypothesis)\n",
    "    \n",
    "    last_char = sent[-1]\n",
    "    last_char_id = int(c2i[last_char])\n",
    "    # print(last_char_id)\n",
    "    \n",
    "    hypothesis = torch.nn.functional.softmax(hypothesis, dim=-1)\n",
    "    \n",
    "    # hypothesis.shape = (BS, SeqLen, Vocab)\n",
    "    \n",
    "    prob = hypothesis[:, -1, last_char_id].item()\n",
    "    \n",
    "    print(\"{} : {:.8f}\".format(sent, prob))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 : 0.73909706\n",
      "\n",
      "여기는 어디인가요 : 0.16988163\n",
      "\n",
      "헤어지고 차단하는  : 0.78526711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_prob_sent(sent=\"안녕하세요\")\n",
    "get_prob_sent(sent=\"여기는 어디인가요\")\n",
    "get_prob_sent(sent=\"헤어지고 차단하는 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
