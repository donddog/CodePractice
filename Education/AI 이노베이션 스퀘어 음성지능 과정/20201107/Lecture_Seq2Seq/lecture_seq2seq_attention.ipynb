{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 내용: SEQ2SEQ 모델링\n",
    "\n",
    "### 1. Vocab: 한국어 음절 단위\n",
    "\n",
    "### 2. 데이터: 한국어 Q&A 문장\n",
    " - ex.)공무원 시험 죽을 거 같아 --> 철밥통 되기가 어디 쉽겠어요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (초기 환경 세팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 데이터 확인\n",
    " - 출처: https://github.com/eagle705/pytorch-transformer-chatbot/tree/master/data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/train_chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 \"./data/valid_chatbot.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 어휘사전 (Vocab) 생성 // (음절 단위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "def create_vocab(train_path, valid_path, vocab_path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    with open(valid_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for sent in line.strip().split('\\t'):\n",
    "                data.append(sent) \n",
    "    \n",
    "    vocab = set()\n",
    "    \n",
    "    for sent in data:\n",
    "        for char in sent:\n",
    "            vocab.add(char)\n",
    "            \n",
    "    vocab_list = list(sorted(vocab))\n",
    "    \n",
    "    vocab_list.insert(0, PAD_TOKEN)\n",
    "    vocab_list.insert(1, UNK_TOKEN)\n",
    "    vocab_list.insert(2, SOS_TOKEN)\n",
    "    vocab_list.insert(3, EOS_TOKEN)\n",
    "    \n",
    "    print(vocab_list)\n",
    "\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(vocab_list, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<sos>', '<eos>', ' ', '!', '%', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'L', 'N', 'O', 'P', 'S', 'X', '_', 'a', 'c', 'g', 'j', 'k', 'n', 'o', 's', '~', '…', 'ㅊ', 'ㅋ', 'ㅎ', 'ㅜ', 'ㅠ', '가', '각', '간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '갚', '개', '객', '갠', '갯', '갱', '걍', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '겉', '게', '겐', '겜', '겟', '겠', '겨', '격', '겪', '견', '결', '겹', '겼', '경', '곁', '계', '곗', '고', '곡', '곤', '곧', '골', '곰', '곱', '곳', '공', '과', '관', '광', '괘', '괜', '괴', '교', '구', '국', '군', '굳', '굴', '굶', '굽', '굿', '궁', '궈', '권', '궜', '귀', '귄', '귈', '귐', '규', '균', '귤', '그', '극', '근', '글', '긁', '금', '급', '긋', '긍', '기', '긴', '길', '김', '깃', '깅', '깊', '까', '깍', '깎', '깐', '깔', '깜', '깝', '깠', '깡', '깨', '깬', '깰', '깼', '꺼', '꺽', '껀', '껄', '껏', '께', '껴', '꼈', '꼬', '꼭', '꼰', '꼴', '꼼', '꼿', '꽁', '꽂', '꽃', '꽈', '꽉', '꽝', '꽤', '꾸', '꾹', '꾼', '꿀', '꿈', '꿎', '꿔', '꿧', '꿨', '꿩', '꿰', '뀌', '뀐', '뀔', '끄', '끈', '끊', '끌', '끓', '끔', '끗', '끝', '끼', '낀', '낄', '낌', '나', '낙', '낚', '난', '날', '남', '납', '낫', '났', '낭', '낮', '낳', '내', '낸', '낼', '냄', '냅', '냈', '냉', '냐', '냥', '너', '넋', '넌', '널', '넓', '넘', '넛', '넣', '네', '넥', '넷', '넹', '녀', '녁', '년', '념', '녔', '녕', '노', '녹', '논', '놀', '놈', '놉', '농', '높', '놓', '놔', '놨', '뇌', '뇨', '뇽', '누', '눅', '눈', '눌', '눕', '눠', '눴', '뉴', '느', '는', '늘', '늙', '늠', '능', '늦', '늪', '니', '닉', '닌', '닐', '님', '닙', '닝', '다', '닥', '닦', '단', '닫', '달', '닭', '닮', '닳', '담', '답', '닷', '당', '닿', '대', '댄', '댈', '댓', '댔', '댜', '더', '덕', '던', '덜', '덤', '덥', '덧', '덩', '덮', '데', '덴', '델', '뎌', '뎠', '도', '독', '돈', '돋', '돌', '돕', '동', '돼', '됐', '되', '된', '될', '됨', '됩', '됬', '두', '둑', '둔', '둘', '둠', '둥', '둬', '뒀', '뒤', '뒷', '뒹', '듀', '드', '득', '든', '듣', '들', '듦', '듬', '듭', '듯', '등', '디', '딘', '딛', '딜', '딧', '딨', '딩', '딪', '따', '딱', '딴', '딸', '땀', '땅', '때', '땐', '땜', '땠', '땡', '떄', '떠', '떡', '떤', '떨', '떴', '떻', '떼', '뗄', '또', '똑', '똥', '뚝', '뚫', '뚱', '뛰', '뛴', '뜁', '뜨', '뜩', '뜬', '뜰', '뜸', '뜻', '띄', '띠', '띰', '띵', '라', '락', '란', '랄', '람', '랍', '랐', '랑', '랖', '래', '랙', '랜', '랠', '램', '랩', '랫', '랬', '랭', '량', '러', '런', '럴', '럼', '럽', '럿', '렀', '렁', '렇', '레', '렉', '렌', '렐', '렘', '렛', '렜', '려', '력', '련', '렬', '렴', '렵', '렷', '렸', '령', '례', '로', '록', '론', '롤', '롭', '롯', '롱', '뢰', '료', '루', '룩', '룰', '룸', '룽', '뤄', '류', '륜', '률', '르', '륵', '른', '를', '름', '릅', '릇', '릎', '리', '릭', '린', '릴', '림', '립', '릿', '링', '마', '막', '만', '많', '말', '맘', '맙', '맛', '망', '맞', '맡', '매', '맥', '맨', '맴', '맷', '맹', '맺', '머', '먹', '먼', '멀', '멈', '멋', '멍', '메', '멘', '멤', '며', '면', '명', '몇', '모', '목', '몫', '몬', '몰', '몸', '몹', '못', '몽', '묘', '무', '묵', '문', '묻', '물', '뭇', '뭉', '뭐', '뭔', '뭘', '뭣', '뮤', '미', '민', '믿', '밀', '밉', '밌', '밍', '밑', '바', '박', '밖', '반', '받', '발', '밝', '밟', '밤', '밥', '방', '밭', '배', '백', '밸', '뱃', '뱄', '뱉', '버', '벅', '번', '벋', '벌', '범', '법', '벗', '벚', '베', '벤', '벨', '벴', '벼', '벽', '변', '별', '볍', '병', '볕', '보', '복', '볶', '본', '볼', '봄', '봅', '봇', '봉', '봐', '봤', '뵈', '부', '북', '분', '불', '붓', '붕', '붙', '뷔', '브', '블', '비', '빈', '빌', '빔', '빗', '빙', '빚', '빛', '빠', '빡', '빨', '빴', '빵', '빼', '빽', '뺄', '뺏', '뺴', '뻐', '뻑', '뻔', '뻘', '뻣', '뻤', '뻥', '뽀', '뽑', '뽕', '뿅', '뿌', '뿍', '뿐', '쁘', '쁜', '쁠', '쁨', '삐', '삔', '사', '삭', '산', '살', '삶', '삼', '삽', '삿', '샀', '상', '새', '색', '샌', '샐', '샘', '샜', '생', '샤', '서', '석', '섞', '선', '섣', '설', '섬', '섭', '섯', '섰', '성', '세', '섹', '센', '셀', '셔', '션', '셥', '셨', '소', '속', '손', '솔', '솜', '송', '쇠', '쇼', '숍', '숏', '수', '숙', '순', '술', '숨', '숫', '숭', '쉬', '쉴', '쉼', '쉽', '슈', '스', '슨', '슬', '슴', '습', '슷', '승', '시', '식', '신', '실', '싫', '심', '십', '싱', '싶', '싸', '싹', '싼', '쌀', '쌈', '쌍', '쌓', '쌤', '쌩', '써', '썩', '썰', '썸', '썹', '썼', '쎄', '쎈', '쎌', '쏘', '쏜', '쏟', '쏠', '쐬', '쑤', '쑥', '쓰', '쓴', '쓸', '씀', '씁', '씌', '씨', '씩', '씬', '씰', '씸', '씹', '씻', '씽', '아', '악', '안', '앉', '않', '알', '압', '앗', '았', '앙', '앞', '애', '액', '앨', '앱', '야', '약', '얄', '얇', '양', '얘', '어', '억', '언', '얻', '얼', '얽', '엄', '업', '없', '엇', '었', '엉', '엊', '에', '엔', '엘', '엠', '엣', '여', '역', '엮', '연', '열', '염', '엽', '엿', '였', '영', '옆', '옇', '예', '옛', '오', '옥', '온', '올', '옮', '옳', '옴', '옵', '옷', '와', '완', '왓', '왔', '왕', '왜', '왠', '왤', '외', '왼', '욌', '요', '욕', '욜', '용', '우', '욱', '운', '울', '움', '웁', '웃', '워', '원', '월', '웠', '웨', '웬', '웹', '웽', '위', '윗', '윙', '유', '육', '윤', '율', '으', '은', '을', '음', '응', '의', '이', '익', '인', '일', '읽', '잃', '임', '입', '잇', '있', '잊', '잌', '자', '작', '잔', '잖', '잘', '잠', '잡', '잤', '장', '잦', '재', '잼', '잿', '쟁', '저', '적', '전', '절', '젊', '점', '접', '젔', '정', '젖', '제', '젝', '젠', '젤', '젯', '져', '젹', '졋', '졌', '조', '족', '존', '졸', '좀', '좁', '종', '좋', '좌', '죄', '죠', '주', '죽', '준', '줄', '줌', '줍', '중', '줘', '줬', '쥐', '쥬', '즈', '즉', '즐', '즘', '즙', '증', '지', '직', '진', '질', '짐', '집', '짓', '징', '짚', '짜', '짝', '짠', '짤', '짧', '짬', '짰', '짱', '째', '짼', '쨌', '쩌', '쩍', '쩐', '쩔', '쩝', '쩡', '쪄', '쪘', '쪼', '쪽', '쫄', '쫌', '쫙', '쭈', '쭉', '쭤', '쯤', '찌', '찍', '찐', '찔', '찜', '찝', '찡', '찢', '차', '착', '찬', '찮', '찰', '참', '찹', '찼', '창', '찾', '채', '책', '챔', '챗', '챘', '챙', '처', '척', '천', '철', '첨', '첩', '첫', '청', '체', '쳇', '쳐', '쳤', '초', '촉', '촌', '총', '촬', '최', '추', '축', '춘', '출', '춤', '춥', '충', '춰', '췄', '취', '츄', '츠', '측', '츤', '층', '치', '칙', '친', '칠', '침', '칩', '칫', '칭', '카', '칼', '캄', '캐', '캔', '캬', '커', '컥', '컨', '컴', '컷', '컸', '컹', '케', '켓', '켜', '켠', '켰', '코', '콕', '콘', '콜', '콤', '콧', '콩', '쾌', '쿠', '쿨', '쿵', '쿼', '퀴', '큐', '크', '큰', '클', '큼', '킁', '키', '킥', '킨', '킬', '킴', '킹', '타', '탁', '탄', '탈', '탐', '탑', '탓', '탔', '탕', '태', '택', '탱', '터', '턱', '턴', '털', '텀', '텁', '텄', '텅', '테', '텍', '텐', '텔', '템', '텨', '텻', '텼', '토', '톡', '톤', '톱', '통', '퇴', '투', '툭', '툰', '툴', '툼', '퉁', '퉜', '튀', '튜', '트', '특', '틀', '틈', '틋', '티', '틱', '팀', '팁', '팅', '파', '팍', '판', '팔', '팠', '패', '팩', '팬', '팸', '퍼', '펑', '페', '펜', '펨', '펭', '펴', '편', '펼', '평', '폐', '포', '폭', '폰', '폼', '퐈', '표', '푸', '푹', '푼', '풀', '품', '풋', '풍', '퓨', '프', '픈', '플', '픔', '픕', '피', '픽', '핀', '필', '핍', '핏', '핑', '하', '학', '한', '할', '함', '합', '핫', '항', '해', '핸', '햇', '했', '행', '햐', '향', '허', '헉', '헌', '헐', '험', '헛', '헤', '헥', '헬', '헷', '헹', '혀', '현', '혈', '혐', '협', '혔', '형', '혜', '호', '혹', '혼', '홀', '홈', '화', '확', '환', '활', '홧', '황', '회', '획', '효', '후', '훅', '훈', '훌', '훔', '훨', '휘', '휙', '휨', '휴', '흐', '흑', '흔', '흘', '흠', '흡', '흥', '희', '흰', '히', '힌', '힐', '힘', '힙']\n"
     ]
    }
   ],
   "source": [
    "create_vocab(train_path=\"./data/train_chatbot.txt\",\n",
    "             valid_path=\"./data/valid_chatbot.txt\",\n",
    "             vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnADataset(Dataset):\n",
    "    def __init__(self, data_path, vocab_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.char2index, self.index2char = self._read_vocab(vocab_path)\n",
    "        self.data = self._preprocess(data_path)\n",
    "    \n",
    "    def _read_vocab(self, vocab_path):\n",
    "        with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "            labels = json.load(f)\n",
    "            char2index = dict()\n",
    "            index2char = dict()\n",
    "\n",
    "            for index, char in enumerate(labels):\n",
    "                char2index[char] = index\n",
    "                index2char[index] = char\n",
    "            \n",
    "        return char2index, index2char\n",
    "    \n",
    "    def _preprocess(self, data_path):\n",
    "        data = []\n",
    "        \n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                sents = line.strip().split('\\t')\n",
    "                \n",
    "                assert len(sents) == 2, \"data error!!\"\n",
    "                \n",
    "                question_sent, answer_sent = sents[0], sents[1]\n",
    "                \n",
    "                data.append((question_sent, answer_sent))\n",
    "                \n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.char2index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        qna = self.data[index]\n",
    "        q_sent, a_sent = qna[0], qna[1]\n",
    "        \n",
    "        src = [self.char2index.get(SOS_TOKEN)]\n",
    "        src += [self.char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        tgt = [self.char2index.get(SOS_TOKEN)]\n",
    "        tgt += [self.char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        tgt += [self.char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        return torch.LongTensor(src), torch.LongTensor(tgt)\n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    xs = [x for x, y in batch]\n",
    "    xs_pad = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    xs_lengths = [x.size(0) for x, y in batch]\n",
    "    xs_lengths = torch.LongTensor(xs_lengths)\n",
    "\n",
    "    ys = [y for x, y in batch]\n",
    "    ys_pad = torch.nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    ys_lengths = [y.size(0) for x, y in batch]\n",
    "    ys_lengths = torch.LongTensor(ys_lengths)\n",
    "\n",
    "    return xs_pad, xs_lengths, ys_pad, ys_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QnADataset(data_path=\"./data/train_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")\n",
    "\n",
    "valid_dataset = QnADataset(data_path=\"./data/valid_chatbot.txt\",\n",
    "                           vocab_path=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # 4\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=text_collate_fn,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   69,  936,  ...,    0,    0,    0],\n",
      "        [   2,  779,  478,  ...,    0,    0,    0],\n",
      "        [   2,  822,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  205,  465,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,    0,    0,    0],\n",
      "        [   2,  805,  268,  ...,    0,    0,    0]]) tensor([[   2,  932,  707,  ...,    0,    0,    0],\n",
      "        [   2,  704,  773,  ...,    0,    0,    0],\n",
      "        [   2,  648,  444,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  647,  908,  ...,    0,    0,    0],\n",
      "        [   2, 1195,  773,  ...,  825,   10,    3],\n",
      "        [   2,  129,  480,  ...,    0,    0,    0]])\n",
      "tensor([14, 14, 15, 15, 15, 12, 11, 14,  9, 11, 10,  8, 10, 16, 13, 40, 17, 13,\n",
      "        24, 11, 11,  4, 13,  8, 16, 35, 27, 20, 17, 19, 11, 12]) tensor([20, 18, 17, 10, 21, 17, 14, 24, 10, 13, 15, 14, 15, 12, 24, 20, 13, 22,\n",
      "        25, 12, 15, 13, 14, 24, 11, 22, 21, 30, 15, 10, 32, 19])\n"
     ]
    }
   ],
   "source": [
    "for x, x_len, y, y_len in train_loader:\n",
    "    print(x, y)\n",
    "    print(x_len, y_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-0. Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden = [batch size, dec hid dim]\n",
    "- encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "- repeat decoder hidden state src_len times\n",
    "- hidden = [batch size, src len, dec hid dim]\n",
    "- encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "- energy = [batch size, src len, dec hid dim]\n",
    "- attention= [batch size, src len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(0, 1, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- src = [batch size, src len]\n",
    "- embedded = [batch size, src len, emb dim]\n",
    "\n",
    "- outputs = [batch size, src len, hid dim * num directions]\n",
    "- hidden = [n layers * num directions, batch size, hid dim]\n",
    "- hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "- outputs are always from the last layer\n",
    "- hidden [-2, :, : ] is the last of the forwards RNN \n",
    "- hidden [-1, :, : ] is the last of the backwards RNN\n",
    "- initial decoder hidden is final hidden state of the forwards and backwards \n",
    "- encoder RNNs fed through a linear layer\n",
    "- outputs = [batch size, src len, enc hid dim * 2]\n",
    "- hidden = [batch size, dec hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input = [batch size]\n",
    "- hidden = [batch size, dec hid dim]\n",
    "- encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "- input = [batch size, 1]\n",
    "- embedded = [batch size, 1, emb dim]\n",
    "- a = [batch size, src len]\n",
    "- a = [batch size, 1, src len]\n",
    "- encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "- weighted = [batch size, 1, enc hid dim * 2]\n",
    "- weighted = [batch size, 1, enc hid dim * 2]\n",
    "- rnn_input = [batch size, 1, (enc hid dim * 2) + emb dim]\n",
    "- output = [batch size, seq len, dec hid dim * n directions]\n",
    "- hidden = [batch size, n layers * n directions, dec hid dim]\n",
    "- seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "- output = [batch size, 1, dec hid dim]\n",
    "- hidden = [1, batch size, dec hid dim]\n",
    "- this also means that output == hidden\n",
    "- prediction = [batch size, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "                \n",
    "        encoder_outputs = encoder_outputs.permute(0, 1, 2)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(0, 1, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        assert (output.transpose(1,0) == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- src = [batch size, src len]\n",
    "- trg = [batch size, trg len]\n",
    "- teacher_forcing_ratio is probability to use teacher forcing\n",
    "- e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "- tensor to store decoder outputs\n",
    "- last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "- first input to the decoder is the <sos> tokens\n",
    "- insert input token embedding, previous hidden and previous cell states\n",
    "- receive output tensor (predictions) and new hidden and cell states\n",
    "- place predictions in a tensor holding predictions for each token\n",
    "- decide if we are going to use teacher forcing or not\n",
    "- get the highest predicted token from our predictions\n",
    "- if teacher forcing, use actual next token as next input\n",
    "- if not, use predicted token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n",
    "        outputs = outputs.transpose(1, 0).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(1246, 256)\n",
      "    (rnn): GRU(256, 512, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(1246, 256)\n",
      "    (rnn): GRU(1280, 512, batch_first=True)\n",
      "    (fc_out): Linear(in_features=1792, out_features=1246, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = train_dataset.vocab_size\n",
    "OUTPUT_DIM = train_dataset.vocab_size\n",
    "\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trg = [batch size, trg len]\n",
    "- output = [batch size, trg len, output dim]\n",
    "- trg = [(trg len - 1) * batch size]\n",
    "- output = [(trg len - 1) * batch size, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.001 # 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=PAD_TOKEN_ID).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, clip, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        \n",
    "        src, src_len, trg, trg_len = batch\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        log_interval = 100\n",
    "        \n",
    "        if i % log_interval == 0 and i >= 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | loss {:.4f}'.format(epoch+1, i+1, len(data_loader), loss.detach().item()))\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trg = [batch size, trg len]\n",
    "- output = [batch size, trg len, output dim]\n",
    "- trg = [(trg len - 1) * batch size]\n",
    "- output = [(trg len - 1) * batch size, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "\n",
    "            src, src_len, trg, trg_len = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  351 batches | loss 7.1762\n",
      "| epoch   1 |   101/  351 batches | loss 3.8861\n",
      "| epoch   1 |   201/  351 batches | loss 3.9214\n",
      "| epoch   1 |   301/  351 batches | loss 3.7714\n",
      "Epoch: 01 | train.loss.best: 1\n",
      "Epoch: 01 | valid.loss.best: 1\n",
      "Epoch: 01 | Time: 0m 38s\n",
      "\tTrain Loss: 4.0238 | Train PPL:  55.9115\n",
      "\t Val. Loss: 4.5570 |  Val. PPL:  95.2952\n",
      "####################################################################################################\n",
      "| epoch   2 |     1/  351 batches | loss 3.6986\n",
      "| epoch   2 |   101/  351 batches | loss 4.0208\n",
      "| epoch   2 |   201/  351 batches | loss 3.3773\n",
      "| epoch   2 |   301/  351 batches | loss 3.7555\n",
      "Epoch: 02 | train.loss.best: 2\n",
      "Epoch: 02 | Time: 0m 38s\n",
      "\tTrain Loss: 3.6552 | Train PPL:  38.6742\n",
      "\t Val. Loss: 4.6782 |  Val. PPL: 107.5743\n",
      "####################################################################################################\n",
      "| epoch   3 |     1/  351 batches | loss 3.4452\n",
      "| epoch   3 |   101/  351 batches | loss 4.3086\n",
      "| epoch   3 |   201/  351 batches | loss 3.4732\n",
      "| epoch   3 |   301/  351 batches | loss 3.5328\n",
      "Epoch: 03 | train.loss.best: 3\n",
      "Epoch: 03 | Time: 0m 38s\n",
      "\tTrain Loss: 3.4911 | Train PPL:  32.8229\n",
      "\t Val. Loss: 4.7409 |  Val. PPL: 114.5366\n",
      "####################################################################################################\n",
      "| epoch   4 |     1/  351 batches | loss 3.2301\n",
      "| epoch   4 |   101/  351 batches | loss 3.1804\n",
      "| epoch   4 |   201/  351 batches | loss 3.4846\n",
      "| epoch   4 |   301/  351 batches | loss 3.4896\n",
      "Epoch: 04 | train.loss.best: 4\n",
      "Epoch: 04 | Time: 0m 38s\n",
      "\tTrain Loss: 3.3034 | Train PPL:  27.2062\n",
      "\t Val. Loss: 4.7479 |  Val. PPL: 115.3365\n",
      "####################################################################################################\n",
      "| epoch   5 |     1/  351 batches | loss 3.0610\n",
      "| epoch   5 |   101/  351 batches | loss 2.9354\n",
      "| epoch   5 |   201/  351 batches | loss 3.2740\n",
      "| epoch   5 |   301/  351 batches | loss 2.9393\n",
      "Epoch: 05 | train.loss.best: 5\n",
      "Epoch: 05 | Time: 0m 38s\n",
      "\tTrain Loss: 3.1617 | Train PPL:  23.6104\n",
      "\t Val. Loss: 4.8906 |  Val. PPL: 133.0316\n",
      "####################################################################################################\n",
      "| epoch   6 |     1/  351 batches | loss 2.5058\n",
      "| epoch   6 |   101/  351 batches | loss 2.5222\n",
      "| epoch   6 |   201/  351 batches | loss 3.1997\n",
      "| epoch   6 |   301/  351 batches | loss 3.2374\n",
      "Epoch: 06 | train.loss.best: 6\n",
      "Epoch: 06 | Time: 0m 38s\n",
      "\tTrain Loss: 3.0190 | Train PPL:  20.4699\n",
      "\t Val. Loss: 5.0202 |  Val. PPL: 151.4491\n",
      "####################################################################################################\n",
      "| epoch   7 |     1/  351 batches | loss 2.3673\n",
      "| epoch   7 |   101/  351 batches | loss 2.8933\n",
      "| epoch   7 |   201/  351 batches | loss 3.1928\n",
      "| epoch   7 |   301/  351 batches | loss 2.9690\n",
      "Epoch: 07 | train.loss.best: 7\n",
      "Epoch: 07 | Time: 0m 39s\n",
      "\tTrain Loss: 2.8431 | Train PPL:  17.1681\n",
      "\t Val. Loss: 5.0470 |  Val. PPL: 155.5555\n",
      "####################################################################################################\n",
      "| epoch   8 |     1/  351 batches | loss 2.5028\n",
      "| epoch   8 |   101/  351 batches | loss 2.5410\n",
      "| epoch   8 |   201/  351 batches | loss 2.5578\n",
      "| epoch   8 |   301/  351 batches | loss 3.0945\n",
      "Epoch: 08 | train.loss.best: 8\n",
      "Epoch: 08 | Time: 0m 39s\n",
      "\tTrain Loss: 2.6974 | Train PPL:  14.8417\n",
      "\t Val. Loss: 5.2660 |  Val. PPL: 193.6446\n",
      "####################################################################################################\n",
      "| epoch   9 |     1/  351 batches | loss 2.4404\n",
      "| epoch   9 |   101/  351 batches | loss 2.2971\n",
      "| epoch   9 |   201/  351 batches | loss 2.6571\n",
      "| epoch   9 |   301/  351 batches | loss 2.3203\n",
      "Epoch: 09 | train.loss.best: 9\n",
      "Epoch: 09 | Time: 0m 39s\n",
      "\tTrain Loss: 2.5756 | Train PPL:  13.1388\n",
      "\t Val. Loss: 5.1240 |  Val. PPL: 168.0047\n",
      "####################################################################################################\n",
      "| epoch  10 |     1/  351 batches | loss 2.7408\n",
      "| epoch  10 |   101/  351 batches | loss 2.1039\n",
      "| epoch  10 |   201/  351 batches | loss 2.8402\n",
      "| epoch  10 |   301/  351 batches | loss 3.2371\n",
      "Epoch: 10 | train.loss.best: 10\n",
      "Epoch: 10 | Time: 0m 38s\n",
      "\tTrain Loss: 2.4123 | Train PPL:  11.1591\n",
      "\t Val. Loss: 5.1016 |  Val. PPL: 164.2878\n",
      "####################################################################################################\n",
      "| epoch  11 |     1/  351 batches | loss 2.1800\n",
      "| epoch  11 |   101/  351 batches | loss 1.7841\n",
      "| epoch  11 |   201/  351 batches | loss 2.2853\n",
      "| epoch  11 |   301/  351 batches | loss 2.6962\n",
      "Epoch: 11 | train.loss.best: 11\n",
      "Epoch: 11 | Time: 0m 39s\n",
      "\tTrain Loss: 2.2813 | Train PPL:   9.7898\n",
      "\t Val. Loss: 5.3592 |  Val. PPL: 212.5465\n",
      "####################################################################################################\n",
      "| epoch  12 |     1/  351 batches | loss 2.2533\n",
      "| epoch  12 |   101/  351 batches | loss 2.1268\n",
      "| epoch  12 |   201/  351 batches | loss 2.1504\n",
      "| epoch  12 |   301/  351 batches | loss 1.9280\n",
      "Epoch: 12 | train.loss.best: 12\n",
      "Epoch: 12 | Time: 0m 38s\n",
      "\tTrain Loss: 2.1550 | Train PPL:   8.6279\n",
      "\t Val. Loss: 5.4722 |  Val. PPL: 237.9730\n",
      "####################################################################################################\n",
      "| epoch  13 |     1/  351 batches | loss 2.3369\n",
      "| epoch  13 |   101/  351 batches | loss 2.2897\n",
      "| epoch  13 |   201/  351 batches | loss 1.8937\n",
      "| epoch  13 |   301/  351 batches | loss 1.9745\n",
      "Epoch: 13 | train.loss.best: 13\n",
      "Epoch: 13 | Time: 0m 38s\n",
      "\tTrain Loss: 2.0503 | Train PPL:   7.7704\n",
      "\t Val. Loss: 5.5791 |  Val. PPL: 264.8216\n",
      "####################################################################################################\n",
      "| epoch  14 |     1/  351 batches | loss 1.6839\n",
      "| epoch  14 |   101/  351 batches | loss 2.1745\n",
      "| epoch  14 |   201/  351 batches | loss 1.9910\n",
      "| epoch  14 |   301/  351 batches | loss 2.2185\n",
      "Epoch: 14 | train.loss.best: 14\n",
      "Epoch: 14 | Time: 0m 38s\n",
      "\tTrain Loss: 1.9381 | Train PPL:   6.9454\n",
      "\t Val. Loss: 5.5019 |  Val. PPL: 245.1635\n",
      "####################################################################################################\n",
      "| epoch  15 |     1/  351 batches | loss 2.1415\n",
      "| epoch  15 |   101/  351 batches | loss 2.5001\n",
      "| epoch  15 |   201/  351 batches | loss 2.0509\n",
      "| epoch  15 |   301/  351 batches | loss 2.3992\n",
      "Epoch: 15 | train.loss.best: 15\n",
      "Epoch: 15 | Time: 0m 38s\n",
      "\tTrain Loss: 1.8904 | Train PPL:   6.6222\n",
      "\t Val. Loss: 5.6114 |  Val. PPL: 273.5368\n",
      "####################################################################################################\n",
      "| epoch  16 |     1/  351 batches | loss 1.4579\n",
      "| epoch  16 |   101/  351 batches | loss 1.6853\n",
      "| epoch  16 |   201/  351 batches | loss 1.7547\n",
      "| epoch  16 |   301/  351 batches | loss 1.6334\n",
      "Epoch: 16 | train.loss.best: 16\n",
      "Epoch: 16 | Time: 0m 38s\n",
      "\tTrain Loss: 1.7868 | Train PPL:   5.9701\n",
      "\t Val. Loss: 5.6753 |  Val. PPL: 291.5752\n",
      "####################################################################################################\n",
      "| epoch  17 |     1/  351 batches | loss 1.6970\n",
      "| epoch  17 |   101/  351 batches | loss 1.3884\n",
      "| epoch  17 |   201/  351 batches | loss 1.7587\n",
      "| epoch  17 |   301/  351 batches | loss 1.9275\n",
      "Epoch: 17 | train.loss.best: 17\n",
      "Epoch: 17 | Time: 0m 38s\n",
      "\tTrain Loss: 1.7177 | Train PPL:   5.5719\n",
      "\t Val. Loss: 5.6732 |  Val. PPL: 290.9626\n",
      "####################################################################################################\n",
      "| epoch  18 |     1/  351 batches | loss 1.4278\n",
      "| epoch  18 |   101/  351 batches | loss 2.0509\n",
      "| epoch  18 |   201/  351 batches | loss 1.5374\n",
      "| epoch  18 |   301/  351 batches | loss 1.8058\n",
      "Epoch: 18 | train.loss.best: 18\n",
      "Epoch: 18 | Time: 0m 38s\n",
      "\tTrain Loss: 1.6617 | Train PPL:   5.2684\n",
      "\t Val. Loss: 5.7353 |  Val. PPL: 309.6017\n",
      "####################################################################################################\n",
      "| epoch  19 |     1/  351 batches | loss 1.4341\n",
      "| epoch  19 |   101/  351 batches | loss 1.4622\n",
      "| epoch  19 |   201/  351 batches | loss 1.5158\n",
      "| epoch  19 |   301/  351 batches | loss 2.1337\n",
      "Epoch: 19 | train.loss.best: 19\n",
      "Epoch: 19 | Time: 0m 38s\n",
      "\tTrain Loss: 1.6177 | Train PPL:   5.0413\n",
      "\t Val. Loss: 5.7167 |  Val. PPL: 303.9014\n",
      "####################################################################################################\n",
      "| epoch  20 |     1/  351 batches | loss 1.4267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  20 |   101/  351 batches | loss 1.6807\n",
      "| epoch  20 |   201/  351 batches | loss 1.6858\n",
      "| epoch  20 |   301/  351 batches | loss 1.5752\n",
      "Epoch: 20 | train.loss.best: 20\n",
      "Epoch: 20 | Time: 0m 38s\n",
      "\tTrain Loss: 1.5426 | Train PPL:   4.6767\n",
      "\t Val. Loss: 5.9945 |  Val. PPL: 401.2066\n",
      "####################################################################################################\n",
      "| epoch  21 |     1/  351 batches | loss 1.6226\n",
      "| epoch  21 |   101/  351 batches | loss 1.3365\n",
      "| epoch  21 |   201/  351 batches | loss 1.3941\n",
      "| epoch  21 |   301/  351 batches | loss 1.6456\n",
      "Epoch: 21 | train.loss.best: 21\n",
      "Epoch: 21 | Time: 0m 38s\n",
      "\tTrain Loss: 1.4822 | Train PPL:   4.4025\n",
      "\t Val. Loss: 6.1041 |  Val. PPL: 447.6856\n",
      "####################################################################################################\n",
      "| epoch  22 |     1/  351 batches | loss 1.3066\n",
      "| epoch  22 |   101/  351 batches | loss 1.3124\n",
      "| epoch  22 |   201/  351 batches | loss 1.3679\n",
      "| epoch  22 |   301/  351 batches | loss 1.4658\n",
      "Epoch: 22 | train.loss.best: 22\n",
      "Epoch: 22 | Time: 0m 38s\n",
      "\tTrain Loss: 1.4616 | Train PPL:   4.3128\n",
      "\t Val. Loss: 5.9987 |  Val. PPL: 402.9067\n",
      "####################################################################################################\n",
      "| epoch  23 |     1/  351 batches | loss 1.7874\n",
      "| epoch  23 |   101/  351 batches | loss 1.3579\n",
      "| epoch  23 |   201/  351 batches | loss 1.5735\n",
      "| epoch  23 |   301/  351 batches | loss 1.3837\n",
      "Epoch: 23 | train.loss.best: 23\n",
      "Epoch: 23 | Time: 0m 39s\n",
      "\tTrain Loss: 1.4073 | Train PPL:   4.0848\n",
      "\t Val. Loss: 6.1917 |  Val. PPL: 488.6531\n",
      "####################################################################################################\n",
      "| epoch  24 |     1/  351 batches | loss 1.1245\n",
      "| epoch  24 |   101/  351 batches | loss 1.5657\n",
      "| epoch  24 |   201/  351 batches | loss 1.2369\n",
      "| epoch  24 |   301/  351 batches | loss 1.3778\n",
      "Epoch: 24 | train.loss.best: 24\n",
      "Epoch: 24 | Time: 0m 38s\n",
      "\tTrain Loss: 1.3761 | Train PPL:   3.9594\n",
      "\t Val. Loss: 6.1497 |  Val. PPL: 468.5815\n",
      "####################################################################################################\n",
      "| epoch  25 |     1/  351 batches | loss 1.2677\n",
      "| epoch  25 |   101/  351 batches | loss 1.2510\n",
      "| epoch  25 |   201/  351 batches | loss 1.2111\n",
      "| epoch  25 |   301/  351 batches | loss 1.4927\n",
      "Epoch: 25 | train.loss.best: 25\n",
      "Epoch: 25 | Time: 0m 38s\n",
      "\tTrain Loss: 1.3546 | Train PPL:   3.8751\n",
      "\t Val. Loss: 6.1608 |  Val. PPL: 473.8057\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 25\n",
    "CLIP = 5\n",
    "\n",
    "best_train_loss = float('inf')\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, epoch)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), './models/s2s_att/train.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | train.loss.best: {epoch+1}')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/s2s_att/valid.loss.best.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | valid.loss.best: {epoch+1}')\n",
    "        \n",
    "    # lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):8.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {math.exp(valid_loss):8.4f}')\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 6.1608 | Test PPL: 473.8057 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/s2s_att/train.loss.best.pt'))\n",
    "\n",
    "test_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):8.4f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, q_sent=\"\", a_sent=None, char2index=None, index2char=None):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src = [char2index.get(SOS_TOKEN)]\n",
    "        src += [char2index.get(token, UNK_TOKEN_ID) for token in q_sent]\n",
    "        src += [char2index.get(EOS_TOKEN)]\n",
    "        \n",
    "        trg = [char2index.get(SOS_TOKEN)]\n",
    "        trg += [char2index.get(token, UNK_TOKEN_ID) for token in a_sent]\n",
    "        trg += [char2index.get(EOS_TOKEN)]\n",
    "\n",
    "        src = torch.LongTensor([src]).to(device)\n",
    "        trg = torch.LongTensor([trg]).to(device)\n",
    "\n",
    "        hyp_ys = model(src, trg, 1)\n",
    "        pred = torch.argmax(hyp_ys[0], dim=-1).detach().cpu().numpy()\n",
    "        \n",
    "        pred_sent = [index2char[token_id] for token_id in pred[1:]]\n",
    "        pred_sent = ''.join(pred_sent)\n",
    "        \n",
    "        print(pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random idx :  2732\n",
      "Q:  사랑이 밥 먹여주나\n",
      "A:  사랑이 밥은 먹여주지 않지만 행복을 줘요.\n",
      "사랑이 밥여 여여주지 줘지만 줘복이 줘줘줘<eos>\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(train_dataset), size=1)[0]\n",
    "print(\"random idx : \", idx)\n",
    "\n",
    "q_sent = train_dataset.data[idx][0]\n",
    "a_sent = train_dataset.data[idx][1]\n",
    "print(\"Q: \", q_sent)\n",
    "print(\"A: \", a_sent)\n",
    "\n",
    "inference(model, q_sent, a_sent, char2index=train_dataset.char2index, index2char=train_dataset.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
