{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 초기 환경 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "1.1.3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from pathlib import Path\n",
    "from torchvision import models\n",
    "# from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "\n",
    "print(torch.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DATA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0   1   2\n",
      "0        3_5_1123.jpg   3   5\n",
      "1       3_20_1048.jpg   3  20\n",
      "2         4_2_401.jpg   4   2\n",
      "3         4_7_740.jpg   4   7\n",
      "4         4_11_93.jpg   4  11\n",
      "...               ...  ..  ..\n",
      "15995  13_15_1600.jpg  13  15\n",
      "15996  13_16_1570.jpg  13  16\n",
      "15997   13_17_986.jpg  13  17\n",
      "15998  13_18_4980.jpg  13  18\n",
      "15999   13_20_282.jpg  13  20\n",
      "\n",
      "[16000 rows x 3 columns]\n",
      "             0\n",
      "0        0.jpg\n",
      "1        1.jpg\n",
      "2        2.jpg\n",
      "3        3.jpg\n",
      "4        4.jpg\n",
      "...        ...\n",
      "3992  3992.jpg\n",
      "3993  3993.jpg\n",
      "3994  3994.jpg\n",
      "3995  3995.jpg\n",
      "3996  3996.jpg\n",
      "\n",
      "[3997 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"./train/train.tsv\", sep='\\t', header=None)\n",
    "print(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./test/test.tsv\", sep='\\t', header=None)\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_label = {\n",
    "    (3, 5): 0,\n",
    "    (3, 20): 1,\n",
    "    (4, 2): 2,\n",
    "    (4, 7): 3,\n",
    "    (4, 11): 4,\n",
    "    (5, 8): 5,\n",
    "    (7, 1): 6,\n",
    "    (7, 20): 7,\n",
    "    (8, 6): 8,\n",
    "    (8, 9): 9,\n",
    "    (10, 20): 10,\n",
    "    (11, 14): 11,\n",
    "    (13, 1): 12,\n",
    "    (13, 6): 13,\n",
    "    (13, 9): 14,\n",
    "    (13, 15): 15,\n",
    "    (13, 16): 16,\n",
    "    (13, 17): 17,\n",
    "    (13, 18): 18,\n",
    "    (13, 20): 19\n",
    "}\n",
    "\n",
    "index2label = {v: k for k, v in combined_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tsv_path, data_path):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.data = self._preprocess(tsv_path)\n",
    "            \n",
    "    def _preprocess(self, tsv_path):\n",
    "        data = pd.read_csv(tsv_path, sep='\\t', header=None)\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Override 필수 함수\n",
    "    @lru_cache(maxsize=100000)\n",
    "    def __getitem__(self, index):\n",
    "        file_name = Path(self.data.iloc[index][0])\n",
    "        \n",
    "        if str(self.data_path) == 'train':\n",
    "            plant_label = self.data.iloc[index][1]\n",
    "            disease_label = self.data.iloc[index][2]\n",
    "        else:\n",
    "            plant_label = -1\n",
    "            disease_label = -1\n",
    "        \n",
    "        file_path = self.data_path / file_name\n",
    "        \n",
    "        data = Image.open(file_path)\n",
    "        data = asarray(data) # (H, W, C) \n",
    "        data = torch.FloatTensor(data)\n",
    "        data = data.permute(2, 0, 1).contiguous()  # (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        # print(data, plant_label, disease_label)\n",
    "        \n",
    "        label = combined_label.get((plant_label, disease_label), -1)\n",
    "        \n",
    "        if str(self.data_path) == 'train' and label == -1:\n",
    "            raise Exception('Label Error!!')\n",
    "        \n",
    "        return data, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800 3200\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PlantImageDataset(tsv_path=\"./train/train.tsv\", data_path=\"train\")\n",
    "\n",
    "\n",
    "train_size = int(len(train_dataset) * 0.8)\n",
    "valid_size = int(len(train_dataset) * 0.2)\n",
    "assert len(train_dataset) == (train_size + valid_size)\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "print(len(train_dataset), len(valid_dataset))\n",
    "\n",
    "test_dataset = PlantImageDataset(tsv_path=\"./test/test.tsv\", data_path=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 #16\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in test_loader:\n",
    "#     print(x, y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlantModel(\n",
      "  (pre_model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PlantModel(nn.Module):\n",
    "    def __init__(self, label_size=20):\n",
    "        super(PlantModel, self).__init__()\n",
    "\n",
    "        self.pre_model = models.resnext101_32x8d()\n",
    "        #self.pre_model = models.wide_resnet101_2()\n",
    "        self.pre_model.fc = nn.Linear(2048, label_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_model(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = PlantModel()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.wide_resnet50_2()\n",
    "# model.fc = nn.Linear(2048, 14)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16()\n",
    "# model.classifier._modules['6'] = nn.Linear(4096, 14)\n",
    "# # model.fc = nn.Linear(1000, 14)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer=None, device=None, epoch=None):\n",
    "    model.train() # 학습 모드\n",
    "    \n",
    "    total_loss = 0.\n",
    "    total_num = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    for batch, (data, label) in enumerate(train_loader):\n",
    "        # data\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hyp = model(data)\n",
    "        \n",
    "        loss = criterion(hyp, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "        total_num += 1\n",
    "        \n",
    "        log_interval = 100\n",
    "        if batch % log_interval == 0 and batch >= 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | grad_norm {:5.2f} | loss {:5.2f} | time {:.4f}'.format(\n",
    "                    epoch+1, batch+1, len(train_loader), grad_norm, loss.detach().item(), elapsed))\n",
    "            start_time = time.time()\n",
    "    \n",
    "    train_loss = total_loss / total_num\n",
    "    return train_loss\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(model, valid_loader, criterion, optimizer=None, device=None, epoch=None, print_output=False):\n",
    "    model.eval() # 평가 모드\n",
    "    \n",
    "    total_loss = 0.\n",
    "    total_num = 0\n",
    "    \n",
    "    correct= 0\n",
    "    total_size = 0\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for batch, (data, label) in enumerate(valid_loader):\n",
    "            # data\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            hyp = model(data)\n",
    "        \n",
    "            loss = criterion(hyp, label)\n",
    "            \n",
    "            if print_output:\n",
    "                hyp = torch.argmax(hyp, dim=-1)\n",
    "                correct += (hyp == label).sum().item()\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "            total_num += 1\n",
    "            total_size += data.size(0)\n",
    "    \n",
    "    valid_loss = total_loss / total_num\n",
    "    accuracy = correct / total_size\n",
    "    \n",
    "    return valid_loss, accuracy\n",
    "\n",
    "PATH = './models/'\n",
    "\n",
    "def save_model():\n",
    "    torch.save(model, PATH + 'model.pt')  # 전체 모델 저장\n",
    "    torch.save(model.state_dict(), PATH + 'model_state_dict.pt')  # 모델 객체의 state_dict 저장\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, PATH + 'all.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/ 1600 batches | grad_norm 8841.16 | loss 26.05 | time 1.2172\n",
      "| epoch   1 |   101/ 1600 batches | grad_norm 11.11 | loss 23.83 | time 33.6796\n",
      "| epoch   1 |   201/ 1600 batches | grad_norm  8.70 | loss 17.20 | time 33.7390\n",
      "| epoch   1 |   301/ 1600 batches | grad_norm 11.33 | loss 16.58 | time 33.8337\n",
      "| epoch   1 |   401/ 1600 batches | grad_norm 12.00 | loss 19.12 | time 33.9338\n",
      "| epoch   1 |   501/ 1600 batches | grad_norm  8.35 | loss 15.76 | time 33.9479\n",
      "| epoch   1 |   601/ 1600 batches | grad_norm 11.09 | loss 17.08 | time 34.2105\n",
      "| epoch   1 |   701/ 1600 batches | grad_norm  8.35 | loss 16.69 | time 33.9752\n",
      "| epoch   1 |   801/ 1600 batches | grad_norm  8.75 | loss 17.55 | time 33.9737\n",
      "| epoch   1 |   901/ 1600 batches | grad_norm 11.55 | loss 19.78 | time 34.0069\n",
      "| epoch   1 |  1001/ 1600 batches | grad_norm  7.52 | loss 14.46 | time 34.1090\n",
      "| epoch   1 |  1101/ 1600 batches | grad_norm  7.83 | loss 18.05 | time 34.0244\n",
      "| epoch   1 |  1201/ 1600 batches | grad_norm 13.90 | loss 22.47 | time 34.0822\n",
      "| epoch   1 |  1301/ 1600 batches | grad_norm  8.78 | loss 13.28 | time 34.0915\n",
      "| epoch   1 |  1401/ 1600 batches | grad_norm 11.64 | loss 11.69 | time 34.0833\n",
      "| epoch   1 |  1501/ 1600 batches | grad_norm 12.29 | loss 16.64 | time 34.2492\n",
      "| epoch   1 | train_loss : 20.3109\n",
      "| epoch   1 | valid_loss : 17.1851 accuracy : 0.3070\n",
      "| epoch   2 |     1/ 1600 batches | grad_norm 11.00 | loss 21.45 | time 0.3460\n",
      "| epoch   2 |   101/ 1600 batches | grad_norm 11.13 | loss 13.85 | time 31.8930\n",
      "| epoch   2 |   201/ 1600 batches | grad_norm 11.14 | loss 14.76 | time 31.9394\n",
      "| epoch   2 |   301/ 1600 batches | grad_norm 10.80 | loss 15.63 | time 32.0001\n",
      "| epoch   2 |   401/ 1600 batches | grad_norm  8.50 | loss 10.16 | time 32.0904\n",
      "| epoch   2 |   501/ 1600 batches | grad_norm  9.21 | loss 14.90 | time 31.9516\n",
      "| epoch   2 |   601/ 1600 batches | grad_norm 14.96 | loss 20.42 | time 31.8978\n",
      "| epoch   2 |   701/ 1600 batches | grad_norm 11.01 | loss 13.02 | time 31.8189\n",
      "| epoch   2 |   801/ 1600 batches | grad_norm 13.52 | loss 12.30 | time 31.8471\n",
      "| epoch   2 |   901/ 1600 batches | grad_norm 11.92 | loss 11.46 | time 31.8462\n",
      "| epoch   2 |  1001/ 1600 batches | grad_norm 15.87 | loss 14.68 | time 31.8381\n",
      "| epoch   2 |  1101/ 1600 batches | grad_norm  9.09 | loss  8.90 | time 31.8471\n",
      "| epoch   2 |  1201/ 1600 batches | grad_norm 15.79 | loss 12.30 | time 31.8403\n",
      "| epoch   2 |  1301/ 1600 batches | grad_norm 17.20 | loss 20.47 | time 31.8632\n",
      "| epoch   2 |  1401/ 1600 batches | grad_norm 11.63 | loss  7.64 | time 31.8316\n",
      "| epoch   2 |  1501/ 1600 batches | grad_norm 10.11 | loss  7.61 | time 31.8309\n",
      "| epoch   2 | train_loss : 12.1294\n",
      "| epoch   2 | valid_loss :  9.7239 accuracy : 0.6129\n",
      "| epoch   3 |     1/ 1600 batches | grad_norm 13.64 | loss 13.71 | time 0.3560\n",
      "| epoch   3 |   101/ 1600 batches | grad_norm 10.88 | loss  7.70 | time 31.8394\n",
      "| epoch   3 |   201/ 1600 batches | grad_norm 13.25 | loss 13.77 | time 31.8265\n",
      "| epoch   3 |   301/ 1600 batches | grad_norm 11.03 | loss  4.76 | time 31.7826\n",
      "| epoch   3 |   401/ 1600 batches | grad_norm  7.66 | loss  5.44 | time 31.8419\n",
      "| epoch   3 |   501/ 1600 batches | grad_norm 10.79 | loss  7.12 | time 31.7816\n",
      "| epoch   3 |   601/ 1600 batches | grad_norm 13.66 | loss 13.61 | time 31.7843\n",
      "| epoch   3 |   701/ 1600 batches | grad_norm  8.52 | loss  5.40 | time 31.8153\n",
      "| epoch   3 |   801/ 1600 batches | grad_norm 16.13 | loss 11.13 | time 31.7924\n",
      "| epoch   3 |   901/ 1600 batches | grad_norm 11.31 | loss  8.63 | time 31.8261\n",
      "| epoch   3 |  1001/ 1600 batches | grad_norm 10.33 | loss  6.67 | time 31.7816\n",
      "| epoch   3 |  1101/ 1600 batches | grad_norm  6.90 | loss  6.96 | time 31.8011\n",
      "| epoch   3 |  1201/ 1600 batches | grad_norm  6.65 | loss  5.46 | time 31.7822\n",
      "| epoch   3 |  1301/ 1600 batches | grad_norm  9.52 | loss  4.49 | time 31.7830\n",
      "| epoch   3 |  1401/ 1600 batches | grad_norm  9.88 | loss  6.63 | time 31.7793\n",
      "| epoch   3 |  1501/ 1600 batches | grad_norm 13.32 | loss  7.42 | time 31.7539\n",
      "| epoch   3 | train_loss :  7.1285\n",
      "| epoch   3 | valid_loss :  6.6657 accuracy : 0.7138\n",
      "| epoch   4 |     1/ 1600 batches | grad_norm 10.35 | loss  4.82 | time 0.3648\n",
      "| epoch   4 |   101/ 1600 batches | grad_norm  7.42 | loss  2.62 | time 35.0993\n",
      "| epoch   4 |   201/ 1600 batches | grad_norm 13.58 | loss  6.54 | time 32.7785\n",
      "| epoch   4 |   301/ 1600 batches | grad_norm  9.77 | loss  4.83 | time 33.3452\n",
      "| epoch   4 |   401/ 1600 batches | grad_norm  2.96 | loss  0.93 | time 32.2357\n",
      "| epoch   4 |   501/ 1600 batches | grad_norm 12.99 | loss  6.27 | time 32.0527\n",
      "| epoch   4 |   601/ 1600 batches | grad_norm 15.31 | loss  8.88 | time 31.8339\n",
      "| epoch   4 |   701/ 1600 batches | grad_norm  6.80 | loss  2.80 | time 31.8685\n",
      "| epoch   4 |   801/ 1600 batches | grad_norm  5.24 | loss  2.09 | time 31.8535\n",
      "| epoch   4 |   901/ 1600 batches | grad_norm  2.93 | loss  1.00 | time 31.8745\n",
      "| epoch   4 |  1001/ 1600 batches | grad_norm  8.92 | loss  3.63 | time 31.9046\n",
      "| epoch   4 |  1101/ 1600 batches | grad_norm 12.71 | loss  8.88 | time 31.7827\n",
      "| epoch   4 |  1201/ 1600 batches | grad_norm  6.50 | loss  2.96 | time 32.0798\n",
      "| epoch   4 |  1301/ 1600 batches | grad_norm  3.12 | loss  1.20 | time 32.0496\n",
      "| epoch   4 |  1401/ 1600 batches | grad_norm  6.81 | loss  2.71 | time 31.8872\n",
      "| epoch   4 |  1501/ 1600 batches | grad_norm 14.51 | loss  8.21 | time 31.9757\n",
      "| epoch   4 | train_loss :  5.2127\n",
      "| epoch   4 | valid_loss :  7.8314 accuracy : 0.7081\n",
      "| epoch   5 |     1/ 1600 batches | grad_norm  9.70 | loss  2.45 | time 0.3494\n",
      "| epoch   5 |   101/ 1600 batches | grad_norm 10.97 | loss  4.67 | time 31.7807\n",
      "| epoch   5 |   201/ 1600 batches | grad_norm 10.42 | loss  3.03 | time 31.8545\n",
      "| epoch   5 |   301/ 1600 batches | grad_norm  9.08 | loss  4.12 | time 31.8319\n",
      "| epoch   5 |   401/ 1600 batches | grad_norm 11.11 | loss  7.67 | time 31.8406\n",
      "| epoch   5 |   501/ 1600 batches | grad_norm  3.25 | loss  1.25 | time 31.9049\n",
      "| epoch   5 |   601/ 1600 batches | grad_norm  2.53 | loss  0.40 | time 31.9096\n",
      "| epoch   5 |   701/ 1600 batches | grad_norm 14.78 | loss  7.60 | time 32.0370\n",
      "| epoch   5 |   801/ 1600 batches | grad_norm  6.70 | loss  4.47 | time 31.8241\n",
      "| epoch   5 |   901/ 1600 batches | grad_norm  5.27 | loss  2.77 | time 31.8340\n",
      "| epoch   5 |  1001/ 1600 batches | grad_norm 10.35 | loss  6.75 | time 31.8478\n",
      "| epoch   5 |  1101/ 1600 batches | grad_norm 13.15 | loss  9.67 | time 31.8174\n",
      "| epoch   5 |  1201/ 1600 batches | grad_norm 13.10 | loss  8.42 | time 31.8664\n",
      "| epoch   5 |  1301/ 1600 batches | grad_norm  6.19 | loss  4.12 | time 31.8519\n",
      "| epoch   5 |  1401/ 1600 batches | grad_norm 11.03 | loss  4.81 | time 31.8803\n",
      "| epoch   5 |  1501/ 1600 batches | grad_norm  4.70 | loss  1.73 | time 31.7980\n",
      "| epoch   5 | train_loss :  4.3220\n",
      "| epoch   5 | valid_loss :  5.6528 accuracy : 0.7589\n",
      "| epoch   6 |     1/ 1600 batches | grad_norm  2.32 | loss  0.73 | time 0.3453\n",
      "| epoch   6 |   101/ 1600 batches | grad_norm  1.15 | loss  0.34 | time 31.6903\n",
      "| epoch   6 |   201/ 1600 batches | grad_norm  6.70 | loss  2.91 | time 31.7719\n",
      "| epoch   6 |   301/ 1600 batches | grad_norm  2.52 | loss  0.43 | time 31.6978\n",
      "| epoch   6 |   401/ 1600 batches | grad_norm  9.86 | loss  1.76 | time 31.6899\n",
      "| epoch   6 |   501/ 1600 batches | grad_norm  9.40 | loss  6.19 | time 31.6948\n",
      "| epoch   6 |   601/ 1600 batches | grad_norm  2.36 | loss  0.92 | time 31.6959\n",
      "| epoch   6 |   701/ 1600 batches | grad_norm  1.28 | loss  0.15 | time 31.7153\n",
      "| epoch   6 |   801/ 1600 batches | grad_norm  2.33 | loss  0.38 | time 31.6660\n",
      "| epoch   6 |   901/ 1600 batches | grad_norm  8.64 | loss  2.58 | time 31.7193\n",
      "| epoch   6 |  1001/ 1600 batches | grad_norm  4.51 | loss  1.05 | time 31.7295\n",
      "| epoch   6 |  1101/ 1600 batches | grad_norm  5.73 | loss  2.27 | time 31.6858\n",
      "| epoch   6 |  1201/ 1600 batches | grad_norm 10.11 | loss  3.54 | time 31.7151\n",
      "| epoch   6 |  1301/ 1600 batches | grad_norm  7.54 | loss  3.59 | time 31.7074\n",
      "| epoch   6 |  1401/ 1600 batches | grad_norm 10.84 | loss  5.78 | time 31.7457\n",
      "| epoch   6 |  1501/ 1600 batches | grad_norm  6.43 | loss  2.84 | time 31.7146\n",
      "| epoch   6 | train_loss :  3.6257\n",
      "| epoch   6 | valid_loss :  2.3672 accuracy : 0.8982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |     1/ 1600 batches | grad_norm  5.57 | loss  1.34 | time 0.3531\n",
      "| epoch   7 |   101/ 1600 batches | grad_norm 14.65 | loss  6.76 | time 31.6718\n",
      "| epoch   7 |   201/ 1600 batches | grad_norm  5.06 | loss  1.41 | time 31.7184\n",
      "| epoch   7 |   301/ 1600 batches | grad_norm 10.86 | loss  4.64 | time 31.6974\n",
      "| epoch   7 |   401/ 1600 batches | grad_norm 11.81 | loss  8.40 | time 31.7001\n",
      "| epoch   7 |   501/ 1600 batches | grad_norm  6.44 | loss  1.76 | time 31.6652\n",
      "| epoch   7 |   601/ 1600 batches | grad_norm  1.85 | loss  0.50 | time 31.6826\n",
      "| epoch   7 |   701/ 1600 batches | grad_norm  2.68 | loss  0.61 | time 31.6804\n",
      "| epoch   7 |   801/ 1600 batches | grad_norm  9.52 | loss  5.66 | time 31.7028\n",
      "| epoch   7 |   901/ 1600 batches | grad_norm  6.54 | loss  1.50 | time 31.7331\n",
      "| epoch   7 |  1001/ 1600 batches | grad_norm  6.79 | loss  4.00 | time 31.6684\n",
      "| epoch   7 |  1101/ 1600 batches | grad_norm  0.64 | loss  0.09 | time 31.7133\n",
      "| epoch   7 |  1201/ 1600 batches | grad_norm  9.65 | loss  7.33 | time 31.6895\n",
      "| epoch   7 |  1301/ 1600 batches | grad_norm 12.40 | loss  3.92 | time 31.7289\n",
      "| epoch   7 |  1401/ 1600 batches | grad_norm  5.96 | loss  1.38 | time 31.7163\n",
      "| epoch   7 |  1501/ 1600 batches | grad_norm  3.24 | loss  0.71 | time 31.6851\n",
      "| epoch   7 | train_loss :  3.1040\n",
      "| epoch   7 | valid_loss :  2.4761 accuracy : 0.8946\n",
      "| epoch   8 |     1/ 1600 batches | grad_norm 10.10 | loss  5.29 | time 0.3551\n",
      "| epoch   8 |   101/ 1600 batches | grad_norm  0.65 | loss  0.16 | time 31.6656\n",
      "| epoch   8 |   201/ 1600 batches | grad_norm  7.09 | loss  2.17 | time 31.8004\n",
      "| epoch   8 |   301/ 1600 batches | grad_norm  8.24 | loss  2.71 | time 31.7589\n",
      "| epoch   8 |   401/ 1600 batches | grad_norm  8.57 | loss  1.63 | time 31.8563\n",
      "| epoch   8 |   501/ 1600 batches | grad_norm 12.72 | loss  7.45 | time 31.9294\n",
      "| epoch   8 |   601/ 1600 batches | grad_norm  7.28 | loss  2.34 | time 31.7901\n",
      "| epoch   8 |   701/ 1600 batches | grad_norm  4.95 | loss  0.90 | time 32.1438\n",
      "| epoch   8 |   801/ 1600 batches | grad_norm  5.67 | loss  2.48 | time 32.3879\n",
      "| epoch   8 |   901/ 1600 batches | grad_norm  4.27 | loss  1.49 | time 32.3987\n",
      "| epoch   8 |  1001/ 1600 batches | grad_norm  6.35 | loss  1.70 | time 32.2134\n",
      "| epoch   8 |  1101/ 1600 batches | grad_norm  5.10 | loss  1.36 | time 32.1173\n",
      "| epoch   8 |  1201/ 1600 batches | grad_norm  1.89 | loss  0.32 | time 32.1137\n",
      "| epoch   8 |  1301/ 1600 batches | grad_norm  7.78 | loss  3.20 | time 32.0450\n",
      "| epoch   8 |  1401/ 1600 batches | grad_norm  1.14 | loss  0.24 | time 32.1980\n",
      "| epoch   8 |  1501/ 1600 batches | grad_norm  8.17 | loss  5.62 | time 32.1258\n",
      "| epoch   8 | train_loss :  2.7594\n",
      "| epoch   8 | valid_loss :  5.0434 accuracy : 0.8234\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-03.\n",
      "| epoch   9 |     1/ 1600 batches | grad_norm 10.14 | loss  4.27 | time 0.3431\n",
      "| epoch   9 |   101/ 1600 batches | grad_norm  7.25 | loss  2.85 | time 31.8887\n",
      "| epoch   9 |   201/ 1600 batches | grad_norm  0.38 | loss  0.07 | time 31.8794\n",
      "| epoch   9 |   301/ 1600 batches | grad_norm  1.84 | loss  0.31 | time 31.8676\n",
      "| epoch   9 |   401/ 1600 batches | grad_norm  3.53 | loss  0.68 | time 31.8667\n",
      "| epoch   9 |   501/ 1600 batches | grad_norm  6.24 | loss  1.70 | time 31.8915\n",
      "| epoch   9 |   601/ 1600 batches | grad_norm  4.53 | loss  1.58 | time 31.8644\n",
      "| epoch   9 |   701/ 1600 batches | grad_norm  5.65 | loss  1.88 | time 31.8604\n",
      "| epoch   9 |   801/ 1600 batches | grad_norm  7.79 | loss  3.63 | time 31.8026\n",
      "| epoch   9 |   901/ 1600 batches | grad_norm  3.02 | loss  0.58 | time 31.8623\n",
      "| epoch   9 |  1001/ 1600 batches | grad_norm  2.36 | loss  0.48 | time 31.8233\n",
      "| epoch   9 |  1101/ 1600 batches | grad_norm  2.29 | loss  0.41 | time 31.8436\n",
      "| epoch   9 |  1201/ 1600 batches | grad_norm  0.62 | loss  0.14 | time 31.8501\n",
      "| epoch   9 |  1301/ 1600 batches | grad_norm  0.82 | loss  0.14 | time 31.8851\n",
      "| epoch   9 |  1401/ 1600 batches | grad_norm  3.13 | loss  0.61 | time 31.8144\n",
      "| epoch   9 |  1501/ 1600 batches | grad_norm  7.90 | loss  1.85 | time 31.8889\n",
      "| epoch   9 | train_loss :  1.6908\n",
      "| epoch   9 | valid_loss :  1.5719 accuracy : 0.9345\n",
      "| epoch  10 |     1/ 1600 batches | grad_norm  5.98 | loss  0.92 | time 0.3587\n",
      "| epoch  10 |   101/ 1600 batches | grad_norm  3.29 | loss  0.35 | time 31.8637\n",
      "| epoch  10 |   201/ 1600 batches | grad_norm  2.84 | loss  0.52 | time 31.8741\n",
      "| epoch  10 |   301/ 1600 batches | grad_norm  0.69 | loss  0.09 | time 31.9078\n",
      "| epoch  10 |   401/ 1600 batches | grad_norm  3.87 | loss  1.00 | time 31.8871\n",
      "| epoch  10 |   501/ 1600 batches | grad_norm  8.17 | loss  1.28 | time 31.8089\n",
      "| epoch  10 |   601/ 1600 batches | grad_norm 14.04 | loss  5.16 | time 31.8739\n",
      "| epoch  10 |   701/ 1600 batches | grad_norm  5.17 | loss  0.94 | time 31.8515\n",
      "| epoch  10 |   801/ 1600 batches | grad_norm  6.02 | loss  1.48 | time 31.9038\n",
      "| epoch  10 |   901/ 1600 batches | grad_norm  8.33 | loss  3.26 | time 31.8623\n",
      "| epoch  10 |  1001/ 1600 batches | grad_norm  2.55 | loss  0.44 | time 31.8492\n",
      "| epoch  10 |  1101/ 1600 batches | grad_norm  0.49 | loss  0.08 | time 31.8569\n",
      "| epoch  10 |  1201/ 1600 batches | grad_norm  8.24 | loss  3.43 | time 31.8905\n",
      "| epoch  10 |  1301/ 1600 batches | grad_norm  1.85 | loss  0.28 | time 31.8411\n",
      "| epoch  10 |  1401/ 1600 batches | grad_norm  2.60 | loss  0.39 | time 31.8649\n",
      "| epoch  10 |  1501/ 1600 batches | grad_norm  6.64 | loss  1.05 | time 31.8736\n",
      "| epoch  10 | train_loss :  1.5032\n",
      "| epoch  10 | valid_loss :  2.7815 accuracy : 0.9114\n",
      "| epoch  11 |     1/ 1600 batches | grad_norm  1.90 | loss  0.35 | time 0.3489\n",
      "| epoch  11 |   101/ 1600 batches | grad_norm  7.64 | loss  2.41 | time 31.8894\n",
      "| epoch  11 |   201/ 1600 batches | grad_norm  0.96 | loss  0.15 | time 31.8975\n",
      "| epoch  11 |   301/ 1600 batches | grad_norm  2.61 | loss  0.39 | time 31.8245\n",
      "| epoch  11 |   401/ 1600 batches | grad_norm  2.47 | loss  0.35 | time 31.9578\n",
      "| epoch  11 |   501/ 1600 batches | grad_norm  0.87 | loss  0.14 | time 32.0528\n",
      "| epoch  11 |   601/ 1600 batches | grad_norm  6.21 | loss  1.55 | time 32.0035\n",
      "| epoch  11 |   701/ 1600 batches | grad_norm  4.49 | loss  0.71 | time 32.0032\n",
      "| epoch  11 |   801/ 1600 batches | grad_norm  0.59 | loss  0.10 | time 32.0087\n",
      "| epoch  11 |   901/ 1600 batches | grad_norm  1.53 | loss  0.21 | time 31.9635\n",
      "| epoch  11 |  1001/ 1600 batches | grad_norm  4.51 | loss  0.73 | time 31.9905\n",
      "| epoch  11 |  1101/ 1600 batches | grad_norm  0.03 | loss  0.00 | time 31.9667\n",
      "| epoch  11 |  1201/ 1600 batches | grad_norm  1.37 | loss  0.18 | time 31.9716\n",
      "| epoch  11 |  1301/ 1600 batches | grad_norm  5.12 | loss  0.95 | time 31.9951\n",
      "| epoch  11 |  1401/ 1600 batches | grad_norm  1.00 | loss  0.17 | time 31.9944\n",
      "| epoch  11 |  1501/ 1600 batches | grad_norm  3.51 | loss  0.80 | time 31.9889\n",
      "| epoch  11 | train_loss :  1.4272\n",
      "| epoch  11 | valid_loss :  0.8889 accuracy : 0.9600\n",
      "| epoch  12 |     1/ 1600 batches | grad_norm  5.27 | loss  0.88 | time 0.3546\n",
      "| epoch  12 |   101/ 1600 batches | grad_norm  8.45 | loss  2.00 | time 31.8351\n",
      "| epoch  12 |   201/ 1600 batches | grad_norm  0.46 | loss  0.07 | time 31.8248\n",
      "| epoch  12 |   301/ 1600 batches | grad_norm 13.57 | loss  8.69 | time 31.8229\n",
      "| epoch  12 |   401/ 1600 batches | grad_norm  4.03 | loss  0.61 | time 31.8299\n",
      "| epoch  12 |   501/ 1600 batches | grad_norm  0.32 | loss  0.05 | time 31.8730\n",
      "| epoch  12 |   601/ 1600 batches | grad_norm 15.49 | loss  6.46 | time 31.9070\n",
      "| epoch  12 |   701/ 1600 batches | grad_norm  3.32 | loss  0.41 | time 31.8671\n",
      "| epoch  12 |   801/ 1600 batches | grad_norm  1.79 | loss  0.24 | time 31.8622\n",
      "| epoch  12 |   901/ 1600 batches | grad_norm  0.28 | loss  0.05 | time 31.8734\n",
      "| epoch  12 |  1001/ 1600 batches | grad_norm  9.19 | loss  1.23 | time 31.8012\n",
      "| epoch  12 |  1101/ 1600 batches | grad_norm  6.34 | loss  1.38 | time 31.8255\n",
      "| epoch  12 |  1201/ 1600 batches | grad_norm  6.70 | loss  0.83 | time 31.8914\n",
      "| epoch  12 |  1301/ 1600 batches | grad_norm 14.64 | loss  4.36 | time 31.8719\n",
      "| epoch  12 |  1401/ 1600 batches | grad_norm  7.65 | loss  1.36 | time 31.9375\n",
      "| epoch  12 |  1501/ 1600 batches | grad_norm  2.76 | loss  0.39 | time 31.9175\n",
      "| epoch  12 | train_loss :  1.2326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 | valid_loss :  1.2322 accuracy : 0.9486\n",
      "| epoch  13 |     1/ 1600 batches | grad_norm  8.43 | loss  1.05 | time 0.3537\n",
      "| epoch  13 |   101/ 1600 batches | grad_norm  0.26 | loss  0.03 | time 31.9055\n",
      "| epoch  13 |   201/ 1600 batches | grad_norm  4.03 | loss  0.51 | time 31.9435\n",
      "| epoch  13 |   301/ 1600 batches | grad_norm 12.16 | loss  2.00 | time 32.0420\n",
      "| epoch  13 |   401/ 1600 batches | grad_norm  0.17 | loss  0.02 | time 32.0891\n",
      "| epoch  13 |   501/ 1600 batches | grad_norm  1.55 | loss  0.21 | time 32.0095\n",
      "| epoch  13 |   601/ 1600 batches | grad_norm 16.57 | loss  6.36 | time 32.1582\n",
      "| epoch  13 |   701/ 1600 batches | grad_norm  2.63 | loss  0.50 | time 32.1584\n",
      "| epoch  13 |   801/ 1600 batches | grad_norm  7.49 | loss  1.39 | time 32.1073\n",
      "| epoch  13 |   901/ 1600 batches | grad_norm 12.00 | loss  1.90 | time 32.0241\n",
      "| epoch  13 |  1001/ 1600 batches | grad_norm 13.57 | loss  2.29 | time 32.0234\n",
      "| epoch  13 |  1101/ 1600 batches | grad_norm  3.28 | loss  0.31 | time 32.0468\n",
      "| epoch  13 |  1201/ 1600 batches | grad_norm  1.70 | loss  0.25 | time 31.8750\n",
      "| epoch  13 |  1301/ 1600 batches | grad_norm  7.04 | loss  1.33 | time 31.9076\n",
      "| epoch  13 |  1401/ 1600 batches | grad_norm  2.46 | loss  0.35 | time 32.0178\n",
      "| epoch  13 |  1501/ 1600 batches | grad_norm 14.17 | loss  4.50 | time 31.9794\n",
      "| epoch  13 | train_loss :  1.1757\n",
      "| epoch  13 | valid_loss :  0.7536 accuracy : 0.9682\n",
      "| epoch  14 |     1/ 1600 batches | grad_norm  6.81 | loss  1.25 | time 0.3552\n",
      "| epoch  14 |   101/ 1600 batches | grad_norm  2.51 | loss  0.34 | time 31.7477\n",
      "| epoch  14 |   201/ 1600 batches | grad_norm  0.28 | loss  0.05 | time 31.7992\n",
      "| epoch  14 |   301/ 1600 batches | grad_norm 10.32 | loss  1.65 | time 31.8054\n",
      "| epoch  14 |   401/ 1600 batches | grad_norm  1.51 | loss  0.17 | time 31.7658\n",
      "| epoch  14 |   501/ 1600 batches | grad_norm  0.96 | loss  0.12 | time 31.8627\n",
      "| epoch  14 |   601/ 1600 batches | grad_norm  2.03 | loss  0.26 | time 32.0741\n",
      "| epoch  14 |   701/ 1600 batches | grad_norm  1.48 | loss  0.15 | time 31.9325\n",
      "| epoch  14 |   801/ 1600 batches | grad_norm  9.64 | loss  1.97 | time 32.1146\n",
      "| epoch  14 |   901/ 1600 batches | grad_norm  0.73 | loss  0.08 | time 31.8640\n",
      "| epoch  14 |  1001/ 1600 batches | grad_norm  3.95 | loss  0.63 | time 31.9025\n",
      "| epoch  14 |  1101/ 1600 batches | grad_norm 14.22 | loss  7.07 | time 32.0159\n",
      "| epoch  14 |  1201/ 1600 batches | grad_norm  4.85 | loss  0.80 | time 31.8126\n",
      "| epoch  14 |  1301/ 1600 batches | grad_norm 13.98 | loss  2.57 | time 31.7987\n",
      "| epoch  14 |  1401/ 1600 batches | grad_norm  4.81 | loss  0.45 | time 31.7903\n",
      "| epoch  14 |  1501/ 1600 batches | grad_norm  9.28 | loss  6.16 | time 31.7946\n",
      "| epoch  14 | train_loss :  1.1159\n",
      "| epoch  14 | valid_loss :  0.5732 accuracy : 0.9753\n",
      "| epoch  15 |     1/ 1600 batches | grad_norm  1.10 | loss  0.12 | time 0.3505\n",
      "| epoch  15 |   101/ 1600 batches | grad_norm  1.91 | loss  0.29 | time 31.7858\n",
      "| epoch  15 |   201/ 1600 batches | grad_norm  0.30 | loss  0.03 | time 31.7473\n",
      "| epoch  15 |   301/ 1600 batches | grad_norm  0.40 | loss  0.05 | time 31.7756\n",
      "| epoch  15 |   401/ 1600 batches | grad_norm  0.35 | loss  0.06 | time 31.7560\n",
      "| epoch  15 |   501/ 1600 batches | grad_norm  6.30 | loss  0.68 | time 31.8091\n",
      "| epoch  15 |   601/ 1600 batches | grad_norm  1.27 | loss  0.21 | time 31.7792\n",
      "| epoch  15 |   701/ 1600 batches | grad_norm  1.73 | loss  0.16 | time 31.8465\n",
      "| epoch  15 |   801/ 1600 batches | grad_norm  4.89 | loss  0.71 | time 31.7560\n",
      "| epoch  15 |   901/ 1600 batches | grad_norm  9.46 | loss  4.16 | time 31.8118\n",
      "| epoch  15 |  1001/ 1600 batches | grad_norm  4.01 | loss  0.79 | time 31.7491\n",
      "| epoch  15 |  1101/ 1600 batches | grad_norm  9.03 | loss  3.89 | time 31.7833\n",
      "| epoch  15 |  1201/ 1600 batches | grad_norm  9.78 | loss  2.14 | time 31.8005\n",
      "| epoch  15 |  1301/ 1600 batches | grad_norm  0.35 | loss  0.04 | time 31.8136\n",
      "| epoch  15 |  1401/ 1600 batches | grad_norm  0.26 | loss  0.06 | time 31.7990\n",
      "| epoch  15 |  1501/ 1600 batches | grad_norm  3.83 | loss  0.61 | time 31.8305\n",
      "| epoch  15 | train_loss :  1.0772\n",
      "| epoch  15 | valid_loss :  0.3931 accuracy : 0.9833\n",
      "| epoch  16 |     1/ 1600 batches | grad_norm  0.43 | loss  0.07 | time 0.3526\n",
      "| epoch  16 |   101/ 1600 batches | grad_norm  5.82 | loss  0.85 | time 31.7478\n",
      "| epoch  16 |   201/ 1600 batches | grad_norm  3.09 | loss  0.31 | time 31.7514\n",
      "| epoch  16 |   301/ 1600 batches | grad_norm  8.48 | loss  1.52 | time 31.8035\n",
      "| epoch  16 |   401/ 1600 batches | grad_norm 17.62 | loss  6.92 | time 31.8115\n",
      "| epoch  16 |   501/ 1600 batches | grad_norm  0.40 | loss  0.04 | time 31.7523\n",
      "| epoch  16 |   601/ 1600 batches | grad_norm  3.94 | loss  0.37 | time 31.7957\n",
      "| epoch  16 |   701/ 1600 batches | grad_norm  0.66 | loss  0.07 | time 31.7699\n",
      "| epoch  16 |   801/ 1600 batches | grad_norm  1.26 | loss  0.18 | time 31.7348\n",
      "| epoch  16 |   901/ 1600 batches | grad_norm 12.12 | loss  1.85 | time 31.7713\n",
      "| epoch  16 |  1001/ 1600 batches | grad_norm  1.70 | loss  0.26 | time 31.8031\n",
      "| epoch  16 |  1101/ 1600 batches | grad_norm  7.39 | loss  0.83 | time 31.8554\n",
      "| epoch  16 |  1201/ 1600 batches | grad_norm  7.53 | loss  2.01 | time 31.7823\n",
      "| epoch  16 |  1301/ 1600 batches | grad_norm 14.69 | loss  3.32 | time 31.7668\n",
      "| epoch  16 |  1401/ 1600 batches | grad_norm  0.19 | loss  0.02 | time 31.7979\n",
      "| epoch  16 |  1501/ 1600 batches | grad_norm  6.62 | loss  0.75 | time 31.7757\n",
      "| epoch  16 | train_loss :  1.0280\n",
      "| epoch  16 | valid_loss :  0.5001 accuracy : 0.9780\n",
      "| epoch  17 |     1/ 1600 batches | grad_norm  0.94 | loss  0.13 | time 0.3533\n",
      "| epoch  17 |   101/ 1600 batches | grad_norm  0.33 | loss  0.05 | time 31.7853\n",
      "| epoch  17 |   201/ 1600 batches | grad_norm  0.34 | loss  0.04 | time 31.7728\n",
      "| epoch  17 |   301/ 1600 batches | grad_norm  6.48 | loss  0.62 | time 31.7800\n",
      "| epoch  17 |   401/ 1600 batches | grad_norm  6.97 | loss  1.24 | time 31.7873\n",
      "| epoch  17 |   501/ 1600 batches | grad_norm 17.66 | loss  2.25 | time 31.7989\n",
      "| epoch  17 |   601/ 1600 batches | grad_norm  0.01 | loss  0.00 | time 31.7802\n",
      "| epoch  17 |   701/ 1600 batches | grad_norm 10.51 | loss  1.98 | time 31.7849\n",
      "| epoch  17 |   801/ 1600 batches | grad_norm  4.67 | loss  0.77 | time 31.8299\n",
      "| epoch  17 |   901/ 1600 batches | grad_norm  0.31 | loss  0.03 | time 31.7847\n",
      "| epoch  17 |  1001/ 1600 batches | grad_norm  3.65 | loss  0.54 | time 31.9306\n",
      "| epoch  17 |  1101/ 1600 batches | grad_norm  0.90 | loss  0.12 | time 32.0206\n",
      "| epoch  17 |  1201/ 1600 batches | grad_norm  2.58 | loss  0.26 | time 31.8108\n",
      "| epoch  17 |  1301/ 1600 batches | grad_norm  0.03 | loss  0.00 | time 32.0043\n",
      "| epoch  17 |  1401/ 1600 batches | grad_norm  0.41 | loss  0.05 | time 31.9613\n",
      "| epoch  17 |  1501/ 1600 batches | grad_norm  0.29 | loss  0.03 | time 31.7704\n",
      "| epoch  17 | train_loss :  0.9612\n",
      "| epoch  17 | valid_loss :  0.4504 accuracy : 0.9784\n",
      "Epoch    17: reducing learning rate of group 0 to 2.5000e-03.\n",
      "| epoch  18 |     1/ 1600 batches | grad_norm  5.05 | loss  0.60 | time 0.3528\n",
      "| epoch  18 |   101/ 1600 batches | grad_norm  3.44 | loss  0.45 | time 31.7376\n",
      "| epoch  18 |   201/ 1600 batches | grad_norm  1.76 | loss  0.20 | time 31.7688\n",
      "| epoch  18 |   301/ 1600 batches | grad_norm  1.56 | loss  0.14 | time 31.9127\n",
      "| epoch  18 |   401/ 1600 batches | grad_norm  1.58 | loss  0.18 | time 31.8601\n",
      "| epoch  18 |   501/ 1600 batches | grad_norm  0.06 | loss  0.01 | time 31.8736\n",
      "| epoch  18 |   601/ 1600 batches | grad_norm  0.26 | loss  0.03 | time 31.8028\n",
      "| epoch  18 |   701/ 1600 batches | grad_norm  0.08 | loss  0.01 | time 31.8972\n",
      "| epoch  18 |   801/ 1600 batches | grad_norm  0.10 | loss  0.01 | time 31.9518\n",
      "| epoch  18 |   901/ 1600 batches | grad_norm  4.26 | loss  0.64 | time 31.9539\n",
      "| epoch  18 |  1001/ 1600 batches | grad_norm  0.12 | loss  0.02 | time 31.8896\n",
      "| epoch  18 |  1101/ 1600 batches | grad_norm  4.38 | loss  0.46 | time 31.9114\n",
      "| epoch  18 |  1201/ 1600 batches | grad_norm  0.58 | loss  0.07 | time 31.8835\n",
      "| epoch  18 |  1301/ 1600 batches | grad_norm  5.88 | loss  1.12 | time 31.8399\n",
      "| epoch  18 |  1401/ 1600 batches | grad_norm  0.21 | loss  0.02 | time 31.9913\n",
      "| epoch  18 |  1501/ 1600 batches | grad_norm  6.39 | loss  1.05 | time 31.8405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  18 | train_loss :  0.5340\n",
      "| epoch  18 | valid_loss :  0.1311 accuracy : 0.9952\n",
      "| epoch  19 |     1/ 1600 batches | grad_norm  9.98 | loss  1.28 | time 0.3510\n",
      "| epoch  19 |   101/ 1600 batches | grad_norm  0.20 | loss  0.02 | time 31.7727\n",
      "| epoch  19 |   201/ 1600 batches | grad_norm  6.97 | loss  0.60 | time 31.8542\n",
      "| epoch  19 |   301/ 1600 batches | grad_norm  0.00 | loss  0.00 | time 32.0643\n",
      "| epoch  19 |   401/ 1600 batches | grad_norm  1.03 | loss  0.13 | time 32.6137\n",
      "| epoch  19 |   501/ 1600 batches | grad_norm  2.90 | loss  0.31 | time 31.8810\n",
      "| epoch  19 |   601/ 1600 batches | grad_norm  0.89 | loss  0.09 | time 32.1978\n",
      "| epoch  19 |   701/ 1600 batches | grad_norm  0.37 | loss  0.05 | time 32.0332\n",
      "| epoch  19 |   801/ 1600 batches | grad_norm 12.95 | loss  3.02 | time 31.8935\n",
      "| epoch  19 |   901/ 1600 batches | grad_norm  0.11 | loss  0.01 | time 31.9427\n",
      "| epoch  19 |  1001/ 1600 batches | grad_norm  5.67 | loss  0.82 | time 31.9499\n",
      "| epoch  19 |  1101/ 1600 batches | grad_norm  1.48 | loss  0.16 | time 31.9466\n",
      "| epoch  19 |  1201/ 1600 batches | grad_norm 11.38 | loss  1.38 | time 31.9191\n",
      "| epoch  19 |  1301/ 1600 batches | grad_norm  0.03 | loss  0.01 | time 32.2474\n",
      "| epoch  19 |  1401/ 1600 batches | grad_norm  8.76 | loss  0.86 | time 33.0451\n",
      "| epoch  19 |  1501/ 1600 batches | grad_norm  8.62 | loss  1.37 | time 33.3504\n",
      "| epoch  19 | train_loss :  0.4510\n",
      "| epoch  19 | valid_loss :  0.1228 accuracy : 0.9954\n",
      "| epoch  20 |     1/ 1600 batches | grad_norm  0.22 | loss  0.03 | time 0.3702\n",
      "| epoch  20 |   101/ 1600 batches | grad_norm  0.03 | loss  0.00 | time 31.9690\n",
      "| epoch  20 |   201/ 1600 batches | grad_norm  0.02 | loss  0.00 | time 31.6871\n",
      "| epoch  20 |   301/ 1600 batches | grad_norm 12.72 | loss  2.12 | time 31.7305\n",
      "| epoch  20 |   401/ 1600 batches | grad_norm  9.96 | loss  1.42 | time 31.6999\n",
      "| epoch  20 |   501/ 1600 batches | grad_norm 10.78 | loss  1.02 | time 31.7087\n",
      "| epoch  20 |   601/ 1600 batches | grad_norm  0.35 | loss  0.03 | time 31.7057\n",
      "| epoch  20 |   701/ 1600 batches | grad_norm 13.18 | loss  5.62 | time 31.6850\n",
      "| epoch  20 |   801/ 1600 batches | grad_norm  5.41 | loss  0.70 | time 31.6809\n",
      "| epoch  20 |   901/ 1600 batches | grad_norm  0.08 | loss  0.01 | time 31.7000\n",
      "| epoch  20 |  1001/ 1600 batches | grad_norm  2.22 | loss  0.21 | time 31.6649\n",
      "| epoch  20 |  1101/ 1600 batches | grad_norm  1.35 | loss  0.11 | time 31.6806\n",
      "| epoch  20 |  1201/ 1600 batches | grad_norm 12.92 | loss  1.85 | time 31.6878\n",
      "| epoch  20 |  1301/ 1600 batches | grad_norm  0.72 | loss  0.06 | time 31.7136\n",
      "| epoch  20 |  1401/ 1600 batches | grad_norm  2.40 | loss  0.14 | time 31.7134\n",
      "| epoch  20 |  1501/ 1600 batches | grad_norm  8.29 | loss  0.93 | time 31.6863\n",
      "| epoch  20 | train_loss :  0.4331\n",
      "| epoch  20 | valid_loss :  0.0817 accuracy : 0.9971\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 20 # 50 # 100\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    # train_one_epoch\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion,  optimizer=optimizer, device=device, epoch=epoch)\n",
    "    print(\"| epoch {:3d} | train_loss : {:7.4f}\".format(epoch+1, train_loss))\n",
    "    \n",
    "    # validate_one_epoch\n",
    "    valid_loss, accuracy = validate_one_epoch(model, valid_loader, criterion, optimizer=None, device=device, epoch=epoch, print_output=True)\n",
    "    print(\"| epoch {:3d} | valid_loss : {:7.4f} accuracy : {:.4f}\".format(epoch+1, valid_loss, accuracy))\n",
    "    \n",
    "    # save_model\n",
    "    save_model()\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_p = []\n",
    "result_d = []\n",
    "\n",
    "for batch, (data, label) in enumerate(test_loader):\n",
    "    # data\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    hyp = model(data)\n",
    "    \n",
    "    pred = torch.argmax(hyp, dim=-1)\n",
    "    \n",
    "    sample_num = pred.size(0)\n",
    "    \n",
    "    for i in range(sample_num):\n",
    "        pred_idx = pred[i].detach().item()\n",
    "        \n",
    "        pred_plant, pred_disease = index2label[pred_idx]\n",
    "        result_p.append(pred_plant)\n",
    "        result_d.append(pred_disease)\n",
    "        \n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0   1   2\n",
      "0        0.jpg   3   5\n",
      "1        1.jpg   3  20\n",
      "2        2.jpg   4   2\n",
      "3        3.jpg   4   7\n",
      "4        4.jpg   4  11\n",
      "...        ...  ..  ..\n",
      "3992  3992.jpg  13   6\n",
      "3993  3993.jpg  13  16\n",
      "3994  3994.jpg  13  17\n",
      "3995  3995.jpg  13  18\n",
      "3996  3996.jpg  13  20\n",
      "\n",
      "[3997 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"./test/test.tsv\", sep='\\t', header=None)\n",
    "# print(data_test)\n",
    "\n",
    "data_test[1] = result_p\n",
    "data_test[2] = result_d\n",
    "\n",
    "print(data_test)\n",
    "\n",
    "data_test.to_csv(\"test_result.tsv\", index=False, header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
