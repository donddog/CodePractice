{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러개의 텐서를 입력받아 여러개의 텐서를 출력하는 모델을 알아 봅시다. 그 예로서 <br>\n",
    "yA = a * (xA * xB ) + b <br>\n",
    "yB = c * (xA + xB ) + d <br>\n",
    "의 관계를 가지는 데이터에서 a, b, c, d 를 학습으로 발견해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "a = 2; b = 1; c = 2; d = 1\n",
    "\n",
    "x_train_A = np.random.rand(1000,1) * 2 - 1\n",
    "x_train_B = np.random.rand(1000,1) * 2 - 1\n",
    "y_train_A = a * (x_train_A * x_train_B) + b\n",
    "y_train_B = c * (x_train_A + x_train_B) + d\n",
    "\n",
    "xA = layers.Input((1,), name='xA')\n",
    "xB = layers.Input((1,), name='xB')\n",
    "hA = layers.Multiply(name='mul')([xA, xB])\n",
    "hB = layers.Add(name='add')([xA, xB])\n",
    "yA = layers.Dense(1, name='yA')(hA)\n",
    "yB = layers.Dense(1, name='yB')(hB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할때 여러개의 입력과 출력이 있으면 리스트 형식으로 넣어주면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mul (Multiply)                  (None, 1)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "yA (Dense)                      (None, 1)            2           mul[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "yB (Dense)                      (None, 1)            2           add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model([xA, xB], [yA, yB])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 출력에 mse loss 를 적용하고 그것들을 더해 전체 loss 를 정의하려면 다음과 같이 컴파일하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 출력에 특정 loss 와 가중치를 지정하여 다 더해 전체 loss 를 정의하려면 다음과 같이 컴파일하면 됩니다. 전체 loss 는 하나이므로 그것을 최적화하는 optimizer 는 하나입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss={'yA': 'mse', 'yB': 'mse'}, loss_weights={'yA': 2., 'yB': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할 때처럼 여러 입력과 출력 데이터를 리스트 형식으로 넣고 학습을 합니다. 전체 loss 는 각각 loss 에 가중치를 곱해서 다 더한 값이라는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 382us/sample - loss: 6.6176 - yA_loss: 2.1755 - yB_loss: 2.2666 - val_loss: 6.5802 - val_yA_loss: 2.1423 - val_yB_loss: 2.2956\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 5.8455 - yA_loss: 1.9455 - yB_loss: 1.9546 - val_loss: 5.8298 - val_yA_loss: 1.9262 - val_yB_loss: 1.9774\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 5.1569 - yA_loss: 1.7396 - yB_loss: 1.6776 - val_loss: 5.1426 - val_yA_loss: 1.7256 - val_yB_loss: 1.6915\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 4.5490 - yA_loss: 1.5568 - yB_loss: 1.4355 - val_loss: 4.5546 - val_yA_loss: 1.5545 - val_yB_loss: 1.4456\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 4.0143 - yA_loss: 1.3954 - yB_loss: 1.2234 - val_loss: 4.0359 - val_yA_loss: 1.4044 - val_yB_loss: 1.2270\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 3.5466 - yA_loss: 1.2544 - yB_loss: 1.0378 - val_loss: 3.5737 - val_yA_loss: 1.2680 - val_yB_loss: 1.0378\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 3.1358 - yA_loss: 1.1300 - yB_loss: 0.8758 - val_loss: 3.1764 - val_yA_loss: 1.1515 - val_yB_loss: 0.8735\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.7771 - yA_loss: 1.0211 - yB_loss: 0.7349 - val_loss: 2.8247 - val_yA_loss: 1.0472 - val_yB_loss: 0.7302\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 2.4642 - yA_loss: 0.9254 - yB_loss: 0.6134 - val_loss: 2.5208 - val_yA_loss: 0.9571 - val_yB_loss: 0.6066\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 2.1932 - yA_loss: 0.8421 - yB_loss: 0.5089 - val_loss: 2.2524 - val_yA_loss: 0.8750 - val_yB_loss: 0.5023\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.9578 - yA_loss: 0.7689 - yB_loss: 0.4200 - val_loss: 2.0212 - val_yA_loss: 0.8045 - val_yB_loss: 0.4122\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.7535 - yA_loss: 0.7047 - yB_loss: 0.3442 - val_loss: 1.8214 - val_yA_loss: 0.7420 - val_yB_loss: 0.3374\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.5761 - yA_loss: 0.6479 - yB_loss: 0.2803 - val_loss: 1.6434 - val_yA_loss: 0.6852 - val_yB_loss: 0.2731\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.4211 - yA_loss: 0.5973 - yB_loss: 0.2266 - val_loss: 1.4892 - val_yA_loss: 0.6350 - val_yB_loss: 0.2192\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.2860 - yA_loss: 0.5522 - yB_loss: 0.1816 - val_loss: 1.3528 - val_yA_loss: 0.5889 - val_yB_loss: 0.1749\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.1677 - yA_loss: 0.5116 - yB_loss: 0.1446 - val_loss: 1.2314 - val_yA_loss: 0.5469 - val_yB_loss: 0.1377\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0632 - yA_loss: 0.4746 - yB_loss: 0.1140 - val_loss: 1.1251 - val_yA_loss: 0.5085 - val_yB_loss: 0.1081\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9702 - yA_loss: 0.4406 - yB_loss: 0.0890 - val_loss: 1.0308 - val_yA_loss: 0.4735 - val_yB_loss: 0.0838\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.8874 - yA_loss: 0.4093 - yB_loss: 0.0688 - val_loss: 0.9456 - val_yA_loss: 0.4406 - val_yB_loss: 0.0643\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.8133 - yA_loss: 0.3804 - yB_loss: 0.0526 - val_loss: 0.8672 - val_yA_loss: 0.4093 - val_yB_loss: 0.0487\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.7456 - yA_loss: 0.3530 - yB_loss: 0.0396 - val_loss: 0.7979 - val_yA_loss: 0.3807 - val_yB_loss: 0.0365\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6848 - yA_loss: 0.3276 - yB_loss: 0.0295 - val_loss: 0.7323 - val_yA_loss: 0.3527 - val_yB_loss: 0.0269\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6288 - yA_loss: 0.3036 - yB_loss: 0.0217 - val_loss: 0.6736 - val_yA_loss: 0.3271 - val_yB_loss: 0.0195\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5775 - yA_loss: 0.2809 - yB_loss: 0.0156 - val_loss: 0.6209 - val_yA_loss: 0.3035 - val_yB_loss: 0.0140\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5302 - yA_loss: 0.2596 - yB_loss: 0.0111 - val_loss: 0.5691 - val_yA_loss: 0.2796 - val_yB_loss: 0.0099\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.4864 - yA_loss: 0.2393 - yB_loss: 0.0078 - val_loss: 0.5213 - val_yA_loss: 0.2572 - val_yB_loss: 0.0068\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.4456 - yA_loss: 0.2202 - yB_loss: 0.0053 - val_loss: 0.4782 - val_yA_loss: 0.2368 - val_yB_loss: 0.0046\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.4078 - yA_loss: 0.2021 - yB_loss: 0.0036 - val_loss: 0.4374 - val_yA_loss: 0.2172 - val_yB_loss: 0.0030\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.3725 - yA_loss: 0.1851 - yB_loss: 0.0023 - val_loss: 0.3997 - val_yA_loss: 0.1988 - val_yB_loss: 0.0020\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.3397 - yA_loss: 0.1691 - yB_loss: 0.0015 - val_loss: 0.3637 - val_yA_loss: 0.1812 - val_yB_loss: 0.0013\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.3091 - yA_loss: 0.1541 - yB_loss: 9.4174e-04 - val_loss: 0.3308 - val_yA_loss: 0.1650 - val_yB_loss: 7.6698e-04\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.2805 - yA_loss: 0.1400 - yB_loss: 5.7421e-04 - val_loss: 0.2993 - val_yA_loss: 0.1494 - val_yB_loss: 4.6217e-04\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.2539 - yA_loss: 0.1268 - yB_loss: 3.4110e-04 - val_loss: 0.2708 - val_yA_loss: 0.1353 - val_yB_loss: 2.7328e-04\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.2292 - yA_loss: 0.1145 - yB_loss: 1.9766e-04 - val_loss: 0.2438 - val_yA_loss: 0.1218 - val_yB_loss: 1.5369e-04\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.2061 - yA_loss: 0.1030 - yB_loss: 1.1072e-04 - val_loss: 0.2192 - val_yA_loss: 0.1096 - val_yB_loss: 8.4788e-05\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1849 - yA_loss: 0.0924 - yB_loss: 6.0209e-05 - val_loss: 0.1959 - val_yA_loss: 0.0979 - val_yB_loss: 4.5346e-05\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.1651 - yA_loss: 0.0825 - yB_loss: 3.1657e-05 - val_loss: 0.1751 - val_yA_loss: 0.0876 - val_yB_loss: 2.3453e-05\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.1470 - yA_loss: 0.0735 - yB_loss: 1.6087e-05 - val_loss: 0.1552 - val_yA_loss: 0.0776 - val_yB_loss: 1.1644e-05\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.1303 - yA_loss: 0.0651 - yB_loss: 7.8895e-06 - val_loss: 0.1374 - val_yA_loss: 0.0687 - val_yB_loss: 5.5464e-06\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.1150 - yA_loss: 0.0575 - yB_loss: 3.6988e-06 - val_loss: 0.1211 - val_yA_loss: 0.0605 - val_yB_loss: 2.5976e-06\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1010 - yA_loss: 0.0505 - yB_loss: 1.6789e-06 - val_loss: 0.1059 - val_yA_loss: 0.0530 - val_yB_loss: 1.1261e-06\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.0883 - yA_loss: 0.0441 - yB_loss: 7.2285e-07 - val_loss: 0.0926 - val_yA_loss: 0.0463 - val_yB_loss: 4.8268e-07\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.0768 - yA_loss: 0.0384 - yB_loss: 2.9964e-07 - val_loss: 0.0800 - val_yA_loss: 0.0400 - val_yB_loss: 1.9063e-07\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 102us/sample - loss: 0.0664 - yA_loss: 0.0332 - yB_loss: 1.1755e-07 - val_loss: 0.0691 - val_yA_loss: 0.0345 - val_yB_loss: 7.3436e-08\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.0571 - yA_loss: 0.0286 - yB_loss: 4.4083e-08 - val_loss: 0.0592 - val_yA_loss: 0.0296 - val_yB_loss: 2.7141e-08\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0488 - yA_loss: 0.0244 - yB_loss: 1.5743e-08 - val_loss: 0.0505 - val_yA_loss: 0.0252 - val_yB_loss: 9.2315e-09\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.0414 - yA_loss: 0.0207 - yB_loss: 5.2765e-09 - val_loss: 0.0427 - val_yA_loss: 0.0213 - val_yB_loss: 2.9577e-09\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.0349 - yA_loss: 0.0175 - yB_loss: 1.6500e-09 - val_loss: 0.0359 - val_yA_loss: 0.0180 - val_yB_loss: 8.9424e-10\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.0292 - yA_loss: 0.0146 - yB_loss: 4.9592e-10 - val_loss: 0.0298 - val_yA_loss: 0.0149 - val_yB_loss: 3.0070e-10\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0243 - yA_loss: 0.0121 - yB_loss: 1.5421e-10 - val_loss: 0.0248 - val_yA_loss: 0.0124 - val_yB_loss: 6.3969e-11\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.0200 - yA_loss: 0.0100 - yB_loss: 4.9063e-11 - val_loss: 0.0202 - val_yA_loss: 0.0101 - val_yB_loss: 4.1917e-11\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.0163 - yA_loss: 0.0082 - yB_loss: 3.8366e-11 - val_loss: 0.0164 - val_yA_loss: 0.0082 - val_yB_loss: 3.9591e-11\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0132 - yA_loss: 0.0066 - yB_loss: 3.5913e-11 - val_loss: 0.0132 - val_yA_loss: 0.0066 - val_yB_loss: 3.5754e-11\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0106 - yA_loss: 0.0053 - yB_loss: 2.8139e-11 - val_loss: 0.0105 - val_yA_loss: 0.0053 - val_yB_loss: 2.8886e-11\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.0084 - yA_loss: 0.0042 - yB_loss: 2.7031e-11 - val_loss: 0.0084 - val_yA_loss: 0.0042 - val_yB_loss: 2.8616e-11\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.0066 - yA_loss: 0.0033 - yB_loss: 2.4076e-11 - val_loss: 0.0064 - val_yA_loss: 0.0032 - val_yB_loss: 2.3185e-11\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.0051 - yA_loss: 0.0025 - yB_loss: 2.1693e-11 - val_loss: 0.0050 - val_yA_loss: 0.0025 - val_yB_loss: 2.2711e-11\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.0039 - yA_loss: 0.0019 - yB_loss: 2.0810e-11 - val_loss: 0.0037 - val_yA_loss: 0.0019 - val_yB_loss: 2.1695e-11\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.0029 - yA_loss: 0.0015 - yB_loss: 2.0092e-11 - val_loss: 0.0028 - val_yA_loss: 0.0014 - val_yB_loss: 1.9948e-11\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.0022 - yA_loss: 0.0011 - yB_loss: 1.7265e-11 - val_loss: 0.0021 - val_yA_loss: 0.0010 - val_yB_loss: 1.8271e-11\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.0016 - yA_loss: 7.9627e-04 - yB_loss: 1.5719e-11 - val_loss: 0.0015 - val_yA_loss: 7.5567e-04 - val_yB_loss: 1.5770e-11\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.0011 - yA_loss: 5.7344e-04 - yB_loss: 1.4670e-11 - val_loss: 0.0011 - val_yA_loss: 5.4182e-04 - val_yB_loss: 1.4266e-11\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 8.1411e-04 - yA_loss: 4.0705e-04 - yB_loss: 1.2359e-11 - val_loss: 7.5711e-04 - val_yA_loss: 3.7855e-04 - val_yB_loss: 1.2148e-11\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 5.6661e-04 - yA_loss: 2.8331e-04 - yB_loss: 1.0885e-11 - val_loss: 5.2318e-04 - val_yA_loss: 2.6159e-04 - val_yB_loss: 1.0113e-11\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 3.8758e-04 - yA_loss: 1.9379e-04 - yB_loss: 9.5610e-12 - val_loss: 3.5359e-04 - val_yA_loss: 1.7680e-04 - val_yB_loss: 1.0044e-11\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 2.5969e-04 - yA_loss: 1.2985e-04 - yB_loss: 8.9079e-12 - val_loss: 2.3499e-04 - val_yA_loss: 1.1750e-04 - val_yB_loss: 8.7061e-12\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.7081e-04 - yA_loss: 8.5404e-05 - yB_loss: 7.1359e-12 - val_loss: 1.5275e-04 - val_yA_loss: 7.6376e-05 - val_yB_loss: 6.8241e-12\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.0950e-04 - yA_loss: 5.4749e-05 - yB_loss: 6.5057e-12 - val_loss: 9.6523e-05 - val_yA_loss: 4.8261e-05 - val_yB_loss: 6.8241e-12\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 6.8677e-05 - yA_loss: 3.4338e-05 - yB_loss: 6.5024e-12 - val_loss: 5.9983e-05 - val_yA_loss: 2.9992e-05 - val_yB_loss: 6.4135e-12\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 4.2093e-05 - yA_loss: 2.1046e-05 - yB_loss: 5.4436e-12 - val_loss: 3.6086e-05 - val_yA_loss: 1.8043e-05 - val_yB_loss: 4.1260e-12\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.5117e-05 - yA_loss: 1.2559e-05 - yB_loss: 3.8577e-12 - val_loss: 2.1243e-05 - val_yA_loss: 1.0622e-05 - val_yB_loss: 4.1260e-12\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.4601e-05 - yA_loss: 7.3007e-06 - yB_loss: 3.8577e-12 - val_loss: 1.2070e-05 - val_yA_loss: 6.0352e-06 - val_yB_loss: 4.1260e-12\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 8.2327e-06 - yA_loss: 4.1163e-06 - yB_loss: 3.8577e-12 - val_loss: 6.8096e-06 - val_yA_loss: 3.4048e-06 - val_yB_loss: 4.1260e-12\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 4.5302e-06 - yA_loss: 2.2651e-06 - yB_loss: 3.6811e-12 - val_loss: 3.5923e-06 - val_yA_loss: 1.7962e-06 - val_yB_loss: 3.6681e-12\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.3910e-06 - yA_loss: 1.1955e-06 - yB_loss: 3.2213e-12 - val_loss: 1.8868e-06 - val_yA_loss: 9.4341e-07 - val_yB_loss: 3.3378e-12\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.2271e-06 - yA_loss: 6.1353e-07 - yB_loss: 3.1402e-12 - val_loss: 9.5570e-07 - val_yA_loss: 4.7785e-07 - val_yB_loss: 3.3378e-12\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 6.1023e-07 - yA_loss: 3.0512e-07 - yB_loss: 3.0128e-12 - val_loss: 4.6041e-07 - val_yA_loss: 2.3021e-07 - val_yB_loss: 2.9717e-12\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 2.9159e-07 - yA_loss: 1.4579e-07 - yB_loss: 2.0412e-12 - val_loss: 2.1305e-07 - val_yA_loss: 1.0652e-07 - val_yB_loss: 2.0355e-12\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.3289e-07 - yA_loss: 6.6445e-08 - yB_loss: 1.9457e-12 - val_loss: 9.7805e-08 - val_yA_loss: 4.8902e-08 - val_yB_loss: 2.0355e-12\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 5.8655e-08 - yA_loss: 2.9327e-08 - yB_loss: 1.7990e-12 - val_loss: 4.1291e-08 - val_yA_loss: 2.0645e-08 - val_yB_loss: 1.7535e-12\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.4619e-08 - yA_loss: 1.2309e-08 - yB_loss: 1.6881e-12 - val_loss: 1.7099e-08 - val_yA_loss: 8.5485e-09 - val_yB_loss: 1.7535e-12\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 9.9409e-09 - yA_loss: 4.9696e-09 - yB_loss: 1.6881e-12 - val_loss: 6.5092e-09 - val_yA_loss: 3.2537e-09 - val_yB_loss: 1.7535e-12\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 3.7744e-09 - yA_loss: 1.8863e-09 - yB_loss: 1.6667e-12 - val_loss: 2.5239e-09 - val_yA_loss: 1.2612e-09 - val_yB_loss: 1.4898e-12\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 105us/sample - loss: 1.3962e-09 - yA_loss: 6.9743e-10 - yB_loss: 1.3884e-12 - val_loss: 8.8139e-10 - val_yA_loss: 4.3997e-10 - val_yB_loss: 1.4362e-12\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 4.8025e-10 - yA_loss: 2.3954e-10 - yB_loss: 1.1775e-12 - val_loss: 2.7994e-10 - val_yA_loss: 1.3948e-10 - val_yB_loss: 9.8954e-13\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.5871e-10 - yA_loss: 7.8880e-11 - yB_loss: 9.5097e-13 - val_loss: 1.0666e-10 - val_yA_loss: 5.2837e-11 - val_yB_loss: 9.8954e-13\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 5.4276e-11 - yA_loss: 2.6663e-11 - yB_loss: 9.5097e-13 - val_loss: 2.4590e-11 - val_yA_loss: 1.1800e-11 - val_yB_loss: 9.8954e-13\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.7490e-11 - yA_loss: 8.2897e-12 - yB_loss: 9.1047e-13 - val_loss: 1.2522e-11 - val_yA_loss: 5.8479e-12 - val_yB_loss: 8.2649e-13\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.1245e-11 - yA_loss: 5.2280e-12 - yB_loss: 7.8857e-13 - val_loss: 1.2522e-11 - val_yA_loss: 5.8479e-12 - val_yB_loss: 8.2649e-13\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.1245e-11 - yA_loss: 5.2280e-12 - yB_loss: 7.8857e-13 - val_loss: 1.2522e-11 - val_yA_loss: 5.8479e-12 - val_yB_loss: 8.2649e-13\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.1082e-11 - yA_loss: 5.1470e-12 - yB_loss: 7.8793e-13 - val_loss: 1.1884e-11 - val_yA_loss: 5.7027e-12 - val_yB_loss: 4.7895e-13\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 9.5975e-12 - yA_loss: 4.5554e-12 - yB_loss: 4.8669e-13 - val_loss: 1.0257e-11 - val_yA_loss: 4.8888e-12 - val_yB_loss: 4.7895e-13\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 8.8852e-12 - yA_loss: 4.1993e-12 - yB_loss: 4.8669e-13 - val_loss: 7.6743e-12 - val_yA_loss: 3.5977e-12 - val_yB_loss: 4.7895e-13\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 6.8854e-12 - yA_loss: 3.1993e-12 - yB_loss: 4.8669e-13 - val_loss: 7.6743e-12 - val_yA_loss: 3.5977e-12 - val_yB_loss: 4.7895e-13\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 6.8083e-12 - yA_loss: 3.1608e-12 - yB_loss: 4.8669e-13 - val_loss: 7.4015e-12 - val_yA_loss: 3.4613e-12 - val_yB_loss: 4.7895e-13\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 6.5007e-12 - yA_loss: 3.0741e-12 - yB_loss: 3.5248e-13 - val_loss: 7.2680e-12 - val_yA_loss: 3.4613e-12 - val_yB_loss: 3.4542e-13\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 5.7922e-12 - yA_loss: 2.7222e-12 - yB_loss: 3.4782e-13 - val_loss: 6.4572e-12 - val_yA_loss: 3.0559e-12 - val_yB_loss: 3.4542e-13\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 4.8835e-12 - yA_loss: 2.2678e-12 - yB_loss: 3.4782e-13 - val_loss: 5.4088e-12 - val_yA_loss: 2.5317e-12 - val_yB_loss: 3.4542e-13\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 4.8096e-12 - yA_loss: 2.2393e-12 - yB_loss: 3.3088e-13 - val_loss: 5.2222e-12 - val_yA_loss: 2.5317e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 4.2767e-12 - yA_loss: 2.0590e-12 - yB_loss: 1.5869e-13 - val_loss: 4.5247e-12 - val_yA_loss: 2.1829e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 3.7934e-12 - yA_loss: 1.8174e-12 - yB_loss: 1.5869e-13 - val_loss: 3.9058e-12 - val_yA_loss: 1.8735e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 3.4990e-12 - yA_loss: 1.6702e-12 - yB_loss: 1.5869e-13 - val_loss: 3.9058e-12 - val_yA_loss: 1.8735e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 3.0576e-12 - yA_loss: 1.4495e-12 - yB_loss: 1.5869e-13 - val_loss: 3.1199e-12 - val_yA_loss: 1.4805e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.7127e-12 - yA_loss: 1.2770e-12 - yB_loss: 1.5869e-13 - val_loss: 2.2225e-12 - val_yA_loss: 1.0318e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.9683e-12 - yA_loss: 9.0482e-13 - yB_loss: 1.5869e-13 - val_loss: 2.2225e-12 - val_yA_loss: 1.0318e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.9683e-12 - yA_loss: 9.0482e-13 - yB_loss: 1.5869e-13 - val_loss: 2.2225e-12 - val_yA_loss: 1.0318e-12 - val_yB_loss: 1.5884e-13\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.9683e-12 - yA_loss: 9.0482e-13 - yB_loss: 1.5869e-13 - val_loss: 2.1607e-12 - val_yA_loss: 1.0318e-12 - val_yB_loss: 9.7052e-14\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.8366e-12 - yA_loss: 8.6745e-13 - yB_loss: 1.0165e-13 - val_loss: 1.8173e-12 - val_yA_loss: 8.6013e-13 - val_yB_loss: 9.7052e-14\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.4325e-12 - yA_loss: 6.6544e-13 - yB_loss: 1.0165e-13 - val_loss: 1.4412e-12 - val_yA_loss: 6.7208e-13 - val_yB_loss: 9.7052e-14\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.2882e-12 - yA_loss: 5.9327e-13 - yB_loss: 1.0165e-13 - val_loss: 1.4412e-12 - val_yA_loss: 6.7208e-13 - val_yB_loss: 9.7052e-14\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.2559e-12 - yA_loss: 5.9327e-13 - yB_loss: 6.9365e-14 - val_loss: 1.3852e-12 - val_yA_loss: 6.7208e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 9.4852e-13 - yA_loss: 4.5177e-13 - yB_loss: 4.4977e-14 - val_loss: 9.9958e-13 - val_yA_loss: 4.7927e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 8.9432e-13 - yA_loss: 4.2467e-13 - yB_loss: 4.4977e-14 - val_loss: 9.9958e-13 - val_yA_loss: 4.7927e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 8.9432e-13 - yA_loss: 4.2467e-13 - yB_loss: 4.4977e-14 - val_loss: 9.9958e-13 - val_yA_loss: 4.7927e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 8.3545e-13 - yA_loss: 3.9524e-13 - yB_loss: 4.4977e-14 - val_loss: 8.9582e-13 - val_yA_loss: 4.2739e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 8.0633e-13 - yA_loss: 3.8067e-13 - yB_loss: 4.4977e-14 - val_loss: 8.9582e-13 - val_yA_loss: 4.2739e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 7.5571e-13 - yA_loss: 3.5537e-13 - yB_loss: 4.4977e-14 - val_loss: 8.1331e-13 - val_yA_loss: 3.8614e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 7.2158e-13 - yA_loss: 3.3830e-13 - yB_loss: 4.4977e-14 - val_loss: 8.1331e-13 - val_yA_loss: 3.8614e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 6.6808e-13 - yA_loss: 3.1155e-13 - yB_loss: 4.4977e-14 - val_loss: 7.0331e-13 - val_yA_loss: 3.3114e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 6.2299e-13 - yA_loss: 2.8900e-13 - yB_loss: 4.4977e-14 - val_loss: 6.2667e-13 - val_yA_loss: 2.9282e-13 - val_yB_loss: 4.1028e-14\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.1184e-13 - yA_loss: 2.4400e-13 - yB_loss: 2.3842e-14 - val_loss: 4.9752e-13 - val_yA_loss: 2.4553e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 3.6803e-13 - yA_loss: 1.8060e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.2583e-13 - yA_loss: 1.0950e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.2583e-13 - yA_loss: 1.0950e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.2583e-13 - yA_loss: 1.0950e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.2583e-13 - yA_loss: 1.0950e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.2583e-13 - yA_loss: 1.0950e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.2583e-13 - yA_loss: 1.0950e-13 - yB_loss: 6.8314e-15 - val_loss: 2.4070e-13 - val_yA_loss: 1.1712e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.2772e-13 - yA_loss: 1.1044e-13 - yB_loss: 6.8314e-15 - val_loss: 2.1572e-13 - val_yA_loss: 1.0462e-13 - val_yB_loss: 6.4662e-15\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.8289e-13 - yA_loss: 8.8029e-14 - yB_loss: 6.8314e-15 - val_loss: 1.7485e-13 - val_yA_loss: 8.4190e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.5632e-13 - yA_loss: 7.4746e-14 - yB_loss: 6.8314e-15 - val_loss: 1.7485e-13 - val_yA_loss: 8.4190e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.3592e-13 - yA_loss: 6.4545e-14 - yB_loss: 6.8314e-15 - val_loss: 1.3253e-13 - val_yA_loss: 6.3032e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.1767e-13 - yA_loss: 5.5421e-14 - yB_loss: 6.8314e-15 - val_loss: 1.3253e-13 - val_yA_loss: 6.3032e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.1767e-13 - yA_loss: 5.5421e-14 - yB_loss: 6.8314e-15 - val_loss: 1.3253e-13 - val_yA_loss: 6.3032e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.0397e-13 - yA_loss: 4.8570e-14 - yB_loss: 6.8314e-15 - val_loss: 9.9807e-14 - val_yA_loss: 4.6670e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 9.0389e-14 - yA_loss: 4.1779e-14 - yB_loss: 6.8314e-15 - val_loss: 9.9807e-14 - val_yA_loss: 4.6670e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 9.0389e-14 - yA_loss: 4.1779e-14 - yB_loss: 6.8314e-15 - val_loss: 9.9807e-14 - val_yA_loss: 4.6670e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 8.4708e-14 - yA_loss: 3.8938e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.9987e-14 - yA_loss: 2.6578e-14 - yB_loss: 6.8314e-15 - val_loss: 6.5591e-14 - val_yA_loss: 2.9562e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.5913e-14 - yA_loss: 2.4541e-14 - yB_loss: 6.8314e-15 - val_loss: 4.8739e-14 - val_yA_loss: 2.1136e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 4.3622e-14 - yA_loss: 1.8271e-14 - yB_loss: 7.0812e-15 - val_loss: 4.8739e-14 - val_yA_loss: 2.1136e-14 - val_yB_loss: 6.4662e-15\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.7991e-14 - yA_loss: 1.0580e-14 - yB_loss: 6.8314e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.4264e-14 - yA_loss: 8.4533e-15 - yB_loss: 7.3576e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.3738e-14 - yA_loss: 8.4533e-15 - yB_loss: 6.8314e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.3849e-14 - yA_loss: 8.4533e-15 - yB_loss: 6.9424e-15 - val_loss: 2.4731e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 8.2457e-15\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.3942e-14 - yA_loss: 8.4533e-15 - yB_loss: 7.0357e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.4276e-14 - yA_loss: 8.4533e-15 - yB_loss: 7.3699e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.4116e-14 - yA_loss: 8.4533e-15 - yB_loss: 7.2095e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 2.3738e-14 - yA_loss: 8.4533e-15 - yB_loss: 6.8314e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.3919e-14 - yA_loss: 8.4533e-15 - yB_loss: 7.0124e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.3999e-14 - yA_loss: 8.4533e-15 - yB_loss: 7.0929e-15 - val_loss: 2.2952e-14 - val_yA_loss: 8.2429e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.2485e-14 - yA_loss: 7.7435e-15 - yB_loss: 6.9979e-15 - val_loss: 2.0385e-14 - val_yA_loss: 6.0696e-15 - val_yB_loss: 8.2457e-15\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.7755e-14 - yA_loss: 5.4350e-15 - yB_loss: 6.8847e-15 - val_loss: 1.8605e-14 - val_yA_loss: 6.0696e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.7775e-14 - yA_loss: 5.4350e-15 - yB_loss: 6.9052e-15 - val_loss: 1.8605e-14 - val_yA_loss: 6.0696e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.9946e-14 - yA_loss: 5.4350e-15 - yB_loss: 9.0763e-15 - val_loss: 1.8605e-14 - val_yA_loss: 6.0696e-15 - val_yB_loss: 6.4662e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.8100e-14 - yA_loss: 5.4350e-15 - yB_loss: 7.2300e-15 - val_loss: 2.0385e-14 - val_yA_loss: 6.0696e-15 - val_yB_loss: 8.2457e-15\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 1.8015e-14 - yA_loss: 5.4350e-15 - yB_loss: 7.1445e-15 - val_loss: 1.8605e-14 - val_yA_loss: 6.0696e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.6276e-14 - yA_loss: 2.6804e-15 - yB_loss: 1.0915e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.2079e-14 - yA_loss: 2.3456e-15 - yB_loss: 7.3882e-15 - val_loss: 1.3889e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 8.2457e-15\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.6940e-14 - yA_loss: 2.3456e-15 - yB_loss: 1.2248e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.6082e-14 - yA_loss: 2.3456e-15 - yB_loss: 2.1391e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.2301e-14 - yA_loss: 2.4833e-15 - yB_loss: 1.7335e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.9657e-14 - yA_loss: 2.5766e-15 - yB_loss: 1.4503e-14 - val_loss: 1.3889e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 8.2457e-15\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.9703e-14 - yA_loss: 2.3456e-15 - yB_loss: 1.5011e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.7112e-14 - yA_loss: 2.3456e-15 - yB_loss: 1.2420e-14 - val_loss: 2.9263e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 2.3619e-14\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.8554e-14 - yA_loss: 2.5377e-15 - yB_loss: 2.3478e-14 - val_loss: 2.4515e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.8872e-14\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 6.9597e-14 - yA_loss: 2.3456e-15 - yB_loss: 6.4906e-14 - val_loss: 1.0769e-13 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.0205e-13\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.8554e-14 - yA_loss: 2.3456e-15 - yB_loss: 2.3862e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.8849e-14 - yA_loss: 2.5566e-15 - yB_loss: 1.3736e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.9311e-14 - yA_loss: 2.6787e-15 - yB_loss: 1.3954e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 2.4749e-14 - yA_loss: 2.4819e-15 - yB_loss: 1.9785e-14 - val_loss: 1.2109e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 6.4662e-15\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 3.5163e-14 - yA_loss: 2.4789e-15 - yB_loss: 3.0205e-14 - val_loss: 1.8533e-13 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.7968e-13\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 5.6852e-14 - yA_loss: 2.5988e-15 - yB_loss: 5.1655e-14 - val_loss: 1.9406e-13 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.8842e-13\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.9088e-13 - yA_loss: 2.5399e-15 - yB_loss: 1.8580e-13 - val_loss: 2.4515e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.8872e-14\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 6.4099e-14 - yA_loss: 3.0329e-15 - yB_loss: 5.8034e-14 - val_loss: 1.3889e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 8.2457e-15\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 7.5065e-14 - yA_loss: 2.5749e-15 - yB_loss: 6.9915e-14 - val_loss: 4.5905e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 4.0262e-14\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 4.1427e-14 - yA_loss: 2.5108e-15 - yB_loss: 3.6406e-14 - val_loss: 1.9406e-13 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.8842e-13\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.1048e-13 - yA_loss: 2.3967e-15 - yB_loss: 1.0568e-13 - val_loss: 2.4515e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.8872e-14\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 8.3070e-14 - yA_loss: 1.2122e-14 - yB_loss: 5.8826e-14 - val_loss: 3.5106e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 2.9462e-14\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.2956e-13 - yA_loss: 5.3847e-15 - yB_loss: 1.1879e-13 - val_loss: 4.7340e-13 - val_yA_loss: 1.6123e-14 - val_yB_loss: 4.4115e-13\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.7403e-13 - yA_loss: 3.8980e-15 - yB_loss: 1.6623e-13 - val_loss: 5.9118e-14 - val_yA_loss: 2.8216e-15 - val_yB_loss: 5.3475e-14\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.4580e-13 - yA_loss: 4.4223e-15 - yB_loss: 1.3695e-13 - val_loss: 1.9108e-13 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.8544e-13\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 3.0097e-12 - yA_loss: 7.8079e-15 - yB_loss: 2.9941e-12 - val_loss: 1.9129e-12 - val_yA_loss: 6.0696e-15 - val_yB_loss: 1.9007e-12\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.8439e-12 - yA_loss: 5.0982e-15 - yB_loss: 1.8337e-12 - val_loss: 1.3192e-12 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.3135e-12\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.1492e-12 - yA_loss: 9.9523e-15 - yB_loss: 5.1293e-12 - val_loss: 1.7529e-11 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.7523e-11\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 4.9477e-12 - yA_loss: 1.4515e-14 - yB_loss: 4.9186e-12 - val_loss: 1.0318e-12 - val_yA_loss: 2.0124e-14 - val_yB_loss: 9.9160e-13\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.1902e-09 - yA_loss: 8.6849e-15 - yB_loss: 2.1902e-09 - val_loss: 2.1800e-09 - val_yA_loss: 1.6663e-14 - val_yB_loss: 2.1800e-09\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 4.1672e-09 - yA_loss: 1.0263e-14 - yB_loss: 4.1672e-09 - val_loss: 1.1636e-09 - val_yA_loss: 2.8216e-15 - val_yB_loss: 1.1636e-09\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.9896e-10 - yA_loss: 5.0495e-15 - yB_loss: 2.9895e-10 - val_loss: 2.4500e-10 - val_yA_loss: 1.6663e-14 - val_yB_loss: 2.4496e-10\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 9.8451e-10 - yA_loss: 1.8565e-14 - yB_loss: 9.8447e-10 - val_loss: 7.3309e-10 - val_yA_loss: 2.8216e-15 - val_yB_loss: 7.3309e-10\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.6132e-10 - yA_loss: 9.3589e-15 - yB_loss: 2.6130e-10 - val_loss: 4.1089e-11 - val_yA_loss: 2.8216e-15 - val_yB_loss: 4.1084e-11\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 8.6340e-11 - yA_loss: 8.8713e-15 - yB_loss: 8.6322e-11 - val_loss: 8.2568e-11 - val_yA_loss: 6.0696e-15 - val_yB_loss: 8.2556e-11\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 3.2047e-08 - yA_loss: 7.0524e-14 - yB_loss: 3.2047e-08 - val_loss: 3.0742e-07 - val_yA_loss: 2.2302e-14 - val_yB_loss: 3.0742e-07\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.4542e-07 - yA_loss: 1.9226e-14 - yB_loss: 1.4542e-07 - val_loss: 1.2989e-07 - val_yA_loss: 1.6663e-14 - val_yB_loss: 1.2989e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yA_weights\n",
      "[array([[2.]], dtype=float32), array([1.0000001], dtype=float32)]\n",
      "yB_weights\n",
      "[array([[2.0003796]], dtype=float32), array([0.9998316], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([x_train_A, x_train_B], [y_train_A, y_train_B], batch_size=8, epochs=200, validation_split=0.2)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "print('yA_weights'); print(model.get_layer('yA').get_weights())\n",
    "print('yB_weights'); print(model.get_layer('yB').get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yA = a * xA + b <br>\n",
    "yB = c * exp(yA) + d * xB + e <br>\n",
    "를 만족하는 데이터에서 a, b, c, d, e 를 발견합시다. 초기 웨이트에 따라 학습이 잘 안될 수도 있으니 여러번 실행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yA (Dense)                      (None, 1)            2           xA[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "exp (Lambda)                    (None, 1)            0           yA[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 2)            0           exp[0][0]                        \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "yB (Dense)                      (None, 1)            3           concat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "a = 2; b = 1; c = 2; d = 3; e = 1\n",
    "\n",
    "x_train_A = np.random.rand(1000,1) * 2 - 1\n",
    "x_train_B = np.random.rand(1000,1) * 2 - 1\n",
    "y_train_A = a * x_train_A + b\n",
    "y_train_B = c * np.exp(y_train_A) + d * x_train_B + e\n",
    "\n",
    "xA = layers.Input((1,), name='xA')\n",
    "xB = layers.Input((1,), name='xB')\n",
    "yA = layers.Dense(1, name='yA')(xA)\n",
    "h1 = layers.Lambda(lambda x: K.exp(x), name='exp')(yA)\n",
    "h2 = layers.Concatenate(name='concat')([h1, xB])\n",
    "yB = layers.Dense(1, name='yB')(h2)\n",
    "\n",
    "model = models.Model([xA, xB], [yA, yB])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 373us/sample - loss: 233.9737 - yA_loss: 4.3374 - yB_loss: 229.6364 - val_loss: 253.2549 - val_yA_loss: 4.6034 - val_yB_loss: 248.6515\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 228.9015 - yA_loss: 4.4518 - yB_loss: 224.4497 - val_loss: 248.6156 - val_yA_loss: 4.7210 - val_yB_loss: 243.8946\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 224.6167 - yA_loss: 4.5601 - yB_loss: 220.0566 - val_loss: 244.5928 - val_yA_loss: 4.8321 - val_yB_loss: 239.7607\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 220.8592 - yA_loss: 4.6538 - yB_loss: 216.2054 - val_loss: 241.0490 - val_yA_loss: 4.9189 - val_yB_loss: 236.1301\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 217.4796 - yA_loss: 4.7256 - yB_loss: 212.7541 - val_loss: 237.7714 - val_yA_loss: 4.9812 - val_yB_loss: 232.7903\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 214.3557 - yA_loss: 4.7667 - yB_loss: 209.5891 - val_loss: 234.7290 - val_yA_loss: 5.0064 - val_yB_loss: 229.7226\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 211.4107 - yA_loss: 4.7730 - yB_loss: 206.6376 - val_loss: 231.8149 - val_yA_loss: 4.9924 - val_yB_loss: 226.8226\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 208.5869 - yA_loss: 4.7391 - yB_loss: 203.8478 - val_loss: 229.0056 - val_yA_loss: 4.9353 - val_yB_loss: 224.0703\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 205.8274 - yA_loss: 4.6627 - yB_loss: 201.1648 - val_loss: 226.2250 - val_yA_loss: 4.8288 - val_yB_loss: 221.3962\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 203.0835 - yA_loss: 4.5361 - yB_loss: 198.5475 - val_loss: 223.4400 - val_yA_loss: 4.6701 - val_yB_loss: 218.7699\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 200.3119 - yA_loss: 4.3580 - yB_loss: 195.9538 - val_loss: 220.5614 - val_yA_loss: 4.4506 - val_yB_loss: 216.1108\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 197.4438 - yA_loss: 4.1202 - yB_loss: 193.3235 - val_loss: 217.5617 - val_yA_loss: 4.1697 - val_yB_loss: 213.3920\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 194.3795 - yA_loss: 3.8170 - yB_loss: 190.5625 - val_loss: 214.3144 - val_yA_loss: 3.8172 - val_yB_loss: 210.4972\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 190.9851 - yA_loss: 3.4423 - yB_loss: 187.5428 - val_loss: 210.5174 - val_yA_loss: 3.3797 - val_yB_loss: 207.1377\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 186.9616 - yA_loss: 2.9889 - yB_loss: 183.9727 - val_loss: 205.8632 - val_yA_loss: 2.8620 - val_yB_loss: 203.0012\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 181.7637 - yA_loss: 2.4584 - yB_loss: 179.3054 - val_loss: 199.5146 - val_yA_loss: 2.2745 - val_yB_loss: 197.2401\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 174.2536 - yA_loss: 1.8747 - yB_loss: 172.3789 - val_loss: 189.6739 - val_yA_loss: 1.6477 - val_yB_loss: 188.0261\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 162.1905 - yA_loss: 1.2859 - yB_loss: 160.9046 - val_loss: 173.4200 - val_yA_loss: 1.0726 - val_yB_loss: 172.3474\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 142.2254 - yA_loss: 0.7987 - yB_loss: 141.4268 - val_loss: 144.7551 - val_yA_loss: 0.6107 - val_yB_loss: 144.1444\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 110.2008 - yA_loss: 0.4588 - yB_loss: 109.7420 - val_loss: 103.9636 - val_yA_loss: 0.3590 - val_yB_loss: 103.6046\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 69.4730 - yA_loss: 0.3069 - yB_loss: 69.1661 - val_loss: 55.6935 - val_yA_loss: 0.2758 - val_yB_loss: 55.4176\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 31.4340 - yA_loss: 0.2904 - yB_loss: 31.1435 - val_loss: 20.9096 - val_yA_loss: 0.2987 - val_yB_loss: 20.6109\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 11.6663 - yA_loss: 0.3281 - yB_loss: 11.3382 - val_loss: 8.9606 - val_yA_loss: 0.3299 - val_yB_loss: 8.6307\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 6.7006 - yA_loss: 0.3398 - yB_loss: 6.3607 - val_loss: 6.6113 - val_yA_loss: 0.3272 - val_yB_loss: 6.2841\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 5.6313 - yA_loss: 0.3272 - yB_loss: 5.3040 - val_loss: 5.6976 - val_yA_loss: 0.3079 - val_yB_loss: 5.3897\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 4.9375 - yA_loss: 0.3049 - yB_loss: 4.6326 - val_loss: 5.0085 - val_yA_loss: 0.2854 - val_yB_loss: 4.7231\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 4.3438 - yA_loss: 0.2822 - yB_loss: 4.0616 - val_loss: 4.4008 - val_yA_loss: 0.2653 - val_yB_loss: 4.1355\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 3.8304 - yA_loss: 0.2634 - yB_loss: 3.5670 - val_loss: 3.8833 - val_yA_loss: 0.2451 - val_yB_loss: 3.6382\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 3.3634 - yA_loss: 0.2433 - yB_loss: 3.1200 - val_loss: 3.4086 - val_yA_loss: 0.2294 - val_yB_loss: 3.1792\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.9621 - yA_loss: 0.2266 - yB_loss: 2.7355 - val_loss: 3.0112 - val_yA_loss: 0.2128 - val_yB_loss: 2.7984\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.6064 - yA_loss: 0.2116 - yB_loss: 2.3948 - val_loss: 2.6290 - val_yA_loss: 0.2002 - val_yB_loss: 2.4288\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.2922 - yA_loss: 0.1967 - yB_loss: 2.0954 - val_loss: 2.3108 - val_yA_loss: 0.1875 - val_yB_loss: 2.1232\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.0251 - yA_loss: 0.1847 - yB_loss: 1.8404 - val_loss: 2.0334 - val_yA_loss: 0.1758 - val_yB_loss: 1.8576\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.7763 - yA_loss: 0.1735 - yB_loss: 1.6028 - val_loss: 1.7939 - val_yA_loss: 0.1639 - val_yB_loss: 1.6300\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.5623 - yA_loss: 0.1622 - yB_loss: 1.4001 - val_loss: 1.5733 - val_yA_loss: 0.1549 - val_yB_loss: 1.4184\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.3762 - yA_loss: 0.1527 - yB_loss: 1.2235 - val_loss: 1.3834 - val_yA_loss: 0.1465 - val_yB_loss: 1.2369\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.2120 - yA_loss: 0.1446 - yB_loss: 1.0674 - val_loss: 1.2203 - val_yA_loss: 0.1380 - val_yB_loss: 1.0824\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.0687 - yA_loss: 0.1359 - yB_loss: 0.9327 - val_loss: 1.0724 - val_yA_loss: 0.1306 - val_yB_loss: 0.9418\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.9413 - yA_loss: 0.1288 - yB_loss: 0.8124 - val_loss: 0.9468 - val_yA_loss: 0.1246 - val_yB_loss: 0.8222\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.8317 - yA_loss: 0.1225 - yB_loss: 0.7092 - val_loss: 0.8360 - val_yA_loss: 0.1173 - val_yB_loss: 0.7186\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.7361 - yA_loss: 0.1158 - yB_loss: 0.6203 - val_loss: 0.7401 - val_yA_loss: 0.1129 - val_yB_loss: 0.6272\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.6518 - yA_loss: 0.1112 - yB_loss: 0.5406 - val_loss: 0.6543 - val_yA_loss: 0.1072 - val_yB_loss: 0.5471\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.5798 - yA_loss: 0.1058 - yB_loss: 0.4740 - val_loss: 0.5816 - val_yA_loss: 0.1028 - val_yB_loss: 0.4787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.5176 - yA_loss: 0.1012 - yB_loss: 0.4164 - val_loss: 0.5198 - val_yA_loss: 0.0995 - val_yB_loss: 0.4203\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.4608 - yA_loss: 0.0972 - yB_loss: 0.3636 - val_loss: 0.4633 - val_yA_loss: 0.0953 - val_yB_loss: 0.3680\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.4134 - yA_loss: 0.0936 - yB_loss: 0.3198 - val_loss: 0.4150 - val_yA_loss: 0.0917 - val_yB_loss: 0.3234\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.3719 - yA_loss: 0.0900 - yB_loss: 0.2819 - val_loss: 0.3745 - val_yA_loss: 0.0888 - val_yB_loss: 0.2858\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.3360 - yA_loss: 0.0873 - yB_loss: 0.2487 - val_loss: 0.3374 - val_yA_loss: 0.0857 - val_yB_loss: 0.2518\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.3053 - yA_loss: 0.0841 - yB_loss: 0.2211 - val_loss: 0.3064 - val_yA_loss: 0.0833 - val_yB_loss: 0.2231\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.2781 - yA_loss: 0.0816 - yB_loss: 0.1965 - val_loss: 0.2792 - val_yA_loss: 0.0809 - val_yB_loss: 0.1982\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.2542 - yA_loss: 0.0795 - yB_loss: 0.1747 - val_loss: 0.2558 - val_yA_loss: 0.0789 - val_yB_loss: 0.1769\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.2336 - yA_loss: 0.0771 - yB_loss: 0.1565 - val_loss: 0.2371 - val_yA_loss: 0.0772 - val_yB_loss: 0.1599\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.2154 - yA_loss: 0.0754 - yB_loss: 0.1401 - val_loss: 0.2149 - val_yA_loss: 0.0747 - val_yB_loss: 0.1403\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.1986 - yA_loss: 0.0733 - yB_loss: 0.1253 - val_loss: 0.2000 - val_yA_loss: 0.0733 - val_yB_loss: 0.1267\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1847 - yA_loss: 0.0717 - yB_loss: 0.1130 - val_loss: 0.1841 - val_yA_loss: 0.0715 - val_yB_loss: 0.1125\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.1713 - yA_loss: 0.0702 - yB_loss: 0.1011 - val_loss: 0.1714 - val_yA_loss: 0.0701 - val_yB_loss: 0.1013\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.1598 - yA_loss: 0.0687 - yB_loss: 0.0911 - val_loss: 0.1589 - val_yA_loss: 0.0685 - val_yB_loss: 0.0904\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1491 - yA_loss: 0.0673 - yB_loss: 0.0819 - val_loss: 0.1488 - val_yA_loss: 0.0673 - val_yB_loss: 0.0815\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1397 - yA_loss: 0.0660 - yB_loss: 0.0737 - val_loss: 0.1389 - val_yA_loss: 0.0659 - val_yB_loss: 0.0730\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.1313 - yA_loss: 0.0647 - yB_loss: 0.0667 - val_loss: 0.1304 - val_yA_loss: 0.0647 - val_yB_loss: 0.0656\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.1233 - yA_loss: 0.0633 - yB_loss: 0.0600 - val_loss: 0.1247 - val_yA_loss: 0.0639 - val_yB_loss: 0.0607\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1168 - yA_loss: 0.0623 - yB_loss: 0.0544 - val_loss: 0.1168 - val_yA_loss: 0.0626 - val_yB_loss: 0.0541\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.1099 - yA_loss: 0.0611 - yB_loss: 0.0488 - val_loss: 0.1082 - val_yA_loss: 0.0609 - val_yB_loss: 0.0473\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.1042 - yA_loss: 0.0598 - yB_loss: 0.0444 - val_loss: 0.1029 - val_yA_loss: 0.0600 - val_yB_loss: 0.0429\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0990 - yA_loss: 0.0588 - yB_loss: 0.0402 - val_loss: 0.0976 - val_yA_loss: 0.0588 - val_yB_loss: 0.0388\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0941 - yA_loss: 0.0575 - yB_loss: 0.0367 - val_loss: 0.0949 - val_yA_loss: 0.0581 - val_yB_loss: 0.0368\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0895 - yA_loss: 0.0566 - yB_loss: 0.0330 - val_loss: 0.0883 - val_yA_loss: 0.0564 - val_yB_loss: 0.0319\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.0857 - yA_loss: 0.0553 - yB_loss: 0.0304 - val_loss: 0.0859 - val_yA_loss: 0.0557 - val_yB_loss: 0.0302\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.0822 - yA_loss: 0.0542 - yB_loss: 0.0280 - val_loss: 0.0807 - val_yA_loss: 0.0540 - val_yB_loss: 0.0267\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0788 - yA_loss: 0.0529 - yB_loss: 0.0259 - val_loss: 0.0784 - val_yA_loss: 0.0532 - val_yB_loss: 0.0252\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0758 - yA_loss: 0.0518 - yB_loss: 0.0240 - val_loss: 0.0758 - val_yA_loss: 0.0521 - val_yB_loss: 0.0237\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0732 - yA_loss: 0.0506 - yB_loss: 0.0225 - val_loss: 0.0731 - val_yA_loss: 0.0508 - val_yB_loss: 0.0223\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.0707 - yA_loss: 0.0493 - yB_loss: 0.0214 - val_loss: 0.0719 - val_yA_loss: 0.0499 - val_yB_loss: 0.0221\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0682 - yA_loss: 0.0483 - yB_loss: 0.0199 - val_loss: 0.0676 - val_yA_loss: 0.0482 - val_yB_loss: 0.0194\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.0657 - yA_loss: 0.0469 - yB_loss: 0.0188 - val_loss: 0.0661 - val_yA_loss: 0.0472 - val_yB_loss: 0.0189\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0637 - yA_loss: 0.0458 - yB_loss: 0.0180 - val_loss: 0.0629 - val_yA_loss: 0.0452 - val_yB_loss: 0.0176\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.0615 - yA_loss: 0.0443 - yB_loss: 0.0172 - val_loss: 0.0619 - val_yA_loss: 0.0446 - val_yB_loss: 0.0173\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0594 - yA_loss: 0.0432 - yB_loss: 0.0162 - val_loss: 0.0590 - val_yA_loss: 0.0430 - val_yB_loss: 0.0160\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0578 - yA_loss: 0.0418 - yB_loss: 0.0160 - val_loss: 0.0574 - val_yA_loss: 0.0418 - val_yB_loss: 0.0156\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.0556 - yA_loss: 0.0406 - yB_loss: 0.0151 - val_loss: 0.0552 - val_yA_loss: 0.0402 - val_yB_loss: 0.0150\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0540 - yA_loss: 0.0393 - yB_loss: 0.0148 - val_loss: 0.0550 - val_yA_loss: 0.0395 - val_yB_loss: 0.0155\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0521 - yA_loss: 0.0380 - yB_loss: 0.0142 - val_loss: 0.0521 - val_yA_loss: 0.0379 - val_yB_loss: 0.0142\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0504 - yA_loss: 0.0367 - yB_loss: 0.0137 - val_loss: 0.0499 - val_yA_loss: 0.0361 - val_yB_loss: 0.0138\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.0485 - yA_loss: 0.0354 - yB_loss: 0.0131 - val_loss: 0.0479 - val_yA_loss: 0.0351 - val_yB_loss: 0.0128\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0466 - yA_loss: 0.0341 - yB_loss: 0.0125 - val_loss: 0.0462 - val_yA_loss: 0.0338 - val_yB_loss: 0.0124\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.0447 - yA_loss: 0.0328 - yB_loss: 0.0119 - val_loss: 0.0444 - val_yA_loss: 0.0325 - val_yB_loss: 0.0119\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.0431 - yA_loss: 0.0314 - yB_loss: 0.0117 - val_loss: 0.0427 - val_yA_loss: 0.0312 - val_yB_loss: 0.0114\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0413 - yA_loss: 0.0302 - yB_loss: 0.0111 - val_loss: 0.0414 - val_yA_loss: 0.0302 - val_yB_loss: 0.0112\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0398 - yA_loss: 0.0290 - yB_loss: 0.0108 - val_loss: 0.0393 - val_yA_loss: 0.0288 - val_yB_loss: 0.0105\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0378 - yA_loss: 0.0277 - yB_loss: 0.0101 - val_loss: 0.0375 - val_yA_loss: 0.0275 - val_yB_loss: 0.0101\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.0360 - yA_loss: 0.0265 - yB_loss: 0.0096 - val_loss: 0.0361 - val_yA_loss: 0.0263 - val_yB_loss: 0.0098\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.0349 - yA_loss: 0.0252 - yB_loss: 0.0096 - val_loss: 0.0339 - val_yA_loss: 0.0249 - val_yB_loss: 0.0090\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 0.0328 - yA_loss: 0.0240 - yB_loss: 0.0088 - val_loss: 0.0336 - val_yA_loss: 0.0241 - val_yB_loss: 0.0096\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.0311 - yA_loss: 0.0228 - yB_loss: 0.0082 - val_loss: 0.0318 - val_yA_loss: 0.0228 - val_yB_loss: 0.0089\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0296 - yA_loss: 0.0217 - yB_loss: 0.0079 - val_loss: 0.0290 - val_yA_loss: 0.0212 - val_yB_loss: 0.0078\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0281 - yA_loss: 0.0206 - yB_loss: 0.0074 - val_loss: 0.0276 - val_yA_loss: 0.0204 - val_yB_loss: 0.0072\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0264 - yA_loss: 0.0195 - yB_loss: 0.0069 - val_loss: 0.0268 - val_yA_loss: 0.0194 - val_yB_loss: 0.0074\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.0248 - yA_loss: 0.0184 - yB_loss: 0.0064 - val_loss: 0.0244 - val_yA_loss: 0.0181 - val_yB_loss: 0.0063\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.0234 - yA_loss: 0.0173 - yB_loss: 0.0061 - val_loss: 0.0237 - val_yA_loss: 0.0173 - val_yB_loss: 0.0064\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.0222 - yA_loss: 0.0163 - yB_loss: 0.0059 - val_loss: 0.0218 - val_yA_loss: 0.0161 - val_yB_loss: 0.0057\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0206 - yA_loss: 0.0154 - yB_loss: 0.0052 - val_loss: 0.0202 - val_yA_loss: 0.0151 - val_yB_loss: 0.0051\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0195 - yA_loss: 0.0144 - yB_loss: 0.0051 - val_loss: 0.0187 - val_yA_loss: 0.0140 - val_yB_loss: 0.0047\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.0180 - yA_loss: 0.0134 - yB_loss: 0.0045 - val_loss: 0.0176 - val_yA_loss: 0.0132 - val_yB_loss: 0.0044\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0168 - yA_loss: 0.0125 - yB_loss: 0.0043 - val_loss: 0.0168 - val_yA_loss: 0.0124 - val_yB_loss: 0.0044\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0156 - yA_loss: 0.0117 - yB_loss: 0.0039 - val_loss: 0.0163 - val_yA_loss: 0.0116 - val_yB_loss: 0.0047\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0146 - yA_loss: 0.0109 - yB_loss: 0.0037 - val_loss: 0.0141 - val_yA_loss: 0.0106 - val_yB_loss: 0.0034\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0136 - yA_loss: 0.0101 - yB_loss: 0.0035 - val_loss: 0.0129 - val_yA_loss: 0.0098 - val_yB_loss: 0.0032\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0125 - yA_loss: 0.0094 - yB_loss: 0.0031 - val_loss: 0.0129 - val_yA_loss: 0.0093 - val_yB_loss: 0.0036\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0116 - yA_loss: 0.0086 - yB_loss: 0.0029 - val_loss: 0.0110 - val_yA_loss: 0.0083 - val_yB_loss: 0.0027\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0105 - yA_loss: 0.0079 - yB_loss: 0.0025 - val_loss: 0.0100 - val_yA_loss: 0.0077 - val_yB_loss: 0.0023\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0096 - yA_loss: 0.0074 - yB_loss: 0.0023 - val_loss: 0.0095 - val_yA_loss: 0.0072 - val_yB_loss: 0.0023\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0087 - yA_loss: 0.0067 - yB_loss: 0.0020 - val_loss: 0.0084 - val_yA_loss: 0.0064 - val_yB_loss: 0.0020\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0080 - yA_loss: 0.0061 - yB_loss: 0.0018 - val_loss: 0.0089 - val_yA_loss: 0.0061 - val_yB_loss: 0.0028\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0074 - yA_loss: 0.0056 - yB_loss: 0.0018 - val_loss: 0.0069 - val_yA_loss: 0.0054 - val_yB_loss: 0.0015\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0066 - yA_loss: 0.0051 - yB_loss: 0.0015 - val_loss: 0.0062 - val_yA_loss: 0.0048 - val_yB_loss: 0.0014\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.0059 - yA_loss: 0.0046 - yB_loss: 0.0013 - val_loss: 0.0056 - val_yA_loss: 0.0044 - val_yB_loss: 0.0012\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0053 - yA_loss: 0.0041 - yB_loss: 0.0011 - val_loss: 0.0051 - val_yA_loss: 0.0039 - val_yB_loss: 0.0012\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0048 - yA_loss: 0.0037 - yB_loss: 0.0011 - val_loss: 0.0045 - val_yA_loss: 0.0035 - val_yB_loss: 9.3326e-04\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0043 - yA_loss: 0.0033 - yB_loss: 9.2562e-04 - val_loss: 0.0040 - val_yA_loss: 0.0031 - val_yB_loss: 8.5837e-04\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0038 - yA_loss: 0.0030 - yB_loss: 7.9959e-04 - val_loss: 0.0035 - val_yA_loss: 0.0028 - val_yB_loss: 7.2154e-04\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.0033 - yA_loss: 0.0026 - yB_loss: 6.6388e-04 - val_loss: 0.0031 - val_yA_loss: 0.0025 - val_yB_loss: 6.0871e-04\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.0029 - yA_loss: 0.0023 - yB_loss: 5.8708e-04 - val_loss: 0.0028 - val_yA_loss: 0.0022 - val_yB_loss: 5.4059e-04\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.0027 - yA_loss: 0.0020 - yB_loss: 6.2071e-04 - val_loss: 0.0025 - val_yA_loss: 0.0019 - val_yB_loss: 5.2151e-04\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0022 - yA_loss: 0.0018 - yB_loss: 4.6997e-04 - val_loss: 0.0021 - val_yA_loss: 0.0017 - val_yB_loss: 3.9682e-04\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0019 - yA_loss: 0.0016 - yB_loss: 3.8573e-04 - val_loss: 0.0018 - val_yA_loss: 0.0015 - val_yB_loss: 3.4316e-04\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0017 - yA_loss: 0.0013 - yB_loss: 3.8594e-04 - val_loss: 0.0016 - val_yA_loss: 0.0012 - val_yB_loss: 4.0403e-04\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.0015 - yA_loss: 0.0012 - yB_loss: 2.9576e-04 - val_loss: 0.0013 - val_yA_loss: 0.0011 - val_yB_loss: 2.4725e-04\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.0013 - yA_loss: 9.8312e-04 - yB_loss: 2.8655e-04 - val_loss: 0.0011 - val_yA_loss: 8.9903e-04 - val_yB_loss: 2.3288e-04\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.0011 - yA_loss: 8.3216e-04 - yB_loss: 2.4106e-04 - val_loss: 0.0011 - val_yA_loss: 7.9761e-04 - val_yB_loss: 2.7785e-04\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 8.7837e-04 - yA_loss: 7.0605e-04 - yB_loss: 1.7232e-04 - val_loss: 9.1553e-04 - val_yA_loss: 6.6644e-04 - val_yB_loss: 2.4909e-04\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 103us/sample - loss: 7.3101e-04 - yA_loss: 5.8719e-04 - yB_loss: 1.4383e-04 - val_loss: 6.9556e-04 - val_yA_loss: 5.5027e-04 - val_yB_loss: 1.4529e-04\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 6.0907e-04 - yA_loss: 4.8966e-04 - yB_loss: 1.1941e-04 - val_loss: 5.4697e-04 - val_yA_loss: 4.4843e-04 - val_yB_loss: 9.8535e-05\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 4.9938e-04 - yA_loss: 4.0419e-04 - yB_loss: 9.5191e-05 - val_loss: 4.6605e-04 - val_yA_loss: 3.7324e-04 - val_yB_loss: 9.2805e-05\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 4.1935e-04 - yA_loss: 3.2916e-04 - yB_loss: 9.0190e-05 - val_loss: 3.6606e-04 - val_yA_loss: 3.0229e-04 - val_yB_loss: 6.3774e-05\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 4.1043e-04 - yA_loss: 2.6899e-04 - yB_loss: 1.4144e-04 - val_loss: 3.2378e-04 - val_yA_loss: 2.3288e-04 - val_yB_loss: 9.0896e-05\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.6836e-04 - yA_loss: 2.1433e-04 - yB_loss: 5.4023e-05 - val_loss: 2.5523e-04 - val_yA_loss: 1.9468e-04 - val_yB_loss: 6.0554e-05\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 2.1311e-04 - yA_loss: 1.7091e-04 - yB_loss: 4.2193e-05 - val_loss: 1.9844e-04 - val_yA_loss: 1.5442e-04 - val_yB_loss: 4.4017e-05\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.7098e-04 - yA_loss: 1.3457e-04 - yB_loss: 3.6409e-05 - val_loss: 1.5725e-04 - val_yA_loss: 1.2207e-04 - val_yB_loss: 3.5187e-05\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.4118e-04 - yA_loss: 1.0499e-04 - yB_loss: 3.6194e-05 - val_loss: 1.2591e-04 - val_yA_loss: 9.6763e-05 - val_yB_loss: 2.9148e-05\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0965e-04 - yA_loss: 8.1854e-05 - yB_loss: 2.7798e-05 - val_loss: 8.7441e-05 - val_yA_loss: 7.2025e-05 - val_yB_loss: 1.5416e-05\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 7.9530e-05 - yA_loss: 6.2130e-05 - yB_loss: 1.7400e-05 - val_loss: 6.5464e-05 - val_yA_loss: 5.3962e-05 - val_yB_loss: 1.1502e-05\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.9331e-05 - yA_loss: 4.6625e-05 - yB_loss: 1.2706e-05 - val_loss: 6.5062e-05 - val_yA_loss: 4.1210e-05 - val_yB_loss: 2.3852e-05\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 5.0436e-05 - yA_loss: 3.4457e-05 - yB_loss: 1.5979e-05 - val_loss: 3.5748e-05 - val_yA_loss: 2.9441e-05 - val_yB_loss: 6.3064e-06\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 3.1788e-05 - yA_loss: 2.5040e-05 - yB_loss: 6.7476e-06 - val_loss: 2.7238e-05 - val_yA_loss: 2.0916e-05 - val_yB_loss: 6.3214e-06\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.4374e-05 - yA_loss: 1.8026e-05 - yB_loss: 6.3477e-06 - val_loss: 2.1629e-05 - val_yA_loss: 1.4803e-05 - val_yB_loss: 6.8253e-06\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.5774e-05 - yA_loss: 1.2749e-05 - yB_loss: 3.0246e-06 - val_loss: 1.3234e-05 - val_yA_loss: 1.0681e-05 - val_yB_loss: 2.5531e-06\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.1488e-05 - yA_loss: 8.9183e-06 - yB_loss: 2.5699e-06 - val_loss: 9.3078e-06 - val_yA_loss: 7.4532e-06 - val_yB_loss: 1.8546e-06\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 7.8224e-06 - yA_loss: 6.0427e-06 - yB_loss: 1.7797e-06 - val_loss: 6.2480e-06 - val_yA_loss: 4.9623e-06 - val_yB_loss: 1.2856e-06\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.8000e-06 - yA_loss: 4.0916e-06 - yB_loss: 1.7084e-06 - val_loss: 4.0813e-06 - val_yA_loss: 3.3415e-06 - val_yB_loss: 7.3982e-07\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 3.3468e-06 - yA_loss: 2.6723e-06 - yB_loss: 6.7452e-07 - val_loss: 2.8751e-06 - val_yA_loss: 2.1797e-06 - val_yB_loss: 6.9538e-07\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.2011e-06 - yA_loss: 1.7141e-06 - yB_loss: 4.8703e-07 - val_loss: 1.6632e-06 - val_yA_loss: 1.3416e-06 - val_yB_loss: 3.2160e-07\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.3808e-06 - yA_loss: 1.0740e-06 - yB_loss: 3.0680e-07 - val_loss: 1.1118e-06 - val_yA_loss: 8.6566e-07 - val_yB_loss: 2.4612e-07\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 2.4485e-06 - yA_loss: 6.8021e-07 - yB_loss: 1.7683e-06 - val_loss: 6.4037e-07 - val_yA_loss: 5.1762e-07 - val_yB_loss: 1.2274e-07\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 5.2294e-07 - yA_loss: 3.9782e-07 - yB_loss: 1.2513e-07 - val_loss: 3.8307e-07 - val_yA_loss: 2.9278e-07 - val_yB_loss: 9.0293e-08\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 3.9006e-07 - yA_loss: 2.2800e-07 - yB_loss: 1.6207e-07 - val_loss: 2.0979e-07 - val_yA_loss: 1.6617e-07 - val_yB_loss: 4.3621e-08\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.8909e-07 - yA_loss: 1.2790e-07 - yB_loss: 6.1195e-08 - val_loss: 1.1431e-07 - val_yA_loss: 9.2987e-08 - val_yB_loss: 2.1326e-08\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 9.7365e-08 - yA_loss: 6.9389e-08 - yB_loss: 2.7976e-08 - val_loss: 1.1312e-07 - val_yA_loss: 4.6179e-08 - val_yB_loss: 6.6941e-08\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 5.8882e-08 - yA_loss: 3.7010e-08 - yB_loss: 2.1872e-08 - val_loss: 3.3948e-08 - val_yA_loss: 2.6081e-08 - val_yB_loss: 7.8671e-09\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.5967e-07 - yA_loss: 1.9541e-08 - yB_loss: 1.4013e-07 - val_loss: 4.3345e-06 - val_yA_loss: 3.3474e-09 - val_yB_loss: 4.3312e-06\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 6.6734e-06 - yA_loss: 2.3631e-08 - yB_loss: 6.6497e-06 - val_loss: 5.7208e-06 - val_yA_loss: 2.2147e-08 - val_yB_loss: 5.6986e-06\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 8.4304e-07 - yA_loss: 4.4621e-09 - yB_loss: 8.3858e-07 - val_loss: 1.6407e-08 - val_yA_loss: 1.9107e-09 - val_yB_loss: 1.4496e-08\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 8.0901e-09 - yA_loss: 1.9933e-09 - yB_loss: 6.0968e-09 - val_loss: 5.2913e-09 - val_yA_loss: 1.5526e-09 - val_yB_loss: 3.7387e-09\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.9012e-07 - yA_loss: 1.4688e-09 - yB_loss: 1.8865e-07 - val_loss: 5.0045e-07 - val_yA_loss: 2.8499e-09 - val_yB_loss: 4.9760e-07\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 5.0078e-08 - yA_loss: 6.0242e-10 - yB_loss: 4.9476e-08 - val_loss: 7.9335e-10 - val_yA_loss: 3.2322e-10 - val_yB_loss: 4.7014e-10\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 7.7331e-09 - yA_loss: 2.2649e-10 - yB_loss: 7.5066e-09 - val_loss: 4.0894e-08 - val_yA_loss: 5.7257e-11 - val_yB_loss: 4.0837e-08\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 3.7494e-06 - yA_loss: 6.7892e-09 - yB_loss: 3.7426e-06 - val_loss: 2.7127e-05 - val_yA_loss: 4.3038e-08 - val_yB_loss: 2.7084e-05\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 9.4505e-05 - yA_loss: 2.8946e-07 - yB_loss: 9.4216e-05 - val_loss: 5.6691e-05 - val_yA_loss: 1.6419e-07 - val_yB_loss: 5.6527e-05\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 9.9389e-06 - yA_loss: 2.8385e-08 - yB_loss: 9.9105e-06 - val_loss: 2.9701e-08 - val_yA_loss: 9.7712e-10 - val_yB_loss: 2.8723e-08\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 3.7309e-07 - yA_loss: 1.5460e-09 - yB_loss: 3.7154e-07 - val_loss: 7.0809e-07 - val_yA_loss: 2.6618e-10 - val_yB_loss: 7.0782e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.1389e-06 - yA_loss: 3.2360e-09 - yB_loss: 1.1357e-06 - val_loss: 2.0717e-07 - val_yA_loss: 1.8866e-09 - val_yB_loss: 2.0528e-07\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.0608e-07 - yA_loss: 3.9829e-10 - yB_loss: 1.0569e-07 - val_loss: 9.4968e-09 - val_yA_loss: 1.3922e-10 - val_yB_loss: 9.3576e-09\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 6.9489e-09 - yA_loss: 4.2235e-11 - yB_loss: 6.9067e-09 - val_loss: 1.5352e-09 - val_yA_loss: 3.4124e-11 - val_yB_loss: 1.5011e-09\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.5800e-08 - yA_loss: 3.4363e-11 - yB_loss: 1.5766e-08 - val_loss: 3.7372e-09 - val_yA_loss: 1.4698e-11 - val_yB_loss: 3.7226e-09\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.2187e-04 - yA_loss: 2.8154e-07 - yB_loss: 1.2159e-04 - val_loss: 0.0010 - val_yA_loss: 1.1544e-06 - val_yB_loss: 0.0010\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 3.5485e-04 - yA_loss: 6.4288e-07 - yB_loss: 3.5420e-04 - val_loss: 4.4336e-04 - val_yA_loss: 7.0866e-07 - val_yB_loss: 4.4265e-04\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.3645e-05 - yA_loss: 6.1774e-08 - yB_loss: 2.3584e-05 - val_loss: 1.9632e-07 - val_yA_loss: 2.0653e-09 - val_yB_loss: 1.9425e-07\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.8705e-08 - yA_loss: 2.9729e-09 - yB_loss: 2.5732e-08 - val_loss: 8.4668e-09 - val_yA_loss: 3.4504e-09 - val_yB_loss: 5.0164e-09\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 7.4507e-09 - yA_loss: 2.7815e-09 - yB_loss: 4.6693e-09 - val_loss: 3.1253e-09 - val_yA_loss: 2.2202e-09 - val_yB_loss: 9.0509e-10\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 3.6300e-09 - yA_loss: 1.7526e-09 - yB_loss: 1.8774e-09 - val_loss: 2.0749e-09 - val_yA_loss: 1.3591e-09 - val_yB_loss: 7.1578e-10\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.1952e-09 - yA_loss: 9.7368e-10 - yB_loss: 2.2149e-10 - val_loss: 9.0534e-10 - val_yA_loss: 7.2472e-10 - val_yB_loss: 1.8062e-10\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 7.6768e-10 - yA_loss: 5.2653e-10 - yB_loss: 2.4115e-10 - val_loss: 2.1057e-09 - val_yA_loss: 4.2365e-10 - val_yB_loss: 1.6821e-09\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.5118e-09 - yA_loss: 2.6852e-10 - yB_loss: 1.2433e-09 - val_loss: 1.0176e-09 - val_yA_loss: 1.9473e-10 - val_yB_loss: 8.2282e-10\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 6.9006e-09 - yA_loss: 1.3092e-10 - yB_loss: 6.7697e-09 - val_loss: 1.6735e-08 - val_yA_loss: 1.9628e-10 - val_yB_loss: 1.6539e-08\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.3115e-08 - yA_loss: 1.1920e-10 - yB_loss: 2.2996e-08 - val_loss: 3.9596e-08 - val_yA_loss: 1.8361e-10 - val_yB_loss: 3.9412e-08\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 4.0785e-08 - yA_loss: 9.4957e-11 - yB_loss: 4.0690e-08 - val_loss: 2.4300e-08 - val_yA_loss: 5.8399e-11 - val_yB_loss: 2.4242e-08\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.2760e-08 - yA_loss: 3.9582e-11 - yB_loss: 1.2721e-08 - val_loss: 4.5926e-11 - val_yA_loss: 8.3071e-12 - val_yB_loss: 3.7619e-11\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 2.1142e-10 - yA_loss: 3.8677e-12 - yB_loss: 2.0755e-10 - val_loss: 1.6769e-10 - val_yA_loss: 2.6013e-12 - val_yB_loss: 1.6509e-10\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 9.5224e-11 - yA_loss: 1.4151e-12 - yB_loss: 9.3809e-11 - val_loss: 3.5711e-11 - val_yA_loss: 1.4275e-12 - val_yB_loss: 3.4283e-11\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 2.4695e-09 - yA_loss: 4.2461e-12 - yB_loss: 2.4653e-09 - val_loss: 2.2890e-10 - val_yA_loss: 6.3078e-13 - val_yB_loss: 2.2827e-10\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 7.0269e-10 - yA_loss: 1.5396e-12 - yB_loss: 7.0115e-10 - val_loss: 3.9724e-11 - val_yA_loss: 7.9846e-14 - val_yB_loss: 3.9644e-11\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 6.7628e-10 - yA_loss: 1.1821e-12 - yB_loss: 6.7509e-10 - val_loss: 2.4104e-08 - val_yA_loss: 2.5404e-11 - val_yB_loss: 2.4079e-08\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.8592e-09 - yA_loss: 5.4144e-12 - yB_loss: 1.8537e-09 - val_loss: 1.0407e-09 - val_yA_loss: 1.2199e-12 - val_yB_loss: 1.0395e-09\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.3342e-07 - yA_loss: 5.1864e-10 - yB_loss: 1.3290e-07 - val_loss: 8.9166e-09 - val_yA_loss: 2.5013e-10 - val_yB_loss: 8.6664e-09\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 5.2089e-08 - yA_loss: 1.1173e-10 - yB_loss: 5.1977e-08 - val_loss: 8.5945e-07 - val_yA_loss: 1.1582e-09 - val_yB_loss: 8.5829e-07\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 3.8219e-06 - yA_loss: 6.8967e-09 - yB_loss: 3.8150e-06 - val_loss: 3.4838e-04 - val_yA_loss: 4.2481e-07 - val_yB_loss: 3.4795e-04\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 9.8093e-04 - yA_loss: 2.4565e-06 - yB_loss: 9.7847e-04 - val_loss: 3.0714e-05 - val_yA_loss: 5.0699e-07 - val_yB_loss: 3.0207e-05\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 5.1982e-05 - yA_loss: 2.0196e-07 - yB_loss: 5.1780e-05 - val_loss: 3.0129e-07 - val_yA_loss: 6.1238e-09 - val_yB_loss: 2.9516e-07\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0417e-06 - yA_loss: 3.9240e-09 - yB_loss: 1.0377e-06 - val_loss: 1.7704e-06 - val_yA_loss: 2.4892e-09 - val_yB_loss: 1.7679e-06\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 3.7944e-08 - yA_loss: 3.9621e-09 - yB_loss: 3.3982e-08 - val_loss: 2.2875e-09 - val_yA_loss: 2.2640e-09 - val_yB_loss: 2.3461e-11\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.6865e-09 - yA_loss: 1.6237e-09 - yB_loss: 6.2808e-11 - val_loss: 1.3853e-09 - val_yA_loss: 1.1977e-09 - val_yB_loss: 1.8759e-10\n"
     ]
    }
   ],
   "source": [
    "model.compile('adam', 'mse')\n",
    "hist = model.fit([x_train_A, x_train_B], [y_train_A, y_train_B], batch_size=8, epochs=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yA_weights\n",
      " [array([[2.0000052]], dtype=float32), array([1.0000343], dtype=float32)] \n",
      "\n",
      "yB_weights\n",
      " [array([[1.9999226],\n",
      "       [3.0000017]], dtype=float32), array([1.0000197], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "yA_weights = model.get_layer('yA').get_weights()\n",
    "yB_weights = model.get_layer('yB').get_weights()\n",
    "\n",
    "print('yA_weights\\n', yA_weights, '\\n')\n",
    "print('yB_weights\\n', yB_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
