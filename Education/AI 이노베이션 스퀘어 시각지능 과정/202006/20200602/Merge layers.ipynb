{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러개의 입력을 하나로 합치는 layer 에 대해서 알아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "x_test_A = np.array([[0,1],[4,5]])\n",
    "x_test_B = np.array([[2,3],[6,7]])\n",
    "\n",
    "xA = layers.Input(shape=(2,), name='xA')\n",
    "xB = layers.Input(shape=(2,), name='xB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력을 그대로 이어서 붙이는 Concatenate 을 사용해 봅시다. 소문자로 시작하는 concatenate 은 괄호 안에 바로 입력 텐서를 넣어주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 4)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "x_test_A\n",
      "[[0 1]\n",
      " [4 5]]\n",
      "\n",
      "x_test_B\n",
      "[[2 3]\n",
      " [6 7]]\n",
      "\n",
      "y_pred\n",
      "[[0. 1. 2. 3.]\n",
      " [4. 5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "y = layers.Concatenate(name='concat')([xA, xB])\n",
    "#y = layers.concatenate([xA, xB], name='cancat')\n",
    "model = models.Model(inputs=[xA, xB], outputs=y)\n",
    "model.summary()\n",
    "\n",
    "y_pred = model.predict([x_test_A, x_test_B])\n",
    "\n",
    "print()\n",
    "print('x_test_A'); print(x_test_A)\n",
    "print()\n",
    "print('x_test_B'); print(x_test_B)\n",
    "print()\n",
    "print('y_pred'); print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력을 더해서 하나로 만드는 Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "x_test_A\n",
      "[[0 1]\n",
      " [4 5]]\n",
      "\n",
      "x_test_B\n",
      "[[2 3]\n",
      " [6 7]]\n",
      "\n",
      "y_pred\n",
      "[[ 2.  4.]\n",
      " [10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "y = layers.Add(name='add')([xA, xB])\n",
    "#y = layers.add([xA, xB], name='add')\n",
    "model = models.Model(inputs=[xA, xB], outputs=y)\n",
    "model.summary()\n",
    "\n",
    "y_pred = model.predict([x_test_A, x_test_B])\n",
    "\n",
    "print()\n",
    "print('x_test_A'); print(x_test_A)\n",
    "print()\n",
    "print('x_test_B'); print(x_test_B)\n",
    "print()\n",
    "print('y_pred'); print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 입력을 곱해서 하나로 만드는 Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mul (Multiply)                  (None, 2)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "x_test_A\n",
      "[[0 1]\n",
      " [4 5]]\n",
      "\n",
      "x_test_B\n",
      "[[2 3]\n",
      " [6 7]]\n",
      "\n",
      "y_pred\n",
      "[[ 0.  3.]\n",
      " [24. 35.]]\n"
     ]
    }
   ],
   "source": [
    "y = layers.Multiply(name='mul')([xA, xB])\n",
    "#y = layers.multiply([xA, xB], name='mul')\n",
    "model = models.Model(inputs=[xA, xB], outputs=y)\n",
    "model.summary()\n",
    "y_pred = model.predict([x_test_A, x_test_B])\n",
    "\n",
    "print()\n",
    "print('x_test_A'); print(x_test_A)\n",
    "print()\n",
    "print('x_test_B'); print(x_test_B)\n",
    "print()\n",
    "print('y_pred'); print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda layer 와 Concatenate layer 를 사용하여 y = c0 + c1 * x + c2 * x^2 + c3 * x^3 관계를 가진 데이터에서 학습을 통하여 c0, c1, c2, c3 를 찾아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "c0 = 1; c1 = 1; c2 = 2; c3 = 3\n",
    "\n",
    "x_train = np.random.rand(1000,1) * 10 - 5\n",
    "y_train = c0 + c1 * x_train +  c2 * x_train**2 + c3 * x_train**3\n",
    "\n",
    "x = layers.Input(shape=(1,), name='x')\n",
    "\n",
    "h1_A, h1_B, h1_C = layers.Lambda(lambda x: [x, x**2, x**3], name='h1_ABC')(x)\n",
    "h2 = layers.Concatenate(name='concat')([h1_A, h1_B, h1_C])\n",
    "y = layers.Dense(1, name='y')(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h1_ABC (Lambda)                 [(None, 1), (None, 1 0           x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 3)            0           h1_ABC[0][0]                     \n",
      "                                                                 h1_ABC[0][1]                     \n",
      "                                                                 h1_ABC[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "y (Dense)                       (None, 1)            4           concat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 15690.2553 - val_loss: 14732.2311\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 14624.1520 - val_loss: 13699.3232\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 13599.9983 - val_loss: 12757.2307\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 12638.8535 - val_loss: 11832.4180\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 11725.9955 - val_loss: 10970.7462\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 10862.9568 - val_loss: 10165.9812\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 10049.1291 - val_loss: 9392.6693\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 9282.9142 - val_loss: 8671.1327\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 8564.1096 - val_loss: 7979.2123\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 7884.5165 - val_loss: 7352.0461\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 7249.8602 - val_loss: 6738.1511\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 6651.2896 - val_loss: 6179.4701\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 6092.8278 - val_loss: 5647.6969\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 5566.3651 - val_loss: 5168.3864\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 5076.0068 - val_loss: 4719.3808\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 4621.3010 - val_loss: 4276.3475\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 4194.6552 - val_loss: 3870.8666\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 3797.2758 - val_loss: 3507.5717\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 3431.2738 - val_loss: 3163.6430\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 3090.8597 - val_loss: 2846.1821\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 2776.6420 - val_loss: 2551.3486\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 2485.8798 - val_loss: 2282.2740\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 2218.4968 - val_loss: 2036.7008\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 1974.0105 - val_loss: 1807.9563\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 1750.2568 - val_loss: 1601.7855\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 1547.2491 - val_loss: 1412.2030\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 1363.0017 - val_loss: 1240.5916\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1195.3253 - val_loss: 1089.0768\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1045.1315 - val_loss: 955.1737\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 911.9535 - val_loss: 824.6549\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 790.8370 - val_loss: 716.9834\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 684.3329 - val_loss: 619.8065\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 589.9587 - val_loss: 534.7556\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 507.5287 - val_loss: 457.6525\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 434.7198 - val_loss: 393.2581\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 371.8681 - val_loss: 335.7306\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 317.1757 - val_loss: 286.3512\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 270.1688 - val_loss: 243.3653\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 229.6376 - val_loss: 208.2046\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 195.4889 - val_loss: 177.0554\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 166.1179 - val_loss: 151.2608\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 141.3738 - val_loss: 129.1012\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 120.5606 - val_loss: 110.1056\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 102.9311 - val_loss: 94.9037\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 88.1994 - val_loss: 81.1326\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 75.7775 - val_loss: 69.8328\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 65.2383 - val_loss: 60.5301\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 56.3808 - val_loss: 52.8856\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 48.9184 - val_loss: 45.7359\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 42.4871 - val_loss: 39.8829\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 36.9755 - val_loss: 34.9829\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 32.2339 - val_loss: 30.4631\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 28.0876 - val_loss: 26.6581\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 24.4857 - val_loss: 23.3659\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 21.3443 - val_loss: 20.3447\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 18.5896 - val_loss: 17.7091\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 16.2032 - val_loss: 15.4704\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 14.0550 - val_loss: 13.4104\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 12.2009 - val_loss: 11.7091\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 10.5796 - val_loss: 10.2126\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 9.1993 - val_loss: 8.8978\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 8.0136 - val_loss: 7.8081\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 7.0119 - val_loss: 6.8505\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 6.1651 - val_loss: 6.0624\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 5.4492 - val_loss: 5.3903\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 4.8573 - val_loss: 4.8372\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 4.3691 - val_loss: 4.3812\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 3.9691 - val_loss: 4.0153\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 3.6450 - val_loss: 3.7059\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 3.3856 - val_loss: 3.4571\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 3.1707 - val_loss: 3.2555\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.9987 - val_loss: 3.0882\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.8573 - val_loss: 2.9507\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.7403 - val_loss: 2.8364\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 2.6422 - val_loss: 2.7378\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 89us/sample - loss: 2.5567 - val_loss: 2.6501\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 2.4811 - val_loss: 2.5710\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 2.4113 - val_loss: 2.4985\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 2.3485 - val_loss: 2.4287\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.2872 - val_loss: 2.3635\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 2.2241 - val_loss: 2.2975\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 2.1624 - val_loss: 2.2334\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 2.1022 - val_loss: 2.1683\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 2.0405 - val_loss: 2.1028\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.9792 - val_loss: 2.0377\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 1.9162 - val_loss: 1.9717\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 1.8538 - val_loss: 1.9059\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.7896 - val_loss: 1.8397\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.7285 - val_loss: 1.7714\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.6623 - val_loss: 1.7036\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 1.5978 - val_loss: 1.6360\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.5342 - val_loss: 1.5679\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.4679 - val_loss: 1.5013\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 1.4033 - val_loss: 1.4327\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.3389 - val_loss: 1.3666\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.2754 - val_loss: 1.2992\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.2130 - val_loss: 1.2336\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.1485 - val_loss: 1.1681\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.0860 - val_loss: 1.1035\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.0246 - val_loss: 1.0401\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.9644 - val_loss: 0.9772\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.9054 - val_loss: 0.9165\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.8497 - val_loss: 0.8572\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.7920 - val_loss: 0.7984\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.7372 - val_loss: 0.7440\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.6861 - val_loss: 0.6884\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.6331 - val_loss: 0.6354\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.5836 - val_loss: 0.5871\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.5385 - val_loss: 0.5372\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.4921 - val_loss: 0.4910\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.4498 - val_loss: 0.4475\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.4080 - val_loss: 0.4057\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.3702 - val_loss: 0.3666\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.3338 - val_loss: 0.3296\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.2991 - val_loss: 0.2945\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2672 - val_loss: 0.2626\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2382 - val_loss: 0.2332\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2103 - val_loss: 0.2054\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1855 - val_loss: 0.1816\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1618 - val_loss: 0.1577\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1408 - val_loss: 0.1362\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1222 - val_loss: 0.1169\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.1046 - val_loss: 0.1002\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.0890 - val_loss: 0.0868\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0760 - val_loss: 0.0719\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 95us/sample - loss: 0.0636 - val_loss: 0.0604\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.0531 - val_loss: 0.0499\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.0440 - val_loss: 0.0410\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.0363 - val_loss: 0.0336\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.0293 - val_loss: 0.0270\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.0236 - val_loss: 0.0217\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.0188 - val_loss: 0.0175\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0149 - val_loss: 0.0135\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0068 - val_loss: 0.0061\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.0010 - val_loss: 8.5995e-04\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 7.1416e-04 - val_loss: 5.8666e-04\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 4.8296e-04 - val_loss: 3.9710e-04\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 3.3709e-04 - val_loss: 2.7180e-04\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 2.1387e-04 - val_loss: 1.6770e-04\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.3647e-04 - val_loss: 1.1501e-04\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 8.4774e-05 - val_loss: 6.4198e-05\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 5.1796e-05 - val_loss: 3.9497e-05\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 3.0806e-05 - val_loss: 2.2868e-05\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 1.7985e-05 - val_loss: 1.3032e-05\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.0134e-05 - val_loss: 7.3167e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 5.5727e-06 - val_loss: 4.0562e-06\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 2.9824e-06 - val_loss: 2.0610e-06\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.5708e-06 - val_loss: 1.0632e-06\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 7.6404e-07 - val_loss: 5.6185e-07\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 3.8600e-07 - val_loss: 2.3314e-07\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.7056e-07 - val_loss: 1.0522e-07\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 7.4928e-08 - val_loss: 5.0101e-08\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 3.3166e-08 - val_loss: 2.1247e-08\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 1.4191e-08 - val_loss: 9.3218e-09\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 5.7556e-09 - val_loss: 3.3046e-09\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 2.4244e-09 - val_loss: 1.4037e-09\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 1.0704e-09 - val_loss: 7.6005e-10\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 6.0742e-10 - val_loss: 6.2944e-10\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 4.8192e-10 - val_loss: 3.3330e-10\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 3.3373e-10 - val_loss: 3.0851e-10\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 3.0201e-10 - val_loss: 2.1300e-10\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.7099e-10 - val_loss: 2.1142e-10\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 2.7302e-10 - val_loss: 1.9412e-10\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.5204e-10 - val_loss: 1.9059e-10\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.2272e-10 - val_loss: 1.9084e-10\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.3969e-10 - val_loss: 1.9767e-10\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 2.5346e-10 - val_loss: 1.9300e-10\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 2.5650e-10 - val_loss: 2.7769e-10\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 2.4973e-10 - val_loss: 2.5760e-10\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 2.5684e-10 - val_loss: 1.9278e-10\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 5.4880e-10 - val_loss: 2.9901e-10\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 2.6683e-10 - val_loss: 3.1246e-10\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 5.5333e-10 - val_loss: 2.3517e-10\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 3.6796e-10 - val_loss: 2.6903e-10\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 4.3085e-10 - val_loss: 3.3905e-10\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 4.4638e-10 - val_loss: 3.4261e-10\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 6.4163e-10 - val_loss: 1.1437e-09\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 3.5236e-10 - val_loss: 4.0132e-10\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 2.8092e-10 - val_loss: 7.3682e-10\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 4.5413e-10 - val_loss: 2.8694e-10\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 4.0033e-10 - val_loss: 2.7662e-10\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 5.1230e-10 - val_loss: 2.2944e-10\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 7.3969e-10 - val_loss: 4.1975e-10\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 5.5820e-10 - val_loss: 4.2030e-10\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 7.0194e-10 - val_loss: 2.3690e-10\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 1.3325e-09 - val_loss: 1.8990e-09\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 8.8606e-10 - val_loss: 3.8044e-10\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 2.0949e-09 - val_loss: 1.0690e-09\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 8.2450e-10 - val_loss: 5.2620e-10\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 6.2756e-10 - val_loss: 2.1824e-10\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 8.4141e-10 - val_loss: 8.9148e-10\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 1.2736e-09 - val_loss: 2.0911e-10\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 1.8421e-09 - val_loss: 1.8334e-10\n"
     ]
    }
   ],
   "source": [
    "model.compile('adam', 'mse')\n",
    "hist = model.fit(x_train, y_train, batch_size=8, epochs=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_weights\n",
      "[array([[0.9999995],\n",
      "       [2.       ],\n",
      "       [3.       ]], dtype=float32), array([0.99999946], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "y_weights = model.get_layer('y').get_weights()\n",
    "\n",
    "print('y_weights'); print(y_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
