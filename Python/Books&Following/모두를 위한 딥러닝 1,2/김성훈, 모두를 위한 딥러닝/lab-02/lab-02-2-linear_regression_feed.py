import tensorflow as tf

tf.set_random_seed(777)

W = tf.Variable(tf.random_normal([1]), name="weight")
b = tf.Variable(tf.random_normal([1]), name="bias")

X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])

hypothesis = X * W + b
cost = tf.reduce_mean(tf.square(hypothesis - Y))
train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(2001):
        _, cost_val, W_val, b_val = sess.run([train, cost, W, b], feed_dict={X: [1,2,3], Y: [1,2,3]})
        if step % 20 == 0:
            print(step, cost_val, W_val, b_val)
            '''실행결과
0 3.5240757 [2.1286771] [-0.8523567]
20 0.19749945 [1.533928] [-1.0505961]
40 0.15214379 [1.4572546] [-1.0239124]
60 0.1379325 [1.4308538] [-0.9779527]
80 0.12527025 [1.4101374] [-0.93219817]
100 0.11377233 [1.3908179] [-0.8884077]
120 0.10332986 [1.3724468] [-0.8466577]
140 0.093845844 [1.3549428] [-0.80686814]
160 0.08523229 [1.3382617] [-0.7689483]
180 0.07740932 [1.3223647] [-0.73281056]
200 0.07030439 [1.3072149] [-0.6983712]
220 0.06385162 [1.2927768] [-0.6655505]
240 0.05799109 [1.2790174] [-0.63427216]
260 0.05266844 [1.2659047] [-0.6044637]
280 0.047834318 [1.2534081] [-0.57605624]
300 0.043443877 [1.2414987] [-0.5489836]
320 0.0394564 [1.2301493] [-0.5231833]
340 0.035834935 [1.2193329] [-0.49859545]
360 0.032545824 [1.2090251] [-0.47516325]
380 0.029558638 [1.1992016] [-0.45283225]
400 0.026845641 [1.18984] [-0.4315508]
420 0.024381675 [1.1809182] [-0.41126958]
440 0.02214382 [1.1724157] [-0.39194146]
460 0.020111356 [1.1643128] [-0.37352163]
480 0.018265454 [1.1565907] [-0.35596743]
500 0.016588978 [1.1492316] [-0.33923826]
520 0.015066384 [1.1422179] [-0.3232953]
540 0.01368351 [1.1355343] [-0.30810148]
560 0.012427575 [1.1291647] [-0.29362184]
580 0.011286932 [1.1230947] [-0.2798227]
600 0.010250964 [1.1173096] [-0.26667204]
620 0.009310094 [1.1117964] [-0.25413945]
640 0.008455581 [1.1065423] [-0.24219586]
660 0.0076795053 [1.1015354] [-0.23081362]
680 0.006974643 [1.0967635] [-0.21996623]
700 0.0063344706 [1.0922159] [-0.20962858]
720 0.0057530706 [1.0878822] [-0.19977672]
740 0.0052250377 [1.0837522] [-0.19038804]
760 0.004745458 [1.0798159] [-0.18144041]
780 0.004309906 [1.076065] [-0.17291337]
800 0.003914324 [1.0724902] [-0.16478711]
820 0.0035550483 [1.0690835] [-0.1570428]
840 0.0032287557 [1.0658368] [-0.14966238]
860 0.0029324207 [1.0627428] [-0.14262886]
880 0.0026632652 [1.059794] [-0.13592596]
900 0.0024188235 [1.056984] [-0.12953788]
920 0.0021968128 [1.0543059] [-0.12345006]
940 0.001995178 [1.0517538] [-0.11764836]
960 0.0018120449 [1.0493214] [-0.11211928]
980 0.0016457299 [1.0470035] [-0.10685005]
1000 0.0014946823 [1.0447946] [-0.10182849]
1020 0.0013574976 [1.0426894] [-0.09704296]
1040 0.001232898 [1.0406833] [-0.09248237]
1060 0.0011197334 [1.038771] [-0.08813594]
1080 0.0010169626 [1.0369489] [-0.08399385]
1100 0.0009236224 [1.0352125] [-0.08004645]
1120 0.0008388485 [1.0335577] [-0.07628451]
1140 0.0007618535 [1.0319806] [-0.07269943]
1160 0.0006919258 [1.0304775] [-0.06928282]
1180 0.00062842044 [1.0290452] [-0.06602671]
1200 0.0005707396 [1.0276802] [-0.06292368]
1220 0.00051835255 [1.0263793] [-0.05996648]
1240 0.00047077626 [1.0251396] [-0.05714824]
1260 0.00042756708 [1.0239582] [-0.0544625]
1280 0.00038832307 [1.0228322] [-0.05190301]
1300 0.00035268333 [1.0217593] [-0.04946378]
1320 0.0003203152 [1.0207369] [-0.04713925]
1340 0.0002909189 [1.0197623] [-0.0449241]
1360 0.00026421514 [1.0188333] [-0.04281275]
1380 0.0002399599 [1.0179482] [-0.04080062]
1400 0.00021793543 [1.0171047] [-0.03888312]
1420 0.00019793434 [1.0163009] [-0.03705578]
1440 0.00017976768 [1.0155348] [-0.03531429]
1460 0.00016326748 [1.0148047] [-0.03365463]
1480 0.00014828023 [1.0141089] [-0.03207294]
1500 0.00013467176 [1.0134459] [-0.03056567]
1520 0.00012231102 [1.0128139] [-0.02912918]
1540 0.0001110848 [1.0122118] [-0.0277602]
1560 0.000100889745 [1.0116379] [-0.02645557]
1580 9.162913e-05 [1.011091] [-0.02521228]
1600 8.322027e-05 [1.0105698] [-0.02402747]
1620 7.5580865e-05 [1.0100728] [-0.02289824]
1640 6.8643785e-05 [1.0095996] [-0.02182201]
1660 6.234206e-05 [1.0091484] [-0.02079643]
1680 5.662038e-05 [1.0087185] [-0.01981908]
1700 5.142322e-05 [1.0083088] [-0.01888768]
1720 4.6704197e-05 [1.0079182] [-0.01800001]
1740 4.2417145e-05 [1.0075461] [-0.01715406]
1760 3.852436e-05 [1.0071915] [-0.01634789]
1780 3.4988276e-05 [1.0068535] [-0.01557961]
1800 3.1776715e-05 [1.0065314] [-0.01484741]
1820 2.8859866e-05 [1.0062244] [-0.0141496]
1840 2.621177e-05 [1.005932] [-0.01348464]
1860 2.380544e-05 [1.0056531] [-0.01285094]
1880 2.1620841e-05 [1.0053875] [-0.012247]
1900 1.9636196e-05 [1.0051342] [-0.01167146]
1920 1.7834054e-05 [1.004893] [-0.01112291]
1940 1.6197106e-05 [1.0046631] [-0.01060018]
1960 1.4711059e-05 [1.004444] [-0.01010205]
1980 1.3360998e-05 [1.0042351] [-0.00962736]
2000 1.21343355e-05 [1.0040361] [-0.00917497]'''
    print(sess.run(hypothesis, feed_dict={X: [5]}))
    '''실행결과: [5.0110054]'''
    print(sess.run(hypothesis, feed_dict={X: [2,5]}))
    '''실행결과: [1.9988972 5.0110054]'''
    print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))
    '''실행결과: [1.4968792 3.5049512]'''

    for step in range(2001):
        _, cost_val, W_val, b_val = sess.run([train, cost, W, b], feed_dict={X: [1,2,3,4,5], Y: [2.1,3.1,4.1,5.1,6.1]})
        if step % 20 == 0:
            print(step, cost_val, W_val, b_val)
            '''실행결과
            0 1.2035878 [1.0696986] [0.01276637]
20 0.16904518 [1.2650416] [0.13934135]
40 0.14761032 [1.2485868] [0.20250577]
60 0.1289092 [1.2323107] [0.26128453]
80 0.112577364 [1.2170966] [0.3162127]
100 0.09831471 [1.2028787] [0.36754355]
120 0.08585897 [1.189592] [0.41551268]
140 0.07498121 [1.1771754] [0.46034035]
160 0.0654817 [1.165572] [0.5022322]
180 0.05718561 [1.1547288] [0.54138047]
200 0.049940635 [1.1445953] [0.5779649]
220 0.043613486 [1.1351256] [0.6121535]
240 0.038087945 [1.1262761] [0.64410305]
260 0.033262506 [1.1180062] [0.6739601]
280 0.029048424 [1.1102779] [0.7018617]
300 0.025368208 [1.1030556] [0.7279361]
320 0.022154227 [1.0963064] [0.7523028]
340 0.019347461 [1.0899993] [0.7750737]
360 0.016896311 [1.0841053] [0.7963533]
380 0.014755693 [1.0785972] [0.8162392]
400 0.012886246 [1.0734499] [0.83482295]
420 0.011253643 [1.0686395] [0.85218966]
440 0.009827888 [1.0641443] [0.868419]
460 0.008582776 [1.0599433] [0.88358533]
480 0.0074953884 [1.0560175] [0.89775866]
500 0.006545782 [1.0523489] [0.9110037]
520 0.005716468 [1.0489205] [0.9233812]
540 0.0049922303 [1.0457168] [0.93494815]
560 0.004359761 [1.0427227] [0.94575745]
580 0.0038074062 [1.0399247] [0.95585895]
600 0.0033250246 [1.0373099] [0.96529907]
620 0.0029037776 [1.0348666] [0.9741207]
640 0.0025359015 [1.0325832] [0.9823645]
660 0.002214623 [1.0304493] [0.99006844]
680 0.0019340345 [1.028455] [0.99726814]
700 0.00168901 [1.0265915] [1.0039961]
720 0.0014750187 [1.02485] [1.0102835]
740 0.0012881459 [1.0232226] [1.0161589]
760 0.0011249502 [1.0217017] [1.0216497]
780 0.0009824366 [1.0202806] [1.026781]
800 0.00085795636 [1.0189523] [1.0315762]
820 0.00074926845 [1.017711] [1.0360574]
840 0.0006543383 [1.0165511] [1.0402449]
860 0.00057143776 [1.0154672] [1.0441583]
880 0.00049904286 [1.0144542] [1.0478154]
900 0.0004358191 [1.0135076] [1.0512332]
920 0.00038059853 [1.0126231] [1.0544269]
940 0.00033238466 [1.0117964] [1.0574113]
960 0.0002902703 [1.0110238] [1.0602009]
980 0.00025349384 [1.0103018] [1.0628073]
1000 0.00022137808 [1.009627] [1.0652432]
1020 0.00019332914 [1.0089965] [1.0675194]
1040 0.00016882908 [1.0084072] [1.0696473]
1060 0.00014743926 [1.0078566] [1.0716351]
1080 0.00012875989 [1.007342] [1.0734928]
1100 0.00011244613 [1.0068612] [1.0752288]
1120 9.8200355e-05 [1.0064118] [1.0768511]
1140 8.5755724e-05 [1.0059919] [1.0783674]
1160 7.489431e-05 [1.0055996] [1.0797837]
1180 6.5406595e-05 [1.0052328] [1.0811077]
1200 5.7120622e-05 [1.0048901] [1.0823449]
1220 4.9882394e-05 [1.0045699] [1.0835012]
1240 4.3564207e-05 [1.0042707] [1.0845816]
1260 3.804614e-05 [1.003991] [1.0855912]
1280 3.3225275e-05 [1.0037296] [1.0865349]
1300 2.901571e-05 [1.0034853] [1.0874166]
1320 2.5340463e-05 [1.003257] [1.0882409]
1340 2.2129901e-05 [1.0030438] [1.089011]
1360 1.9328054e-05 [1.0028446] [1.0897301]
1380 1.6878726e-05 [1.0026582] [1.0904027]
1400 1.4740454e-05 [1.0024842] [1.0910312]
1420 1.2873619e-05 [1.0023215] [1.0916185]
1440 1.1241735e-05 [1.0021695] [1.0921675]
1460 9.818069e-06 [1.0020274] [1.0926803]
1480 8.574677e-06 [1.0018947] [1.0931597]
1500 7.4886166e-06 [1.0017706] [1.0936075]
1520 6.539272e-06 [1.0016547] [1.0940262]
1540 5.711003e-06 [1.0015464] [1.0944173]
1560 4.9874334e-06 [1.001445] [1.094783]
1580 4.3559958e-06 [1.0013504] [1.0951246]
1600 3.804345e-06 [1.0012621] [1.0954438]
1620 3.322312e-06 [1.0011792] [1.0957422]
1640 2.9007756e-06 [1.0011021] [1.0960212]
1660 2.5334934e-06 [1.00103] [1.0962818]
1680 2.2123513e-06 [1.0009624] [1.0965253]
1700 1.9319202e-06 [1.0008993] [1.096753]
1720 1.6872369e-06 [1.0008405] [1.0969656]
1740 1.4738443e-06 [1.0007855] [1.0971642]
1760 1.2871467e-06 [1.0007341] [1.0973498]
1780 1.12424e-06 [1.0006859] [1.0975232]
1800 9.815564e-07 [1.0006411] [1.0976855]
1820 8.573661e-07 [1.0005993] [1.0978369]
1840 7.4871434e-07 [1.00056] [1.0979784]
1860 6.5427787e-07 [1.0005234] [1.0981107]
1880 5.712507e-07 [1.0004891] [1.0982342]
1900 4.989224e-07 [1.0004572] [1.0983498]
1920 4.358085e-07 [1.0004272] [1.0984578]
1940 3.8070743e-07 [1.0003992] [1.0985587]
1960 3.3239553e-07 [1.000373] [1.098653]
1980 2.9042917e-07 [1.0003488] [1.0987409]
2000 2.5372992e-07 [1.000326] [1.0988233]'''

    print(sess.run(hypothesis, feed_dict={X: [5]}))
    '''실행결과 [6.1004534]'''
    print(sess.run(hypothesis, feed_dict={X: [2,5]}))
    '''실행결과 [3.0994754 6.1004534]'''
    print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))
    '''실행결과 [2.5993123 4.599964 ]'''