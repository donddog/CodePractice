{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: JPype1>=0.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (0.6.3)\n",
      "Installing collected packages: konlpy\n",
      "Successfully installed konlpy-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing f:\\jpype1-0.6.3-cp37-cp37m-win_amd64.whl\n",
      "Installing collected packages: JPype1\n",
      "Successfully installed JPype1-0.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install JPype1-0.6.3-cp37-cp37m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading Text: Package 'Text' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "##### 1. 대소문자 통합(소문자)\n",
    "##### 2. 구두점 처리(I'd, I'm) => tokenizing => 대안: 형태소 분석기\n",
    "##### 3. 불용어(stopwords) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'd like to learn more somthing.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more somthing.\"\n",
    "\n",
    "sentence.lower() #(1번 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "Id like to learn more somthing\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "print(punctuation) #ex ) Finland's => finland, finlands which?\n",
    "#한국어 ex) '오늘'의 => 오늘 의, 오늘의 which?\n",
    "print(re.escape(punctuation))\n",
    "print(re.sub(\"[{0}]\".format(re.escape(punctuation)), \"\", sentence))\n",
    "#Id로 나오는 부분이 문제가 될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Id like to learn more somthing'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.lower()\n",
    "pattern = re.compile(\"[{0}]\".format(re.escape(punctuation)))\n",
    "pattern.sub(\"\",sentence.lower())\n",
    "re.sub(\"[{0}]\".format(re.escape(punctuation)), \"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id like to learn more somthing'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.sub(\"\",sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘은 ' 목'요일\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(\" \".join(word_tokenize(sentence.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘은 목요일\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘은 '목'요일\"\n",
    "#pattern = re.compile(\"[{0}]\".format(re.escape(punctuation)))\n",
    "#print(pattern.sub(\"\",sentence.lower()))\n",
    "print(re.sub(\"[{0}]\".format(re.escape(punctuation)), \"\", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i d sasdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"\\s{2,}\",\" \", \"i   d    sasdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936\n",
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.open(\"english\").read()\n",
    "\n",
    "print(len(stop))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped\n",
      "love\n",
      "Skipped\n",
      "Skipped\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love you.\"\n",
    "\n",
    "# for _ in sentence.lower().split():\n",
    "#     if _ in stop:\n",
    "#         print(\"Skipped\")\n",
    "#     else:\n",
    "#         print(_)\n",
    "\n",
    "for _ in word_tokenize(sentence.lower()):\n",
    "    if pattern.sub(\"\", _) in stop:\n",
    "# for _ in pattern.sub(\"\", sentence.lower()).split():\n",
    "#     if _ in stop:\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n",
      "Skipped\n",
      "better\n",
      "Skipped\n",
      "ugly\n",
      "Skipped\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Beautiful is better than ugly.\"\n",
    "\n",
    "for _ in word_tokenize(sentence.lower()):\n",
    "    if pattern.sub(\"\", _) in stop:\n",
    "# for _ in pattern.sub(\"\", sentence.lower()).split():\n",
    "#     if _ in stop:\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        print(_)\n",
    "        \n",
    "#한국의 경우는 함부로 날릴 수 없음. 특히 1음절의 경우 ex) 이\n",
    "#섞어 쓰는게 ngram. 한국어는 stopwords 만드는게 쉽지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69791"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "corpus = gutenberg.open(\"austen-emma.txt\").read()\n",
    "len(word_tokenize(corpus)) #19만개\n",
    "\n",
    "words = list()\n",
    "for _ in word_tokenize(corpus.lower()):\n",
    "    if pattern.sub(\"\", _) not in stop:\n",
    "        words.append(_)\n",
    "        #print(\"Skipped\")\n",
    "#     else:\n",
    "#         print(_)\n",
    "len(words) #6만 9천개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['어머니', '는', '짜장면', '이', '싫다', '고', '하셨', '어']\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어머니', '짜장면', '싫다', '하셨']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korstop = {\"은\",\"는\",\"이\",\"가\",\"께\",\"을\",\"를\",\"고\",\"께서\",\"게\",\"에게\", \"어\"}\n",
    "\n",
    "sentence = \"어머니 는 짜장면 이 싫다 고 하셨 어.\"\n",
    "sentence = pattern.sub(\"\", sentence)\n",
    "\n",
    "\n",
    "print(len(word_tokenize(sentence)))\n",
    "print(word_tokenize(sentence))\n",
    "print(len([_ for _ in word_tokenize(sentence) if _ not in korstop]))\n",
    "[_ for _ in word_tokenize(sentence) if _ not in korstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 처리를 최소화하는 것이 좋음\n",
    "#ex ) to be or not to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 길이 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "learn\n",
      "more\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['like', 'learn', 'more']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more somthing.\"\n",
    "for _ in pattern.sub(\"\", sentence.lower()).split():\n",
    "    if 2 < len(_) < 6:\n",
    "        print(_)\n",
    "\n",
    "#정규식\n",
    "#정규식 연습 = https://regexr.com\n",
    "minimum = 3\n",
    "maximum = 5\n",
    "pattern2 = re.compile(r\"\\b\\w{%d,%d}\\b\" % (minimum, maximum))\n",
    "#앞에 r은 이스케이프처리하지 않도록\n",
    "pattern2.findall(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like 2\n",
      "to 2\n",
      "learn 2\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "\n",
    "sentence = \"I'd like to learn more somthing. I'd like to learn. I'd\"\n",
    "obj = Text(word_tokenize(pattern.sub(\"\", sentence.lower())))\n",
    "for _ in obj.vocab():\n",
    "    if 1 < obj.vocab().get(_) < 3:\n",
    "        print(_, obj.vocab().get(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-7fc6505de0f2>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-7fc6505de0f2>\"\u001b[1;36m, line \u001b[1;32m40\u001b[0m\n\u001b[1;33m    #         print(_, obj.vocab().get(_))\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "original = Text(word_tokenize(corpus))\n",
    "\n",
    "original.vocab().most_common(50)\n",
    "#len(set(original.vocab()))\n",
    "original.vocab().N()\n",
    "original.vocab().B()\n",
    "\n",
    "lowercase = Text(word_tokenize(corpus.lower()))\n",
    "print(Text(word_tokenize(corpus.lower())).vocab().N())\n",
    "print(Text(word_tokenize(corpus.lower())).vocab().B())\n",
    "\n",
    "punct1 =  Text(word_tokenize(pattern.sub(\"\", corpus.lower())))\n",
    "print(punct1.vocab().N(), punct1.vocab().B())\n",
    "\n",
    "punct2 =  Text(word_tokenize(pattern.sub(\"\", corpus.lower())))\n",
    "print(punct2.vocab().N(), punct2.vocab().B())\n",
    "\n",
    "stops = Text([_ for _ in word_tokenize(pattern.sub(\" \", corpus.lower())) if _ not in stop])\n",
    "        \n",
    "print(stops.vocab().N(), stops.vocab().B())\n",
    "\n",
    "original.vocab().most_common(10)\n",
    "\n",
    "obj = Text(word_tokenize(pattern.sub(\"\", corpus.lower())))\n",
    "\n",
    "minimum = 3\n",
    "pattern3 = re.compile(r\"\\b\\w{%d,}\\b\" % (minimum))\n",
    "#앞에 r은 이스케이프처리하지 않도록\n",
    "length = Text([_ for _ in word_tokenize(pattern.sub(\"\", corpus.lower()))\n",
    "               if _ not in stop and re.search(r\"\\b\\w{4,}\\b\")\n",
    "#print(length.count)\n",
    "  \n",
    "#freq = [(_, f)]\n",
    "# K = pattern3.findall(corpus)\n",
    "\n",
    "# print(K.vocab().N(), K.vocab().B())\n",
    "\n",
    "# for _ in obj.vocab():\n",
    "#     if obj.vocab().get(_) < 10:\n",
    "#         print(_, obj.vocab().get(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => lexicon resource (X, 돈)\n",
    "\n",
    "def ngram(data, n=2):\n",
    "    result = defaultdict(int)\n",
    "\n",
    "    for term, freq in data.items():\n",
    "        tokens = term.split()\n",
    "        for i in range(len(tokens) - (n-1)):\n",
    "            result[' '.join(tokens[i:i+n])] += freq\n",
    "    return result\n",
    "\n",
    "def umjeol(text, n =2):\n",
    "    ngram = list()\n",
    "\n",
    "    for i in range(len(text)-(n-1)):\n",
    "        ngram.append(''.join(text[i:i+n]))\n",
    "    return ngram\n",
    "\n",
    "stop = []\n",
    "sentence = \"\"\n",
    "result = list()\n",
    "\n",
    "[_ for _ in sentence.split() if _ not in stop]\n",
    "for _ in sentence.split():\n",
    "#     if _ not in stop:\n",
    "    if not re.search(stop[0], re.sub(r\"\\b[0-9+\\b]\", \"\", _)):\n",
    "        result.append(_)\n",
    "    else:\n",
    "        result.append(\"*\"*len(_))\n",
    "\n",
    "\" \".join(result)\n",
    "\n",
    "for _ in sentence.split():\n",
    "    for ngram in umjeol(_):\n",
    "        if ngram in stop:\n",
    "            flag = True\n",
    "    \n",
    "    if not flag:\n",
    "        result.append(_)\n",
    "        \n",
    "    else:\n",
    "        result.append(\"*\", len(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    splitTerm('low'):5,\n",
    "    splitTerm('lowest'):2,\n",
    "    splitTerm('newer'):6,\n",
    "    splitTerm('wider'):3\n",
    "}\n",
    "for _ in range(5):\n",
    "    bigram = ngram(data)\n",
    "    maxKey = max(bigram, key=bigram.get)\n",
    "    data = mergerNgram(maxKey, data)\n",
    "print(data)\n",
    "\n",
    "pattern= defaultdict()\n",
    "for _ in data:\n",
    "    for token n _.split():\n",
    "        pattern[token] += data[_]\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords => list (dictionary)\n",
    "# BPE\n",
    "# tokenizing\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "MODEL = r\"C:\\Users\\J\\Desktop\\stanford-postagger-full-2018-10-16\\stanford-postagger-full-2018-10-16\\models\\english-bidirectional-distsim.tagger\"\n",
    "PARSER = r\"C:\\Users\\J\\Desktop\\stanford-postagger-full-2018-10-16\\stanford-postagger-full-2018-10-16\\stanford-postagger-3.9.2.jar\"\n",
    "pos = StanfordPOSTagger(MODEL, PARSER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4c135d80ea73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "pos.tag(word_tokenize(sent_tokenize(corpus)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
