{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "font_name = font_manager.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 어플리케이션: 영화 리뷰 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_train의 타입: <class 'list'>\n",
      "text_train의 길이: 75000\n",
      "text_train[6]:\n",
      "b'Gloomy Sunday - Ein Lied von Liebe und Tod directed by Rolf Sch\\xc3\\xbcbel in 1999 is a romantic, absorbing, beautiful, and heartbreaking movie. It started like Jules and Jim; it ended as one of Agatha Christie\\'s books, and in between it said something about love, friendship, devotion, jealousy, war, Holocaust, dignity, and betrayal, and it did better than The Black Book which is much more popular. It is not perfect, and it made me, a cynic, wonder in the end on the complexity of the relationships and sensational revelations, and who is who to whom but the movie simply overwhelmed me. Perfect or not, it is unforgettable. All four actors as the parts of the tragic not even a triangle but a rectangle were terrific. I do believe that three men could fell deeply for one girl as beautiful and dignified as Ilona in a star-making performance by young Hungarian actress Erica Marozs\\xc3\\xa1n and who would not? The titular song is haunting, sad, and beautiful, and no doubt deserves the movie been made about it and its effect on the countless listeners. I love the movie and I am surprised that it is so little known in this country. It is a gem.<br /><br />The fact that it is based on a story of the song that had played such important role in the lives of all characters made me do some research, and the real story behind the song of Love and Death seems as fascinating as the fictional one. The song was composed in 1930s by Rezs\\xc3\\xb6 Seress and was believed to have caused many suicides in Hungary and all over Europe as the world was moving toward the most devastating War of the last century. Rezs\\xc3\\xb6 Seress, a Jewish-Hungarian pianist and composer, was thrown to the Concentration Camp but survived, unlike his mother. In January, 1968, Seress committed suicide in Budapest by jumping out of a window. According to his obituary in the New York Times, \"Mr. Seres complained that the success of \"Gloomy Sunday\" actually increased his unhappiness, because he knew he would never be able to write a second hit.\" <br /><br />Many singers from all over the world have recorded their versions of the songs in different languages. Over 70 performers have covered the song since 1935, and some famous names include Billie Holiday, Paul Robeson, Pyotr Leschenko (in Russian, under title \"Mratschnoje Woskresenje\"), Bjork, Sarah McLachlan, and many more. The one that really got to me and made me shiver is by Diamanda Gal\\xc3\\xa1s, the Greek born American singer/pianist/performer with the voice of such tragic power that I still can\\'t get over her singing. Gal\\xc3\\xa1s has been described as \"capable of the most unnerving vocal terror\", and in her work she mostly concentrates on the topics of \"suffering, despair, condemnation, injustice and loss of dignity.\" When she sings the Song of Love and Death, her voice that could\\'ve belonged to the most tragic heroines of Ancient Greece leaves no hope and brings the horror and grief of love lost forever to the unbearable and incomparable heights.<br /><br />8.5/10'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"C:/Users/J/Desktop/aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"text_train의 타입: {}\".format(type(text_train)))\n",
    "print(\"text_train의 길이: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]:\\n{}\".format(text_train[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수 (훈련 데이터): [12500 12500 50000]\n"
     ]
    }
   ],
   "source": [
    "print(\"클래스별 샘플 수 (훈련 데이터): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 문서 수: 25000\n",
      "클래스별 샘플 수 (테스트 데이터): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"테스트 데이터의 문서 수: {}\".format(len(text_test)))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플 데이터에 BOW 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 13\n",
      "어휘 사전의 내용:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"어휘 사전의 내용:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "\n",
    "print(\"BOW: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW의 밀집 표현:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW의 밀집 표현:\\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영화 리뷰에 대한 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x124255 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10315542 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 개수: 124255\n",
      "처음 20개 특성:\n",
      "['00', '000', '0000', '0000000000000000000000000000000001', '0000000000001', '000000001', '000000003', '00000001', '000001745', '00001', '0001', '00015', '0002', '0007', '00083', '000ft', '000s', '000th', '001', '002']\n",
      "20010에서 20030까지 특성:\n",
      "['cheapen', 'cheapened', 'cheapening', 'cheapens', 'cheaper', 'cheapest', 'cheapie', 'cheapies', 'cheapjack', 'cheaply', 'cheapness', 'cheapo', 'cheapozoid', 'cheapquels', 'cheapskate', 'cheapskates', 'cheapy', 'chearator', 'cheat', 'cheata']\n",
      "매 2000번째 특성:\n",
      "['00', '_require_', 'aideed', 'announcement', 'asteroid', 'banquière', 'besieged', 'bollwood', 'btvs', 'carboni', 'chcialbym', 'clotheth', 'consecration', 'cringeful', 'deadness', 'devagan', 'doberman', 'duvall', 'endocrine', 'existent', 'fetiches', 'formatted', 'garard', 'godlie', 'gumshoe', 'heathen', 'honoré', 'immatured', 'interested', 'jewelry', 'kerchner', 'köln', 'leydon', 'lulu', 'mardjono', 'meistersinger', 'misspells', 'mumblecore', 'ngah', 'oedpius', 'overwhelmingly', 'penned', 'pleading', 'previlage', 'quashed', 'recreating', 'reverent', 'ruediger', 'sceme', 'settling', 'silveira', 'soderberghian', 'stagestruck', 'subprime', 'tabloids', 'themself', 'tpf', 'tyzack', 'unrestrained', 'videoed', 'weidler', 'worrisomely', 'zombified']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"특성 개수: {}\".format(len(feature_names)))\n",
    "print(\"처음 20개 특성:\\n{}\".format(feature_names[:20]))\n",
    "print(\"20010에서 20030까지 특성:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"매 2000번째 특성:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "\n",
    "print(\"크로스 밸리데이션 평균 점수: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 매개변수: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"테스트 점수: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"min_df로 제한한 X_train: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"최적의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"불용어 개수: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"매 10번째 불용어:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"불용어가 제거된 X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "param_grid = {'countvectorizer__max_df': [100, 1000, 10000, 20000], 'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grid.best_estimator_.named_steps['countvectorizer'].vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 5).T\n",
    "heatmap = mglearn.tools.heatmap(\n",
    "    scores, xlabel=\"C\", ylabel=\"max_df\", cmap=\"viridis\", fmt=\"%.3f\",\n",
    "    xticklabels=param_grid['logisticregression__C'],\n",
    "    yticklabels=param_grid['countvectorizer__max_df'])\n",
    "\n",
    "plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf–idf로 데이터 스케일 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "X_train = vectorizer.transform(text_train)\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"가장 낮은 tfidf를 가진 특성:\\n{}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"가장 높은 tfidf를 가진 특성: \\n{}\".format(feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "\n",
    "print(\"가장 낮은 idf를 가진 특성:\\n{}\".format(feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 계수 조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps[\"logisticregression\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mglearn.tools.visualize_coefficients(\n",
    "    grid.best_estimator_.named_steps[\"logisticregression\"].coef_[0],\n",
    "    feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 단어로 만든 BOW (n-Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bards_words:\\n{}\".format(bards_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:{}\\n\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 매개변수:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
    "heatmap = mglearn.tools.heatmap(\n",
    "    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n",
    "    xticklabels=param_grid['logisticregression__C'],\n",
    "    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\n",
    "\n",
    "plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
    "mglearn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=40)\n",
    "plt.ylim(-22, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "mglearn.tools.visualize_coefficients(coef.ravel()[mask], feature_names[mask], n_top_features=40)\n",
    "plt.ylim(-22, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 고급 토큰화, 어간 추출, 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy 영어 모듈을 다운로드 하려면 쉘에서 'python -m spacy.en.download all' 명령을 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def compare_normalization(doc):\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    \n",
    "    print(\"표제어:\")\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    print(\"어간:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
    "                       \"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "en_nlp = spacy.load('en')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "    regexp.findall(string))\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document, entity=False, parse=False)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99,\n",
    "                            train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수 \"\n",
    "      \"(기본 CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "\n",
    "print(\"최상의 크로스 밸리데이션 점수 \"\n",
    "      \"(표제어): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(n_components=100, learning_method=\"batch\",\n",
    "                                   max_iter=25, random_state=0)\n",
    "document_topics100 = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([7, 16, 24, 25, 28, 36, 37, 41, 45, 51, 53, 54, 63, 89, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "music = np.argsort(document_topics100[:, 45])[::-1]\n",
    "\n",
    "for i in music[:10]:\n",
    "    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words)\n",
    "               for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
    "\n",
    "for col in [0, 1]:\n",
    "    start = col * 50\n",
    "    end = (col + 1) * 50\n",
    "    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n",
    "    ax[col].set_yticks(np.arange(50))\n",
    "    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n",
    "    ax[col].invert_yaxis()\n",
    "    ax[col].set_xlim(0, 2000)\n",
    "    yax = ax[col].get_yaxis()\n",
    "    yax.set_tick_params(pad=130)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
