{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "MAX_LEN = 111 # EDA에서 추출된 Max Length\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = \"data_out/KOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 학습 데이터 개수: 81000\n",
      "개체명 인식 테스트 데이터 개수: 9000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 준비\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"train.tsv\")\n",
    "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"label.txt\")\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"test.tsv\")\n",
    "\n",
    "def read_file(input_path):\n",
    "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
    "\n",
    "train_ner_dict = {\"sentence\": train_sentences, \"label\": train_labels}\n",
    "train_ner_df = pd.DataFrame(train_ner_dict)\n",
    "\n",
    "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
    "test_ner_dict = {\"sentence\": test_sentences, \"label\": test_labels}\n",
    "test_ner_df = pd.DataFrame(test_ner_dict)\n",
    "\n",
    "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))\n",
    "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))\n",
    "\n",
    "# 개체명 인식 학습 데이터 개수: 81000\n",
    "# 개체명 인식 테스트 데이터 개수: 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 레이블 개수: 30\n"
     ]
    }
   ],
   "source": [
    "# Label 불러오기\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]\n",
    "\n",
    "ner_labels = get_labels(DATA_LABEL_PATH)\n",
    "\n",
    "print(\"개체명 인식 레이블 개수: {}\".format(len(ner_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 토크나이저 설정\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 0\n",
    "sep_token_label_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        truncation=True,\n",
    "        add_special_tokens = True, #'[CLS]'와 '[SEP]' 추가\n",
    "        max_length = MAX_LEN,           # 문장 패딩 및 자르기 진행\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True   # 어탠션 마스크 생성\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "def convert_label(words, labels_idx, ner_begin_label, max_seq_len):\n",
    "            \n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "\n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]\n",
    "        tokens.extend(word_tokens)\n",
    "        \n",
    "        # 슬롯 레이블 값이 Begin이면 I로 추가\n",
    "        if int(slot_label) in ner_begin_label:\n",
    "            label_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(word_tokens) - 1))\n",
    "        else:\n",
    "            label_ids.extend([int(slot_label)] * len(word_tokens))\n",
    "  \n",
    "    # [CLS] and [SEP] 설정\n",
    "    special_tokens_count = 2\n",
    "    if len(label_ids) > max_seq_len - special_tokens_count:\n",
    "        label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # [SEP] 토큰 추가\n",
    "    label_ids += [sep_token_label_id]\n",
    "\n",
    "    # [CLS] 토큰 추가\n",
    "    label_ids = [cls_token_label_id] + label_ids\n",
    "    \n",
    "    padding_length = max_seq_len - len(label_ids)\n",
    "    label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "    \n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
      "['PER-B', 'FLD-B', 'AFW-B', 'ORG-B', 'LOC-B', 'CVL-B', 'DAT-B', 'TIM-B', 'NUM-B', 'EVT-B', 'ANM-B', 'PLT-B', 'MAT-B', 'TRM-B']\n"
     ]
    }
   ],
   "source": [
    "# 테스트용\n",
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "ner_begin_label_string = [ner_labels[label_index] for label_index in ner_begin_label]\n",
    "\n",
    "print(ner_begin_label)\n",
    "print(ner_begin_label_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "\n",
    "def create_inputs_targets(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []\n",
    "\n",
    "    for i, data in enumerate(df[['sentence', 'label']].values):\n",
    "        sentence, labels = data\n",
    "        words = sentence.split()\n",
    "        labels = labels.split()\n",
    "        labels_idx = []\n",
    "        \n",
    "        for label in labels:\n",
    "            labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index(\"UNK\"))\n",
    "\n",
    "        assert len(words) == len(labels_idx)\n",
    "\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(sentence, MAX_LEN)\n",
    "\n",
    "        convert_label_id = convert_label(words, labels_idx, ner_begin_label, MAX_LEN)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_list.append(convert_label_id)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_list = np.asarray(label_list, dtype=int) #레이블 토크나이징 리스트\n",
    "    inputs = (input_ids, attention_masks, token_type_ids)\n",
    "    \n",
    "    return inputs, label_list\n",
    "\n",
    "train_inputs, train_labels = create_inputs_targets(train_ner_df)\n",
    "test_inputs, test_labels = create_inputs_targets(test_ner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertNERClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertNERClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                                name=\"ner_classifier\")\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "\n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "                \n",
    "        sequence_output = self.dropout(sequence_output, training=training)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "ner_model = TFBertNERClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=len(ner_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "    # 0의 레이블 값은 손실 값을 계산할 때 제외\n",
    "    active_loss = tf.reshape(labels, (-1,)) != 0\n",
    "        \n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n",
    "        \n",
    "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "    \n",
    "    return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def compute_f1_pre_rec(self, labels, preds):\n",
    "\n",
    "        return {\n",
    "            \"precision\": precision_score(labels, preds, suffix=True),\n",
    "            \"recall\": recall_score(labels, preds, suffix=True),\n",
    "            \"f1\": f1_score(labels, preds, suffix=True)\n",
    "        }\n",
    "\n",
    "\n",
    "    def show_report(self, labels, preds):\n",
    "        return classification_report(labels, preds, suffix=True)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        results = {}\n",
    "        \n",
    "        pred = self.model.predict(self.x_eval)\n",
    "        label = self.y_eval\n",
    "        pred_argmax = np.argmax(pred, axis = 2)\n",
    "\n",
    "        slot_label_map = {i: label for i, label in enumerate(ner_labels)}\n",
    "\n",
    "        out_label_list = [[] for _ in range(label.shape[0])]\n",
    "        preds_list = [[] for _ in range(label.shape[0])]\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            for j in range(label.shape[1]):\n",
    "                if label[i, j] != 0:\n",
    "                    out_label_list[i].append(slot_label_map[label[i][j]])\n",
    "                    preds_list[i].append(slot_label_map[pred_argmax[i][j]])\n",
    "                    \n",
    "        result = self.compute_f1_pre_rec(out_label_list, preds_list)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"********\")\n",
    "        print(\"F1 Score\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"{}, {:.4f}\".format(key, results[key]))\n",
    "        print(\"\\n\" + self.show_report(out_label_list, preds_list))\n",
    "        print(\"********\")\n",
    "\n",
    "f1_score_callback = F1Metrics(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "# ner_model.compile(optimizer=optimizer, loss=compute_loss, run_eagerly=True)\n",
    "ner_model.compile(optimizer=optimizer, loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR\\tf2_bert_ner -- Folder create complete \n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000239861383A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000239861383A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertNERClassifier.call of <__main__.TFBertNERClassifier object at 0x0000023988094188>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertNERClassifier.call of <__main__.TFBertNERClassifier object at 0x0000023988094188>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertModel.call of <transformers.modeling_tf_bert.TFBertModel object at 0x00000239FDE3DB08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertModel.call of <transformers.modeling_tf_bert.TFBertModel object at 0x00000239FDE3DB08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.modeling_tf_bert.TFBertMainLayer object at 0x00000239FDE4A6C8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.modeling_tf_bert.TFBertMainLayer object at 0x00000239FDE4A6C8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.modeling_tf_bert.TFBertEmbeddings object at 0x0000023987D76AC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.modeling_tf_bert.TFBertEmbeddings object at 0x0000023987D76AC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.modeling_tf_bert.TFBertEncoder object at 0x0000023987C930C8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.modeling_tf_bert.TFBertEncoder object at 0x0000023987C930C8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertLayer.call of <transformers.modeling_tf_bert.TFBertLayer object at 0x0000023987C93EC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.modeling_tf_bert.TFBertLayer object at 0x0000023987C93EC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertAttention.call of <transformers.modeling_tf_bert.TFBertAttention object at 0x0000023987C93F08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.modeling_tf_bert.TFBertAttention object at 0x0000023987C93F08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.modeling_tf_bert.TFBertSelfAttention object at 0x00000239FDE5AD08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.modeling_tf_bert.TFBertSelfAttention object at 0x00000239FDE5AD08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x0000023987EA8848>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x0000023987EA8848>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x0000023987D85688>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x0000023987D85688>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x0000023990C3C548>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x0000023990C3C548>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPooler.call of <transformers.modeling_tf_bert.TFBertPooler object at 0x0000023987D79108>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.modeling_tf_bert.TFBertPooler object at 0x0000023987D79108>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function compute_loss at 0x0000023990C303A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function compute_loss at 0x0000023990C303A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2532/2532 [==============================] - ETA: 0s - loss: 0.4356WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000023C312C0288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000023C312C0288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.7474\n",
      "precision, 0.7184\n",
      "recall, 0.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.41      0.46      0.43       393\n",
      "         ANM       0.51      0.74      0.60       699\n",
      "         CVL       0.67      0.71      0.69      5735\n",
      "         DAT       0.80      0.89      0.84      2510\n",
      "         EVT       0.60      0.71      0.65      1093\n",
      "         FLD       0.50      0.45      0.47       228\n",
      "         LOC       0.66      0.79      0.72      2124\n",
      "         MAT       0.11      0.08      0.10        12\n",
      "         NUM       0.86      0.88      0.87      5544\n",
      "         ORG       0.74      0.76      0.75      4055\n",
      "         PER       0.75      0.83      0.79      4412\n",
      "         PLT       0.00      0.00      0.00        34\n",
      "         TIM       0.78      0.86      0.82       314\n",
      "         TRM       0.58      0.61      0.59      1950\n",
      "\n",
      "   micro avg       0.72      0.78      0.75     29103\n",
      "   macro avg       0.57      0.63      0.59     29103\n",
      "weighted avg       0.72      0.78      0.75     29103\n",
      "\n",
      "********\n",
      "2532/2532 [==============================] - 1300s 513ms/step - loss: 0.4356\n",
      "Epoch 2/3\n",
      "2532/2532 [==============================] - ETA: 0s - loss: 0.2774WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.7808\n",
      "precision, 0.7760\n",
      "recall, 0.7856\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.42      0.47      0.44       393\n",
      "         ANM       0.70      0.66      0.68       699\n",
      "         CVL       0.76      0.68      0.72      5735\n",
      "         DAT       0.89      0.90      0.89      2510\n",
      "         EVT       0.71      0.73      0.72      1093\n",
      "         FLD       0.53      0.54      0.54       228\n",
      "         LOC       0.77      0.77      0.77      2124\n",
      "         MAT       0.04      0.08      0.06        12\n",
      "         NUM       0.87      0.90      0.88      5544\n",
      "         ORG       0.76      0.80      0.78      4055\n",
      "         PER       0.82      0.83      0.82      4412\n",
      "         PLT       0.36      0.15      0.21        34\n",
      "         TIM       0.80      0.88      0.84       314\n",
      "         TRM       0.57      0.68      0.62      1950\n",
      "\n",
      "   micro avg       0.78      0.79      0.78     29103\n",
      "   macro avg       0.64      0.65      0.64     29103\n",
      "weighted avg       0.78      0.79      0.78     29103\n",
      "\n",
      "********\n",
      "2532/2532 [==============================] - 1292s 510ms/step - loss: 0.2774\n",
      "Epoch 3/3\n",
      "2532/2532 [==============================] - ETA: 0s - loss: 0.2165WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.7943\n",
      "precision, 0.7715\n",
      "recall, 0.8184\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.45      0.56      0.50       393\n",
      "         ANM       0.59      0.78      0.67       699\n",
      "         CVL       0.74      0.78      0.76      5735\n",
      "         DAT       0.89      0.91      0.90      2510\n",
      "         EVT       0.66      0.75      0.71      1093\n",
      "         FLD       0.57      0.58      0.57       228\n",
      "         LOC       0.77      0.80      0.79      2124\n",
      "         MAT       0.10      0.08      0.09        12\n",
      "         NUM       0.87      0.91      0.89      5544\n",
      "         ORG       0.77      0.81      0.79      4055\n",
      "         PER       0.81      0.84      0.83      4412\n",
      "         PLT       0.26      0.15      0.19        34\n",
      "         TIM       0.82      0.91      0.86       314\n",
      "         TRM       0.61      0.68      0.64      1950\n",
      "\n",
      "   micro avg       0.77      0.82      0.79     29103\n",
      "   macro avg       0.64      0.68      0.66     29103\n",
      "weighted avg       0.77      0.82      0.80     29103\n",
      "\n",
      "********\n",
      "2532/2532 [==============================] - 1285s 507ms/step - loss: 0.2165\n",
      "{'loss': [0.4356498718261719, 0.27741047739982605, 0.21649129688739777]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_ner\"\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = ner_model.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                        callbacks=[cp_callback, f1_score_callback])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
