{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sy6MNxeg81lH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, metadata = tfds.load('cycle_gan/summer2winter_yosemite', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A, train_B = dataset['trainA'], dataset['trainB'] \n",
    "test_A, test_B = dataset['testA'], dataset['testB'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "EPOCHS = 50\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, label):  \n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    \n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_A = train_A.map(normalize, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_B = train_B.map(normalize, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_A = test_A.map(normalize, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_B = test_B.map(normalize, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpA = next(iter(train_A))\n",
    "inpB = next(iter(train_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "qbwjoGltG3tQ",
    "outputId": "245dad06-e693-4696-fef3-fd4af9e060cc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"Train Set A\")\n",
    "plt.imshow(inpA[0]*0.5 + 0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Train Set B\")\n",
    "plt.imshow(inpB[0]*0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size=3, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                      kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1jkACqCBWR5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size=3, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                               padding='same',\n",
    "                                               kernel_initializer=initializer,\n",
    "                                               use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGRHxHaxwIS_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__(name='')\n",
    "        filters1, filters2, filters3 = filters\n",
    "\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "\n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "    \n",
    "block1 = ResnetIdentityBlock(3, [512, 512, 512])\n",
    "block2 = ResnetIdentityBlock(3, [512, 512, 512])\n",
    "block3 = ResnetIdentityBlock(3, [512, 512, 512])\n",
    "\n",
    "\n",
    "resnet = [block1, block2, block3]\n",
    "\n",
    "print(block1(tf.zeros([1, 16, 16, 512])).shape)\n",
    "print([x.name for x in block1.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAgEDGGTBEWT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False), \n",
    "        downsample(128, 4),\n",
    "        downsample(256, 4), \n",
    "        downsample(512, 4) \n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(256, 4),\n",
    "        upsample(128, 4), \n",
    "        upsample(64, 4), \n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
    "                                           strides=2,\n",
    "                                           padding='same',\n",
    "                                           kernel_initializer=initializer,\n",
    "                                           activation='tanh') \n",
    "\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "    x = inputs\n",
    "\n",
    "    skips = []\n",
    "\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "        \n",
    "    for block in resnet:\n",
    "        x = block(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = up(x)\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dV-cAfYAvv4T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1660
    },
    "colab_type": "code",
    "id": "-megdpETc0ME",
    "outputId": "431b1415-bced-4e28-a6b1-5f7521a22f85",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(generator, 'generator.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "SjEs2tIWgUWR",
    "outputId": "6c2c392a-173f-43b6-f929-ef3dc2e280fd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "RCgMOJHoq9H0",
    "outputId": "eb6f927f-73c4-40fc-c906-15a0fc1f0ec3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_output = generator(inpA, training=False)\n",
    "gen_output = (gen_output + 1) / 2\n",
    "plt.imshow(gen_output[0])\n",
    "\n",
    "print(gen_output.shape,gen_output[0,...].numpy().max(), gen_output[0,...].numpy().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wr1b427FBG68",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    inputs = tf.keras.layers.Input(shape=[None,None,3])\n",
    "    x = inputs\n",
    "    g_filter = 64\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(g_filter),\n",
    "        downsample(g_filter * 2),\n",
    "        downsample(g_filter * 4),\n",
    "        downsample(g_filter * 8),\n",
    "    ]\n",
    "    \n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same')\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "omgw-39yFL_Q",
    "outputId": "07677fc2-3a2d-489e-cc66-88bb0189692f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "dis_output = discriminator(inpA, training=False)\n",
    "\n",
    "print(dis_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "id": "oew7I18lYPH9",
    "outputId": "1b66e3e8-f728-487f-fa53-e2e6c5cb7a48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(discriminator, 'discriminator.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcZ4EnSvDdFE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator_A = Discriminator()\n",
    "discriminator_B = Discriminator()\n",
    "\n",
    "generator_AB = Generator()\n",
    "generator_BA = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ko2iHw8PsHiF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1otm0gvsKRM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jigp9GoF7THl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoEekETnl5Ya",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid = np.ones((BATCH_SIZE, 16, 16, 1)).astype('float32')\n",
    "fake = np.zeros((BATCH_SIZE, 16, 16, 1)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dZ0KaHlwV5O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_batch(imgs_A, imgs_B):\n",
    "    with tf.GradientTape() as g, tf.GradientTape() as d_tape:\n",
    "        fake_B = generator_AB(imgs_A, training=True)\n",
    "        fake_A = generator_BA(imgs_B, training=True)\n",
    "        \n",
    "        logits_real_A = discriminator_A(imgs_A, training=True)\n",
    "        logits_fake_A = discriminator_A(fake_A, training=True)\n",
    "        dA_loss = discriminator_loss(logits_real_A, logits_fake_A)\n",
    "        \n",
    "        logits_real_B = discriminator_B(imgs_B, training=True)\n",
    "        logits_fake_B = discriminator_B(fake_B, training=True)\n",
    "        dB_loss = discriminator_loss(logits_real_B, logits_fake_B)\n",
    "        \n",
    "        d_loss = (dA_loss + dB_loss) / 2\n",
    "        \n",
    "        reconstr_A = generator_BA(fake_B, training=True)\n",
    "        reconstr_B = generator_AB(fake_A, training=True)\n",
    "        \n",
    "        id_A = generator_BA(imgs_A, training=True)\n",
    "        id_B = generator_AB(imgs_B, training=True)\n",
    "\n",
    "        gen_loss = tf.math.reduce_sum([\n",
    "            1 * tf.math.reduce_mean(mean_squared_error(logits_fake_A, valid)),\n",
    "            1 * tf.math.reduce_mean(mean_squared_error(logits_fake_B, valid)),\n",
    "            10 * tf.math.reduce_mean(mean_squared_error(reconstr_A, imgs_A)),\n",
    "            10 * tf.math.reduce_mean(mean_squared_error(reconstr_B, imgs_B)),\n",
    "            0.1 * tf.math.reduce_mean(mean_squared_error(id_A, imgs_A)),\n",
    "            0.1 * tf.math.reduce_mean(mean_squared_error(id_B, imgs_B)),\n",
    "        ])\n",
    "        \n",
    "    gradients_of_d = d_tape.gradient(d_loss, discriminator_A.trainable_variables + discriminator_B.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_d, discriminator_A.trainable_variables + discriminator_B.trainable_variables))\n",
    "\n",
    "    gradients_of_generator = g.gradient(gen_loss, generator_AB.trainable_variables + generator_BA.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients_of_generator, generator_AB.trainable_variables + generator_BA.trainable_variables))\n",
    "    \n",
    "    return dA_loss, dB_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ6GpnTXu8E_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dird_A = './training_checkpointsd_A'\n",
    "checkpoint_prefixd_A = os.path.join(checkpoint_dird_A, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_dird_B = './training_checkpointsd_B'\n",
    "checkpoint_prefixd_B = os.path.join(checkpoint_dird_B, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_dirg_AB = './training_checkpointsg_AB'\n",
    "checkpoint_prefixg_AB = os.path.join(checkpoint_dirg_AB, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_dirg_BA = './training_checkpointsg_BA'\n",
    "checkpoint_prefixg_BA = os.path.join(checkpoint_dirg_BA, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMtERiaPkVBQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(trainA_, trainB_, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(zip(trainA_, trainB_)):\n",
    "            dA_loss, dB_loss, g_loss = train_batch(imgs_A, imgs_B)\n",
    "            \n",
    "            if batch_i % 1000 == 0:\n",
    "                test_imgA = next(iter(test_A))\n",
    "                test_imgB = next(iter(test_B))\n",
    "                \n",
    "                print ('Time taken for epoch {} batch index {} is {} seconds\\n'.format(epoch, batch_i, time.time()-start))\n",
    "                print(\"discriminator A: \", dA_loss.numpy())\n",
    "                print(\"discriminator B: \", dB_loss.numpy())\n",
    "                print(\"generator: {}\\n\".format(g_loss))\n",
    "\n",
    "                fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharey=True, sharex=True)\n",
    "                gen_outputA = generator_AB(test_imgA, training=False)\n",
    "                gen_outputB = generator_BA(test_imgB, training=False)\n",
    "                \n",
    "                axs[0,0].imshow(test_imgA[0]*0.5 + 0.5)\n",
    "                axs[0,0].set_title(\"Generator A Input\")\n",
    "                \n",
    "                axs[0,1].imshow(gen_outputA[0]*0.5 + 0.5)\n",
    "                axs[0,1].set_title(\"Generator A Output\")\n",
    "                \n",
    "                axs[1,0].imshow(test_imgB[0]*0.5 + 0.5)\n",
    "                axs[1,0].set_title(\"Generator B Input\")\n",
    "                \n",
    "                axs[1,1].imshow(gen_outputB[0]*0.5 + 0.5)\n",
    "                axs[1,1].set_title(\"Generator B Output\")\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "                discriminator_A.save_weights(checkpoint_prefixd_A.format(epoch=epoch))\n",
    "                discriminator_B.save_weights(checkpoint_prefixd_B.format(epoch=epoch))\n",
    "                generator_AB.save_weights(checkpoint_prefixg_AB.format(epoch=epoch))\n",
    "                generator_BA.save_weights(checkpoint_prefixg_BA.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VL7VXj5uUtVr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(train_A, train_B, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ei214WaKmmdL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator_A.save_weights('discriminator_A.h5')\n",
    "discriminator_B.save_weights('discriminator_B.h5')\n",
    "generator_AB.save_weights('generator_AB.h5')\n",
    "generator_BA.save_weights('generator_BA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8yJWJgAQdlZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator_A.load_weights('./discriminator_A.h5')\n",
    "discriminator_B.load_weights('./discriminator_B.h5')\n",
    "generator_AB.load_weights('./generator_AB.h5')\n",
    "generator_BA.load_weights('./generator_BA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12155
    },
    "colab_type": "code",
    "id": "zubIQxCd78Pn",
    "outputId": "9cec5195-75e1-4140-ef28-fdfd4793a87c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "for batch_i, (imgs_A, imgs_B) in enumerate(zip(test_A, test_B)):\n",
    "    fake_B = generator_AB(imgs_A, training=False)\n",
    "    fake_A = generator_BA(imgs_B, training=False)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    axs[0,0].imshow(imgs_A[0]*0.5 + 0.5)\n",
    "    axs[0,0].set_title(\"Generator A Input\")\n",
    "    axs[0,1].imshow(fake_B[0]*0.5 + 0.5)\n",
    "    axs[0,1].set_title(\"Generator A Output\")\n",
    "    axs[1,0].imshow(imgs_B[0]*0.5 + 0.5)\n",
    "    axs[1,0].set_title(\"Generator B Input\")\n",
    "    axs[1,1].imshow(fake_A[0]*0.5 + 0.5)\n",
    "    axs[1,1].set_title(\"Generator B Output\")\n",
    "    plt.show()\n",
    "    \n",
    "    count -= 1\n",
    "    if count <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCznhfFHuNKy",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cycle_gan_v3_2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
