{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzY3w4DFMwJx"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAl-m3CJM9f2"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm-eURywwxqM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/tensorflow_text/intro\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/examples/intro.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/examples/intro.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/examples/intro.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkDse1F8QfLC"
      },
      "source": [
        "# TF.Text\n",
        "\n",
        "## Introduction\n",
        "\n",
        "TensorFlow Text provides a collection of text related classes and ops ready to use with TensorFlow 2.0. The library can perform the preprocessing regularly required by text-based models, and includes other features useful for sequence modeling not provided by core TensorFlow.\n",
        "\n",
        "The benefit of using these ops in your text preprocessing is that they are done in the TensorFlow graph. You do not need to worry about tokenization in training being different than the tokenization at inference, or managing preprocessing scripts.  \n",
        "  \n",
        "소개\n",
        "TensorFlow Text는 TensorFlow 2.0에서 사용할 준비가 된 텍스트 관련 클래스 및 작업 모음을 제공합니다. 라이브러리는 텍스트 기반 모델에 필요한 사전 처리를 정기적으로 수행 할 수 있으며 핵심 TensorFlow에서 제공하지 않는 시퀀스 모델링에 유용한 기타 기능을 포함합니다.\n",
        "\n",
        "텍스트 전처리에서 이러한 작업을 사용하는 이점은 TensorFlow 그래프에서 수행된다는 것입니다. 교육에서 토큰 화가 추론 또는 전처리 스크립트 관리에서 토큰 화와 다른 것에 대해 걱정할 필요가 없습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vzUaGC_QimJ"
      },
      "source": [
        "## Eager Execution\n",
        "\n",
        "TensorFlow Text requires TensorFlow 2.0, and is fully compatible with eager mode and graph mode.\n",
        "\n",
        "---\n",
        "\n",
        "Note: On rare occassions, this import may fail looking for the TF library. Please reset the runtime and rerun the pip install below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3iZ_QvCzuLD"
      },
      "source": [
        "!pip install tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjeiCYH2HgLG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBz-AF1lQwO0"
      },
      "source": [
        "## Unicode\n",
        "\n",
        "Most ops expect that the strings are in UTF-8. If you're using a different encoding, you can use the core tensorflow transcode op to transcode into UTF-8. You can also use the same op to coerce your string to structurally valid UTF-8 if your input could be invalid.  \n",
        "  \n",
        "유니 코드\n",
        "대부분의 작업은 문자열이 UTF-8로되어 있다고 예상합니다. 다른 인코딩을 사용하는 경우 핵심 tensorflow 트랜스 코딩 작업을 사용하여 UTF-8로 트랜스 코딩 할 수 있습니다. 입력이 유효하지 않은 경우 동일한 연산을 사용하여 문자열을 구조적으로 유효한 UTF-8로 강제 변환 할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mup7OvnUQyrk"
      },
      "source": [
        "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
        "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhF9xiL4RdGZ"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and/or punctuation.\n",
        "\n",
        "The main interfaces are `Tokenizer` and `TokenizerWithOffsets` which each have a single method `tokenize` and `tokenize_with_offsets` respectively. There are multiple tokenizers available now. Each of these implement `TokenizerWithOffsets` (which extends `Tokenizer`) which includes an option for getting byte offsets into the original string. This allows the caller to know the bytes in the original string the token was created from.\n",
        "\n",
        "All of the tokenizers return RaggedTensors with the inner-most dimension of tokens mapping to the original individual strings. As a result, the resulting shape's rank is increased by one. Please review the ragged tensor guide if you are unfamiliar with them. [ragged_tensors](https://www.tensorflow.org/guide/ragged_tensors)\n",
        "\n",
        "### WhitespaceTokenizer\n",
        "\n",
        "This is a basic tokenizer that splits UTF-8 strings on ICU defined whitespace characters (eg. space, tab, new line).  \n",
        "  \n",
        "토큰 화\n",
        "토큰 화는 문자열을 토큰으로 나누는 프로세스입니다. 일반적으로 이러한 토큰은 단어, 숫자 및 / 또는 구두점입니다.\n",
        "\n",
        "주요 인터페이스는 각각 단일 메서드 tokenize 및 tokenize_with_offsets 가있는 Tokenizer 및 TokenizerWithOffsets 입니다. 현재 사용 가능한 여러 토크 나이저가 있습니다. 이러한 각각의 구현 TokenizerWithOffsets (확장하는 Tokenizer ) 원래의 문자열로 바이트 오프셋을 얻기위한 옵션이 포함되어있다. 이를 통해 호출자는 토큰이 생성 된 원래 문자열의 바이트를 알 수 있습니다.\n",
        "\n",
        "모든 토크 나이 저는 원래 개별 문자열에 매핑되는 토큰의 가장 안쪽 차원이있는 RaggedTensors를 반환합니다. 결과적으로 결과 모양의 순위가 1 씩 증가합니다. 익숙하지 않은 경우 비정형 텐서 가이드를 검토하십시오. https://www.tensorflow.org/guide/ragged_tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj6QRMJ_RbTf"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "print(tokens.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTuPjpyrGwKK"
      },
      "source": [
        "### UnicodeScriptTokenizer\n",
        "\n",
        "This tokenizer splits UTF-8 strings based on Unicode script boundaries. The script codes used correspond to International Components for Unicode (ICU) UScriptCode values. See: http://icu-project.org/apiref/icu4c/uscript_8h.html\n",
        "\n",
        "In practice, this is similar to the `WhitespaceTokenizer` with the most apparent difference being that it will split punctuation (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language texts from each other.  \n",
        "  \n",
        "UnicodeScriptTokenizer\n",
        "이 토크 나이 저는 유니 코드 스크립트 경계를 기반으로 UTF-8 문자열을 분할합니다. 사용되는 스크립트 코드는 ICU (International Components for Unicode) UScriptCode 값에 해당합니다. 참조 : http://icu-project.org/apiref/icu4c/uscript_8h.html\n",
        "\n",
        "실제로 이것은 WhitespaceTokenizer 와 유사하지만 언어 텍스트 (예 : USCRIPT_LATIN, USCRIPT_CYRILLIC 등)에서 구두점 (USCRIPT_COMMON)을 분리하는 동시에 언어 텍스트를 서로 분리한다는 점이 가장 뚜렷합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIAu7D7JG6xC"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "print(tokens.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzuqb8xkG7Cp"
      },
      "source": [
        "### Unicode split\n",
        "\n",
        "When tokenizing languages without whitespace to segment words, it is common to just split by character, which can be accomplished using the [unicode_split](https://www.tensorflow.org/api_docs/python/tf/strings/unicode_split) op found in core."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCKTl2mzdZIo"
      },
      "source": [
        "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
        "print(tokens.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yGs1mWkeOD8"
      },
      "source": [
        "### Offsets\n",
        "\n",
        "When tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements `TokenizerWithOffsets` has a *tokenize_with_offsets* method that will return the byte offsets along with the tokens. The start_offsets lists the bytes in the original string each token starts at, and the end_offsets lists the bytes immediately after the point where each token ends.  Note: the start offsets are inclusive and the end offsets are exclusive.  \n",
        "  \n",
        "오프셋\n",
        "문자열을 토큰화할 때 원래 문자열에서 토큰의 출처를 알고 싶은 경우가 종종 있습니다. 이러한 이유로 TokenizerWithOffsets 를 구현하는 각 토크 나이저에는 토큰과 함께 바이트 오프셋을 반환하는 tokenize_with_offsets 메서드가 있습니다. offset_starts는 각 토큰이 시작하는 원래 문자열의 바이트를 나열하고 offset_limits는 각 토큰이 끝나는 바이트를 나열합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yzUormbiQwz"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "print(tokens.to_list())\n",
        "print(start_offsets.to_list())\n",
        "print(end_offsets.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO-tRdH72Vl9"
      },
      "source": [
        "### TF.Data Example\n",
        "\n",
        "Tokenizers work as expected with the tf.data API. A simple example is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MCqcHXg3gtp"
      },
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
        "iterator = iter(tokenized_docs)\n",
        "print(next(iterator).to_list())\n",
        "print(next(iterator).to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAFvE0bDi5mq"
      },
      "source": [
        "## Other Text Ops\n",
        "\n",
        "TF.Text packages other useful preprocessing ops. We will review a couple below.\n",
        "\n",
        "### Wordshape\n",
        "\n",
        "A common feature used in some natural language understanding models is to see if the text string has a certain property. For example, a sentence breaking model might contain features which check for word capitalization or if a punctuation character is at the end of a string.\n",
        "\n",
        "Wordshape defines a variety of useful regular expression based helper functions for matching various relevant patterns in your input text. Here are a few examples.  \n",
        "  \n",
        "기타 텍스트 작업\n",
        "TF.Text는 다른 유용한 전처리 작업을 패키징합니다. 아래에서 몇 가지를 검토하겠습니다.\n",
        "\n",
        "워드 셰이프\n",
        "일부 자연어 이해 모델에서 사용되는 일반적인 기능은 텍스트 문자열에 특정 속성이 있는지 확인하는 것입니다. 예를 들어, 문장 분리 모델에는 단어 대문자 사용 또는 구두점 문자가 문자열 끝에 있는지 확인하는 기능이 포함될 수 있습니다.\n",
        "\n",
        "Wordshape는 입력 텍스트에서 다양한 관련 패턴을 일치시키기 위해 유용한 정규식 기반 도우미 함수를 다양하게 정의합니다. 다음은 몇 가지 예입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HK5KOj5IJ5m"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "\n",
        "# Is capitalized?\n",
        "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
        "# Are all letters uppercased?\n",
        "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
        "# Does the token contain punctuation?\n",
        "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
        "# Is the token a number?\n",
        "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
        "\n",
        "print(f1.to_list())\n",
        "print(f2.to_list())\n",
        "print(f3.to_list())\n",
        "print(f4.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zrnuXTc3ziR"
      },
      "source": [
        "### N-grams & Sliding Window\n",
        "\n",
        "N-grams are sequential words given a sliding window size of *n*. When combining the tokens, there are three reduction mechanisms supported. For text, you would want to use `Reduction.STRING_JOIN` which appends the strings to each other. The default separator character is a space, but this can be changed with the string_separater argument.\n",
        "\n",
        "The other two reduction methods are most often used with numerical values, and these are `Reduction.SUM` and `Reduction.MEAN`.  \n",
        "  \n",
        "N- 그램 및 슬라이딩 윈도우\n",
        "N- 그램은 슬라이딩 윈도우 크기가 n 인 경우 순차적 인 단어입니다. 토큰을 결합 할 때 지원되는 세 가지 감소 메커니즘이 있습니다. 텍스트의 경우 문자열을 서로 추가하는 Reduction.STRING_JOIN 을 사용할 수 있습니다. 기본 구분 문자는 공백이지만 string_separater 인수로 변경할 수 있습니다.\n",
        "\n",
        "다른 두 가지 감소 방법은 숫자 값과 함께 가장 자주 사용되며 Reduction.SUM 및 Reduction.MEAN 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czIf9HcoIquB"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "\n",
        "# Ngrams, in this case bi-gram (n = 2)\n",
        "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
        "\n",
        "print(bigrams.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4YAEEUGG1dM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}